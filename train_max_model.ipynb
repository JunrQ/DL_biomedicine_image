{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1027 14:25:56 @logger.py:94]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Log directory train_log/vgg_max_model/ exists! Please either backup/delete it, or use a new directory.\n",
      "\u001b[32m[1027 14:25:56 @logger.py:96]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m If you're resuming from a previous run you can choose to keep it.\n",
      "\u001b[32m[1027 14:25:56 @logger.py:97]\u001b[0m Select Action: k (keep) / b (backup) / d (delete) / n (new) / q (quit):\n",
      "d\n",
      "\u001b[32m[1027 14:25:58 @logger.py:74]\u001b[0m Argv: /home/fuxiaofeng/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py -f /run/user/1012/jupyter/kernel-d10c0982-6dae-46d2-bb16-00ea9e1ccd1b.json\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from config.max_model import ModelConfig\n",
    "from functional import seq\n",
    "from models import MaxModel\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from tensorpack import (TrainConfig, SyncMultiGPUTrainerParameterServer as Trainer, \n",
    "                        PredictConfig, MultiProcessDatasetPredictor as Predictor,\n",
    "                        SaverRestore, logger)\n",
    "from tensorpack.callbacks import (ScheduledHyperParamSetter, MaxSaver, ModelSaver,\n",
    "                                  DataParallelInferenceRunner as InfRunner)\n",
    "from tensorpack.predict import SimpleDatasetPredictor\n",
    "from tensorpack.tfutils.common import get_default_sess_config\n",
    "from utils import DataManager\n",
    "from utils.validation import (Accumulator, AggregateMetric, calcu_metrics)\n",
    "\n",
    "vgg_loc = \"./data/vgg_16.ckpt\"\n",
    "# log 存放位置\n",
    "#     log 中包含 Inferencer 给出的 metrics， 可用tensorboard查看\n",
    "#     命令 tensorboard --logdir XXXX --port XXXX\n",
    "#     类似 jupyter notebook，tensorboard 需要通过浏览器使用， 请建立 SSH tunnel\n",
    "#     log_dir 下也会存放 ModelSaver 生成的 checkpoint文件，这些文件占用空间很大，\n",
    "#     建议将 log_dir 放在 /data 分区下，或建立软链接\n",
    "log_dir = 'train_log/vgg_max_model/'\n",
    "logger.set_logger_dir(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 可以在这里修改配置\n",
    "config = ModelConfig()\n",
    "# 新增：（width, height) 图片大小。DataManager需要知道图片的大小。这里也可以进行resize操作\n",
    "config.image_size = (320, 128)\n",
    "# 新增：划分比例\n",
    "config.proportion = {'train': 0.6, 'val': 0.2, 'test': 0.2}\n",
    "# 新增: 在划分数据集时，允许比例的误差\n",
    "config.tolerance_margin = 0.02\n",
    "# 新增：在划分数据集时，是否打乱顺序\n",
    "#    设置为 True，每次划分的结果会不同\n",
    "#    不过现在我把随机数种子固定了，无论true 或 false每次划分的结果都一样\n",
    "#    之后可能会取消固定随机数种子\n",
    "config.shuffle_separation = True\n",
    "# 新增：要使用哪些方向\n",
    "config.directions = ['ventral', 'dorsal', 'lateral']\n",
    "# 新增：DataManager 需要知道 batch_size\n",
    "config.batch_size = 20\n",
    "# 为了兼容 DataManager， 我把 stage_allowed 改为 stages \n",
    "config.stages = [6]\n",
    "# 同样的， top_k_labels 改为 annotation_number\n",
    "config.annotation_number = 10\n",
    "# max_img 改为 max_sequence_length\n",
    "config.max_sequence_length = 10\n",
    "# 请修改为图片所在位置\n",
    "config.image_directory = str(Path.home()) + \\\n",
    "    \"/Documents/flyexpress/DL_biomedicine_image/data/pic_data/\"\n",
    "\n",
    "# standard_images.csv 与 standard_annotations.csv 附带在 repo 的 data 目录下\n",
    "config.image_table_location = str(Path.home()) + \\\n",
    "    \"/Documents/flyexpress/DL_biomedicine_image/data/standard_images.csv\"\n",
    "config.annotation_table_location = str(Path.home()) + \\\n",
    "    \"/Documents/flyexpress/DL_biomedicine_image/data/standard_annotations.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从vgg checkpoint 回复权值时，要忽略的tensor名称\n",
    "ignore_restore = ['learning_rate', 'global_step']\n",
    "# 在训练中，会保存一个某个metric最大（或最小）的模型，用于之后测试\n",
    "save_name = \"max-micro_auc.ckpt\"\n",
    "# 将 probability 转化为 0， 1 的阈值\n",
    "threshold = 0.5\n",
    "# 在验证集与测试集上要计算哪些 metrics\n",
    "validation_metrics = ['mean_average_precision', 'macro_auc', 'micro_auc',\n",
    "                      'macro_f1', 'micro_f1', 'ranking_mean_average_precision',\n",
    "                      'coverage', 'ranking_loss', 'one_error']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group numbers:\n",
      "train: 1476, validation: 449, test: 527\n",
      "Image numbers:\n",
      "train: 8489, validation: 2594, test: 3087\n",
      "                                         train       val      test\n",
      "embryonic midgut                      1.044321  0.995556  1.125000\n",
      "ventral nerve cord                    1.748603  1.823899  1.977401\n",
      "embryonic brain                       1.860465  1.841772  2.028736\n",
      "embryonic hindgut                     2.400922  2.277372  2.513333\n",
      "embryonic dorsal epidermis            2.904762  2.741667  2.875000\n",
      "embryonic/larval muscle system        3.146067  2.870690  2.932836\n",
      "embryonic central nervous system      3.193182  3.235849  3.543103\n",
      "embryonic ventral epidermis           3.405970  3.235849  3.319672\n",
      "embryonic head epidermis              4.698842  4.907895  4.547368\n",
      "dorsal prothoracic pharyngeal muscle  5.201681  4.831169  4.988636\n"
     ]
    }
   ],
   "source": [
    "data_manager = DataManager.from_config(config)\n",
    "print(data_manager.get_imbalance_ratio())\n",
    "train_data = data_manager.get_validation_stream()\n",
    "val_data = data_manager.get_test_stream()\n",
    "test_data = data_manager.get_test_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1027 14:26:00 @inference_runner.py:83]\u001b[0m InferenceRunner will eval on an InputSource of size 65\n",
      "\u001b[32m[1027 14:26:01 @input_source.py:178]\u001b[0m Setting up the queue 'QueueInput/input_queue' for CPU prefetching ...\n",
      "\u001b[32m[1027 14:26:01 @input_source.py:459]\u001b[0m Setting up StagingArea for GPU prefetching ...\n",
      "\u001b[32m[1027 14:26:01 @training.py:41]\u001b[0m Training a model of 2 towers\n",
      "\u001b[32m[1027 14:26:01 @training.py:92]\u001b[0m Building graph for training tower 0 on device LeastLoadedDeviceSetter-/gpu:0...\n",
      "\u001b[32m[1027 14:26:01 @regularize.py:108]\u001b[0m Add REGULARIZATION_LOSSES of 7 tensors on the total cost.\n",
      "\u001b[32m[1027 14:26:01 @training.py:92]\u001b[0m Building graph for training tower 1 on device LeastLoadedDeviceSetter-/gpu:1...\n",
      "\u001b[32m[1027 14:26:01 @regularize.py:108]\u001b[0m Add REGULARIZATION_LOSSES of 7 tensors on the total cost.\n",
      "\u001b[32m[1027 14:26:02 @model_utils.py:47]\u001b[0m \u001b[36mModel Parameters: \n",
      "\u001b[0mname                               shape                 dim  device\n",
      "---------------------------------  ----------------  -------  -------------\n",
      "adaption/conv1/kernel:0            [3, 3, 512, 512]  2359296  /device:GPU:0\n",
      "adaption/conv1/bias:0              [512]                 512  /device:GPU:1\n",
      "adaption/conv2/kernel:0            [3, 3, 512, 512]  2359296  /device:GPU:1\n",
      "adaption/conv2/bias:0              [512]                 512  /device:GPU:0\n",
      "adaption/conv3/kernel:0            [3, 3, 512, 512]  2359296  /device:GPU:0\n",
      "adaption/conv3/bias:0              [512]                 512  /device:GPU:1\n",
      "adaption/net_reduce_fc/kernel:0    [1, 1, 512, 2]       1024  /device:GPU:1\n",
      "adaption/net_reduce_fc/bias:0      [2]                     2  /device:GPU:1\n",
      "adaption/dense/kernel:0            [168, 128]          21504  /device:GPU:1\n",
      "adaption/dense/bias:0              [128]                 128  /device:GPU:1\n",
      "adaption/net_max_feature/kernel:0  [1, 1, 512, 512]   262144  /device:GPU:1\n",
      "adaption/net_max_feature/bias:0    [512]                 512  /device:GPU:1\n",
      "adaption/adaption_output/kernel:0  [640, 10]            6400  /device:GPU:1\n",
      "adaption/adaption_output/bias:0    [10]                   10  /device:GPU:1\u001b[36m\n",
      "Total #vars=14, #param=7371148 (28.12 MB assuming all float32)\u001b[0m\n",
      "\u001b[32m[1027 14:26:02 @base.py:207]\u001b[0m Setup callbacks graph ...\n",
      "\u001b[32m[1027 14:26:02 @input_source.py:178]\u001b[0m Setting up the queue 'DataParallelInferenceRunner/QueueInput/input_queue' for CPU prefetching ...\n",
      "\u001b[32m[1027 14:26:02 @predictor_factory.py:54]\u001b[0m Building predictor tower 'InferenceTower0' on device /gpu:0 ...\n",
      "\u001b[32m[1027 14:26:02 @predictor_factory.py:54]\u001b[0m Building predictor tower 'InferenceTower1' on device /gpu:1 ...\n",
      "\u001b[32m[1027 14:26:02 @summary.py:34]\u001b[0m Maintain moving average summary of 1 tensors.\n",
      "\u001b[32m[1027 14:26:02 @graph.py:91]\u001b[0m Applying collection UPDATE_OPS of 2 ops.\n",
      "\u001b[32m[1027 14:26:03 @sessinit.py:136]\u001b[0m Variable global_step:0 in the graph will be not loaded from the checkpoint!\n",
      "\u001b[32m[1027 14:26:03 @sessinit.py:89]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m The following variables are in the graph, but not found in the checkpoint: adaption/conv1/kernel, adaption/conv1/bias, adaption/conv2/kernel, adaption/conv2/bias, adaption/conv3/kernel, adaption/conv3/bias, adaption/net_reduce_fc/kernel, adaption/net_reduce_fc/bias, adaption/dense/kernel, adaption/dense/bias, adaption/net_max_feature/kernel, adaption/net_max_feature/bias, adaption/adaption_output/kernel, adaption/adaption_output/bias, learning_rate, beta1_power, beta2_power\n",
      "\u001b[32m[1027 14:26:03 @sessinit.py:89]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m The following variables are in the checkpoint, but not found in the graph: global_step:0, vgg_16/conv5/conv5_1/biases:0, vgg_16/conv5/conv5_1/weights:0, vgg_16/conv5/conv5_2/biases:0, vgg_16/conv5/conv5_2/weights:0, vgg_16/conv5/conv5_3/biases:0, vgg_16/conv5/conv5_3/weights:0, vgg_16/fc6/biases:0, vgg_16/fc6/weights:0, vgg_16/fc7/biases:0, vgg_16/fc7/weights:0, vgg_16/fc8/biases:0, vgg_16/fc8/weights:0, vgg_16/mean_rgb:0\n",
      "\u001b[32m[1027 14:26:03 @base.py:212]\u001b[0m Creating the session ...\n",
      "\u001b[32m[1027 14:26:05 @base.py:216]\u001b[0m Initializing the session ...\n",
      "\u001b[32m[1027 14:26:05 @sessinit.py:116]\u001b[0m Restoring checkpoint from ./data/vgg_16.ckpt ...\n",
      "INFO:tensorflow:Restoring parameters from ./data/vgg_16.ckpt\n",
      "\u001b[32m[1027 14:26:05 @base.py:223]\u001b[0m Graph Finalized.\n",
      "\u001b[32m[1027 14:26:05 @param.py:144]\u001b[0m After epoch 0, learning_rate will change to 0.00010000\n",
      "\u001b[32m[1027 14:26:05 @concurrency.py:36]\u001b[0m Starting EnqueueThread DataParallelInferenceRunner/QueueInput/input_queue ...\n",
      "\u001b[32m[1027 14:26:05 @concurrency.py:36]\u001b[0m Starting EnqueueThread QueueInput/input_queue ...\n",
      "\u001b[32m[1027 14:26:05 @input_source.py:418]\u001b[0m Pre-filling staging area ...\n",
      "\u001b[32m[1027 14:26:06 @base.py:257]\u001b[0m Start Epoch 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########|56/56[00:25<00:00, 2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1027 14:26:32 @base.py:267]\u001b[0m Epoch 1 (global_step 56) finished, time:25.58 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          |0/65[00:00<?,?it/s]/home/fuxiaofeng/Applications/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/fuxiaofeng/Applications/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      " 15%|#5        |10/65[00:20<00:07, 7.07it/s]"
     ]
    }
   ],
   "source": [
    "model = MaxModel(config)\n",
    "tf.reset_default_graph()\n",
    "train_config = TrainConfig(model=model, dataflow=train_data,\n",
    "                           callbacks=[\n",
    "                               # 在这里设置 learning rate\n",
    "                               ScheduledHyperParamSetter('learning_rate', [(0, 1e-4), (20, 1e-4)]),\n",
    "                               InfRunner(val_data, [AggregateMetric(validation_metrics, threshold)],\n",
    "                                         [0, 1]),\n",
    "                               ModelSaver(max_to_keep=5),\n",
    "                               # 可以将micro_auc 改为其他的\n",
    "                               MaxSaver('micro_auc', save_name),\n",
    "                           ],\n",
    "                           session_init=SaverRestore(\n",
    "                               model_path=vgg_loc, ignore=ignore_restore),\n",
    "                           # 跑多少个 epoch\n",
    "                           max_epoch=2, nr_tower=2)\n",
    "Trainer(train_config).train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#np.set_printoptions(formatter={'float_kind': lambda x: '%.3f' % x})\n",
    "model = MaxModel(config)\n",
    "tf.reset_default_graph()\n",
    "pred_config = PredictConfig(model=model,\n",
    "                            session_init=SaverRestore(\n",
    "                                model_path=log_dir + save_name),\n",
    "                            output_names=['logits_export', 'label'],\n",
    "                            )\n",
    "pred = SimpleDatasetPredictor(pred_config, test_data)\n",
    "\n",
    "accumulator = seq(pred.get_result()) \\\n",
    "    .smap(lambda a, b: (a.shape[0], calcu_metrics(a, b, config.validation_metrics, threshold))) \\\n",
    "    .aggregate(Accumulator(*config.validation_metrics), lambda accu, args: accu.feed(args[0], *args[1]))\n",
    "metrics = accumulator.retrive()\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
