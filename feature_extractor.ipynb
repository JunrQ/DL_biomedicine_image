{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "\u001b[32m[1111 21:19:04 @logger.py:67]\u001b[0m Existing log file 'train_log/block3-d0.5-g2-512-hidden-ngls-r5/log.log' backuped to 'train_log/block3-d0.5-g2-512-hidden-ngls-r5/log.log.1111-211904'\n",
      "\u001b[32m[1111 21:19:04 @logger.py:74]\u001b[0m Argv: /home/yangyang/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py -f /run/user/1007/jupyter/kernel-ed3f6884-b96d-49d3-9366-9497aaaaa739.json\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from config.rnn import default\n",
    "from models import RNN\n",
    "import numpy as np\n",
    "from functional import seq\n",
    "import tensorflow as tf\n",
    "from tensorpack import (TrainConfig, AsyncMultiGPUTrainer as Trainer, \n",
    "                        PredictConfig, MultiProcessDatasetPredictor as Predictor,\n",
    "                        SaverRestore, logger)\n",
    "from tensorpack.callbacks import (ScheduledHyperParamSetter, MaxSaver, ModelSaver,\n",
    "                                  DataParallelInferenceRunner as InfRunner)\n",
    "from tensorpack.predict import SimpleDatasetPredictor\n",
    "from tensorpack.tfutils.common import get_default_sess_config\n",
    "from utils import DataManager\n",
    "from utils.validation import (Accumulator, AggregateMetric, calcu_metrics)\n",
    "\n",
    "resnet_loc = \"./data/resnet_v2_101/resnet_v2_101.ckpt\"\n",
    "log_dir = 'train_log/block3-d0.5-g2-512-hidden-ngls-r5/'\n",
    "logger.set_logger_dir(log_dir, action='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': (3357, 13543), 'train': (3428, 13712), 'val': (0, 0)}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = default\n",
    "config.proportion = {'train': 0.52, 'val': 0.0, 'test': 0.48}\n",
    "config.stages = [2, 3, 4, 5, 6]\n",
    "config.annotation_number = 10\n",
    "config.weight_decay = 0.0\n",
    "config.dropout_keep_prob = 0.5\n",
    "config.gamma = 2\n",
    "config.use_glimpse = True\n",
    "config.use_hidden = True\n",
    "config.read_time = 6\n",
    "config.batch_size = 64\n",
    "config.max_sequence_length = 15\n",
    "\n",
    "ignore_restore = ['learning_rate', 'global_step']\n",
    "save_name = \"max-micro_f1.tfmodel\"\n",
    "threshold = 0.4\n",
    "\n",
    "data_manager = DataManager.from_config(config)\n",
    "test_data = data_manager.get_test_stream()\n",
    "train_data = data_manager.get_train_stream()\n",
    "data_manager.get_num_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ignore_restore = ['learning_rate', 'global_step', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yangyang/Documents/flyexpress/DL_biomedicine_image/utils/data_manager/__init__.py:182: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  posi_ratio = np.sum(binary_annot, axis=0) / binary_annot.shape[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "\u001b[32m[1111 21:19:14 @collection.py:133]\u001b[0m New collections created in : tf.GraphKeys.MODEL_VARIABLES, resnet_v2_101_end_points\n",
      "\u001b[32m[1111 21:19:14 @collection.py:152]\u001b[0m These collections were modified but restored in : (tf.GraphKeys.SUMMARIES: 0->3), (tf.GraphKeys.UPDATE_OPS: 0->2)\n",
      "\u001b[32m[1111 21:19:14 @sessinit.py:89]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m The following variables are in the graph, but not found in the checkpoint: rnn/process/gate_att/weights, rnn/process/gate_att/biases\n",
      "\u001b[32m[1111 21:19:14 @sessinit.py:89]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m The following variables are in the checkpoint, but not found in the graph: beta1_power:0, beta2_power:0, global_step:0, learning_rate:0\n",
      "\u001b[32m[1111 21:19:18 @sessinit.py:116]\u001b[0m Restoring checkpoint from train_log/block3-d0.5-g2-512-hidden-ngls-r5/max-micro_f1.tfmodel ...\n",
      "INFO:tensorflow:Restoring parameters from train_log/block3-d0.5-g2-512-hidden-ngls-r5/max-micro_f1.tfmodel\n"
     ]
    }
   ],
   "source": [
    "#np.set_printoptions(formatter={'float_kind': lambda x: '%.3f' % x})\n",
    "model = RNN(config, label_weights=data_manager.get_imbalance_ratio().train.values)\n",
    "tf.reset_default_graph()\n",
    "pred_config = PredictConfig(model=model,\n",
    "                            session_init=SaverRestore(\n",
    "                                ignore=ignore_restore,\n",
    "                                model_path=log_dir + save_name),\n",
    "                            output_names=['feature', 'length', 'label'],\n",
    "                            )\n",
    "# pred = Predictor(pred_config, test_data, nr_proc=2, ordered=False)\n",
    "pred = SimpleDatasetPredictor(pred_config, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########|53/53[01:51<00:00, 0.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "features = []\n",
    "labels = []\n",
    "group_ctr = 0\n",
    "img_ctr = 0\n",
    "for feat_batch, length_batch, label_batch in pred.get_result():\n",
    "    for i in range(0, feat_batch.shape[0]):\n",
    "        group_feat = feat_batch[i, :, :]\n",
    "        length = length_batch[i]\n",
    "        label = label_batch[i]\n",
    "        group_ctr += 1\n",
    "        img_ctr += length\n",
    "        '''\n",
    "        for feat in group_feat[0:length]:\n",
    "            features.append(feat)\n",
    "            labels.append(label)\n",
    "        '''\n",
    "        features.append(group_feat[0:length])\n",
    "        labels.append(label)\n",
    "\n",
    "print(group_ctr)\n",
    "data_set = list(zip(iter(features), iter(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/data/yangyang/10_labels_test_bagged.pickle', 'wb') as f:\n",
    "    pickle.dump(data_set, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 512)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set[40][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
