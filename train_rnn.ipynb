{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1114 22:15:24 @logger.py:94]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Log directory train_log/sift-512-hidden-gls-r5/ exists! Please either backup/delete it, or use a new directory.\n",
      "\u001b[32m[1114 22:15:24 @logger.py:96]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m If you're resuming from a previous run you can choose to keep it.\n",
      "\u001b[32m[1114 22:15:24 @logger.py:97]\u001b[0m Select Action: k (keep) / b (backup) / d (delete) / n (new) / q (quit):\n",
      "k\n",
      "\u001b[32m[1114 22:21:45 @logger.py:67]\u001b[0m Existing log file 'train_log/sift-512-hidden-gls-r5/log.log' backuped to 'train_log/sift-512-hidden-gls-r5/log.log.1114-222145'\n",
      "\u001b[32m[1114 22:21:45 @logger.py:74]\u001b[0m Argv: /home/yangyang/Applications/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py -f /run/user/1007/jupyter/kernel-e7d475b0-1365-4815-b5b2-b6c4a45352cd.json\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from config.rnn import default\n",
    "from models import RNN\n",
    "import numpy as np\n",
    "from functional import seq\n",
    "import tensorflow as tf\n",
    "from tensorpack import (TrainConfig, SyncMultiGPUTrainer as Trainer, \n",
    "                        PredictConfig, MultiProcessDatasetPredictor as Predictor,\n",
    "                        SaverRestore, logger)\n",
    "from tensorpack.callbacks import (ScheduledHyperParamSetter, MaxSaver, ModelSaver,\n",
    "                                  DataParallelInferenceRunner as InfRunner)\n",
    "from tensorpack.predict import SimpleDatasetPredictor\n",
    "from tensorpack.tfutils.common import get_default_sess_config\n",
    "from utils import DataManager\n",
    "from utils.validation import (Accumulator, AggregateMetric, calcu_metrics)\n",
    "import pickle\n",
    "\n",
    "resnet_loc = \"./data/resnet_v2_101/resnet_v2_101.ckpt\"\n",
    "log_dir = 'train_log/sift-512-hidden-gls-r5/'\n",
    "feature_loc = '/data/yangyang/10_labels_feature.pickle'\n",
    "logger.set_logger_dir(log_dir)\n",
    "\n",
    "with open(feature_loc, 'rb') as f:\n",
    "    feature = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': (4278, 18731), 'val': (0, 0), 'test': (4183, 18548)}\n",
      "                                          train  val       test\n",
      "amnioserosa                           21.051546  NaN  21.489247\n",
      "anterior endoderm primordium          17.050633  NaN  17.427313\n",
      "anterior midgut primordium             8.527840  NaN   8.865566\n",
      "brain primordium                      12.667732  NaN  13.374570\n",
      "cellular blastoderm                    9.434146  NaN   9.670918\n",
      "dorsal prothoracic pharyngeal muscle  20.073892  NaN  19.915000\n",
      "embryonic brain                        9.042254  NaN   8.912322\n",
      "embryonic central nervous system      13.701031  NaN  13.780919\n",
      "embryonic dorsal epidermis            12.452830  NaN  12.237342\n",
      "embryonic foregut                     21.877005  NaN  21.132275\n",
      "embryonic head epidermis              18.990654  NaN  18.365741\n",
      "embryonic hindgut                     10.530997  NaN  10.951429\n",
      "embryonic midgut                       6.082781  NaN   6.077834\n",
      "embryonic ventral epidermis           14.170213  NaN  13.886121\n",
      "embryonic/larval muscle system        12.980392  NaN  12.943333\n",
      "faint ubiquitous                       4.148014  NaN   4.164198\n",
      "foregut primordium                    19.274882  NaN  20.561856\n",
      "head mesoderm primordium P2           16.390244  NaN  16.800000\n",
      "head mesoderm primordium P4           20.938462  NaN  20.451282\n",
      "hindgut proper primordium             15.842520  NaN  16.502092\n",
      "posterior endoderm primordium         15.976190  NaN  16.649789\n",
      "posterior endoderm primordium P2      21.051546  NaN  20.561856\n",
      "posterior midgut primordium            8.121535  NaN   8.421171\n",
      "procephalic ectoderm primordium       22.124324  NaN  22.632768\n",
      "trunk mesoderm primordium              6.750000  NaN   7.028791\n",
      "trunk mesoderm primordium P2          19.179245  NaN  18.919048\n",
      "ubiquitous                             1.677096  NaN   1.601368\n",
      "ventral nerve cord                     8.767123  NaN   8.616092\n",
      "ventral nerve cord primordium         14.116608  NaN  14.608209\n",
      "yolk nuclei                           12.163077  NaN  12.581169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yangyang/Documents/flyexpress/DL_biomedicine_image/utils/data_manager/__init__.py:182: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  posi_ratio = np.sum(binary_annot, axis=0) / binary_annot.shape[0]\n"
     ]
    }
   ],
   "source": [
    "config = default\n",
    "config.proportion = {'train': 0.52, 'val': 0.00, 'test': 0.48}\n",
    "config.stages = [2, 3, 4, 5, 6]\n",
    "config.annotation_number = 30\n",
    "config.weight_decay = 0.0\n",
    "config.dropout_keep_prob = 0.5\n",
    "config.doubly_stochastic_lambda = 0.5\n",
    "config.gamma = 0\n",
    "config.use_glimpse = True\n",
    "config.use_hidden = True\n",
    "config.read_time = 5\n",
    "config.batch_size = 128\n",
    "config.use_foreign = True\n",
    "\n",
    "ignore_restore = ['learning_rate', 'global_step']\n",
    "save_name = \"max-micro_f1.tfmodel\"\n",
    "threshold = 0.5\n",
    "\n",
    "data_manager = DataManager.from_config(config)\n",
    "data_manager.override_feature(feature)\n",
    "print(data_manager.get_num_info())\n",
    "print(data_manager.get_imbalance_ratio())\n",
    "train_data = data_manager.get_train_stream()\n",
    "#val_data = data_manager.get_validation_stream()\n",
    "test_data = data_manager.get_test_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = RNN(config, label_weights=data_manager.get_imbalance_ratio().train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:39:19 @inference_runner.py:82]\u001b[0m InferenceRunner will eval on an InputSource of size 5\n",
      "\u001b[32m[1112 12:39:19 @input_source.py:180]\u001b[0m Setting up the queue 'QueueInput/input_queue' for CPU prefetching ...\n",
      "\u001b[32m[1112 12:39:19 @training.py:90]\u001b[0m Building graph for training tower 0 on device LeastLoadedDeviceSetter-/gpu:1...\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "\u001b[32m[1112 12:39:20 @model_utils.py:47]\u001b[0m \u001b[36mModel Parameters: \n",
      "\u001b[0mname                              shape           dim\n",
      "--------------------------------  -----------  ------\n",
      "rnn/process/logits_att/weights:0  [1024, 1]      1024\n",
      "rnn/process/logits_att/biases:0   [1]               1\n",
      "rnn/process/gate_att/weights:0    [512, 1]        512\n",
      "rnn/process/gate_att/biases:0     [1]               1\n",
      "rnn/process/fc_fg/weights:0       [1024, 512]  524288\n",
      "rnn/process/fc_fg/biases:0        [512]           512\n",
      "rnn/process/fc_ig/weights:0       [1024, 512]  524288\n",
      "rnn/process/fc_ig/biases:0        [512]           512\n",
      "logits/weights:0                  [512, 10]      5120\n",
      "logits/biases:0                   [10]             10\u001b[36m\n",
      "Total #vars=10, #param=1056268 (4.03 MB assuming all float32)\u001b[0m\n",
      "\u001b[32m[1112 12:39:20 @base.py:207]\u001b[0m Setup callbacks graph ...\n",
      "\u001b[32m[1112 12:39:20 @input_source.py:180]\u001b[0m Setting up the queue 'DataParallelInferenceRunner/QueueInput/input_queue' for CPU prefetching ...\n",
      "\u001b[32m[1112 12:39:20 @predict.py:51]\u001b[0m Building predictor tower 'InferenceTower0' on device /gpu:1 ...\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "\u001b[32m[1112 12:39:21 @collection.py:152]\u001b[0m These collections were modified but restored in InferenceTower0: (tf.GraphKeys.SUMMARIES: 7->10), (tf.GraphKeys.UPDATE_OPS: 2->4)\n",
      "\u001b[32m[1112 12:39:21 @summary.py:34]\u001b[0m Maintain moving average summary of 1 tensors.\n",
      "\u001b[32m[1112 12:39:21 @graph.py:90]\u001b[0m Applying collection UPDATE_OPS of 2 ops.\n",
      "\u001b[32m[1112 12:39:22 @base.py:212]\u001b[0m Creating the session ...\n",
      "\u001b[32m[1112 12:39:22 @base.py:216]\u001b[0m Initializing the session ...\n",
      "\u001b[32m[1112 12:39:22 @base.py:223]\u001b[0m Graph Finalized.\n",
      "\u001b[32m[1112 12:39:22 @param.py:144]\u001b[0m After epoch 0, learning_rate will change to 0.00100000\n",
      "\u001b[32m[1112 12:39:22 @concurrency.py:36]\u001b[0m Starting EnqueueThread DataParallelInferenceRunner/QueueInput/input_queue ...\n",
      "\u001b[32m[1112 12:39:22 @concurrency.py:36]\u001b[0m Starting EnqueueThread QueueInput/input_queue ...\n",
      "\u001b[32m[1112 12:39:22 @base.py:257]\u001b[0m Start Epoch 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########|23/23[00:02<00:00, 7.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:39:25 @base.py:267]\u001b[0m Epoch 1 (global_step 23) finished, time:2.98 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:01<00:00, 4.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:39:27 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-23.\n",
      "\u001b[32m[1112 12:39:27 @saver.py:159]\u001b[0m Model with maximum 'micro_f1' saved.\n",
      "\u001b[32m[1112 12:39:27 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 22\n",
      "\u001b[32m[1112 12:39:27 @monitor.py:362]\u001b[0m QueueInput/queue_size: 0.10396\n",
      "\u001b[32m[1112 12:39:27 @monitor.py:362]\u001b[0m coverage: 5.2923\n",
      "\u001b[32m[1112 12:39:27 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 2.2009\n",
      "\u001b[32m[1112 12:39:27 @monitor.py:362]\u001b[0m learning_rate: 0.001\n",
      "\u001b[32m[1112 12:39:27 @monitor.py:362]\u001b[0m loss/value: 16.589\n",
      "\u001b[32m[1112 12:39:27 @monitor.py:362]\u001b[0m macro_auc: 0.5316\n",
      "\u001b[32m[1112 12:39:27 @monitor.py:362]\u001b[0m macro_f1: 0.25981\n",
      "\u001b[32m[1112 12:39:27 @monitor.py:362]\u001b[0m mean_average_precision: 0.21665\n",
      "\u001b[32m[1112 12:39:27 @monitor.py:362]\u001b[0m micro_auc: 0.54085\n",
      "\u001b[32m[1112 12:39:27 @monitor.py:362]\u001b[0m micro_f1: 0.27998\n",
      "\u001b[32m[1112 12:39:27 @monitor.py:362]\u001b[0m one_error: 0.8164\n",
      "\u001b[32m[1112 12:39:27 @monitor.py:362]\u001b[0m ranking_loss: 0.46453\n",
      "\u001b[32m[1112 12:39:27 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.37101\n",
      "\u001b[32m[1112 12:39:27 @monitor.py:362]\u001b[0m training_ap: 0.18984\n",
      "\u001b[32m[1112 12:39:27 @monitor.py:362]\u001b[0m training_auc: 0.51442\n",
      "\u001b[32m[1112 12:39:27 @base.py:257]\u001b[0m Start Epoch 2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,13.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:39:28 @base.py:267]\u001b[0m Epoch 2 (global_step 46) finished, time:1.71 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:01<00:00, 4.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:39:29 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-46.\n",
      "\u001b[32m[1112 12:39:29 @saver.py:159]\u001b[0m Model with maximum 'micro_f1' saved.\n",
      "\u001b[32m[1112 12:39:29 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 25.868\n",
      "\u001b[32m[1112 12:39:29 @monitor.py:362]\u001b[0m QueueInput/queue_size: 1.5384\n",
      "\u001b[32m[1112 12:39:29 @monitor.py:362]\u001b[0m coverage: 5.0695\n",
      "\u001b[32m[1112 12:39:29 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 1.5263\n",
      "\u001b[32m[1112 12:39:30 @monitor.py:362]\u001b[0m learning_rate: 0.001\n",
      "\u001b[32m[1112 12:39:30 @monitor.py:362]\u001b[0m loss/value: 15.72\n",
      "\u001b[32m[1112 12:39:30 @monitor.py:362]\u001b[0m macro_auc: 0.55399\n",
      "\u001b[32m[1112 12:39:30 @monitor.py:362]\u001b[0m macro_f1: 0.26322\n",
      "\u001b[32m[1112 12:39:30 @monitor.py:362]\u001b[0m mean_average_precision: 0.2303\n",
      "\u001b[32m[1112 12:39:30 @monitor.py:362]\u001b[0m micro_auc: 0.55819\n",
      "\u001b[32m[1112 12:39:30 @monitor.py:362]\u001b[0m micro_f1: 0.28554\n",
      "\u001b[32m[1112 12:39:30 @monitor.py:362]\u001b[0m one_error: 0.79679\n",
      "\u001b[32m[1112 12:39:30 @monitor.py:362]\u001b[0m ranking_loss: 0.44512\n",
      "\u001b[32m[1112 12:39:30 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.38575\n",
      "\u001b[32m[1112 12:39:30 @monitor.py:362]\u001b[0m training_ap: 0.19471\n",
      "\u001b[32m[1112 12:39:30 @monitor.py:362]\u001b[0m training_auc: 0.52319\n",
      "\u001b[32m[1112 12:39:30 @base.py:257]\u001b[0m Start Epoch 3 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:39:31 @base.py:267]\u001b[0m Epoch 3 (global_step 69) finished, time:1.50 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 6.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:39:32 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-69.\n",
      "\u001b[32m[1112 12:39:32 @saver.py:159]\u001b[0m Model with maximum 'micro_f1' saved.\n",
      "\u001b[32m[1112 12:39:32 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 35.145\n",
      "\u001b[32m[1112 12:39:32 @monitor.py:362]\u001b[0m QueueInput/queue_size: 4.4666\n",
      "\u001b[32m[1112 12:39:32 @monitor.py:362]\u001b[0m coverage: 4.8075\n",
      "\u001b[32m[1112 12:39:32 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 1.5848\n",
      "\u001b[32m[1112 12:39:32 @monitor.py:362]\u001b[0m learning_rate: 0.001\n",
      "\u001b[32m[1112 12:39:32 @monitor.py:362]\u001b[0m loss/value: 14.784\n",
      "\u001b[32m[1112 12:39:32 @monitor.py:362]\u001b[0m macro_auc: 0.57669\n",
      "\u001b[32m[1112 12:39:32 @monitor.py:362]\u001b[0m macro_f1: 0.27672\n",
      "\u001b[32m[1112 12:39:32 @monitor.py:362]\u001b[0m mean_average_precision: 0.2431\n",
      "\u001b[32m[1112 12:39:32 @monitor.py:362]\u001b[0m micro_auc: 0.58429\n",
      "\u001b[32m[1112 12:39:32 @monitor.py:362]\u001b[0m micro_f1: 0.30125\n",
      "\u001b[32m[1112 12:39:32 @monitor.py:362]\u001b[0m one_error: 0.76292\n",
      "\u001b[32m[1112 12:39:32 @monitor.py:362]\u001b[0m ranking_loss: 0.41445\n",
      "\u001b[32m[1112 12:39:32 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.41645\n",
      "\u001b[32m[1112 12:39:32 @monitor.py:362]\u001b[0m training_ap: 0.1983\n",
      "\u001b[32m[1112 12:39:32 @monitor.py:362]\u001b[0m training_auc: 0.53126\n",
      "\u001b[32m[1112 12:39:32 @base.py:257]\u001b[0m Start Epoch 4 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:39:34 @base.py:267]\u001b[0m Epoch 4 (global_step 92) finished, time:1.55 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 5.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:39:34 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-92.\n",
      "\u001b[32m[1112 12:39:34 @saver.py:159]\u001b[0m Model with maximum 'micro_f1' saved.\n",
      "\u001b[32m[1112 12:39:34 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 41.393\n",
      "\u001b[32m[1112 12:39:34 @monitor.py:362]\u001b[0m QueueInput/queue_size: 13.4\n",
      "\u001b[32m[1112 12:39:34 @monitor.py:362]\u001b[0m coverage: 4.6952\n",
      "\u001b[32m[1112 12:39:34 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.35819\n",
      "\u001b[32m[1112 12:39:34 @monitor.py:362]\u001b[0m learning_rate: 0.001\n",
      "\u001b[32m[1112 12:39:34 @monitor.py:362]\u001b[0m loss/value: 13.923\n",
      "\u001b[32m[1112 12:39:34 @monitor.py:362]\u001b[0m macro_auc: 0.6001\n",
      "\u001b[32m[1112 12:39:34 @monitor.py:362]\u001b[0m macro_f1: 0.28868\n",
      "\u001b[32m[1112 12:39:34 @monitor.py:362]\u001b[0m mean_average_precision: 0.26525\n",
      "\u001b[32m[1112 12:39:34 @monitor.py:362]\u001b[0m micro_auc: 0.60312\n",
      "\u001b[32m[1112 12:39:34 @monitor.py:362]\u001b[0m micro_f1: 0.3142\n",
      "\u001b[32m[1112 12:39:34 @monitor.py:362]\u001b[0m one_error: 0.73262\n",
      "\u001b[32m[1112 12:39:34 @monitor.py:362]\u001b[0m ranking_loss: 0.39982\n",
      "\u001b[32m[1112 12:39:34 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.42854\n",
      "\u001b[32m[1112 12:39:34 @monitor.py:362]\u001b[0m training_ap: 0.20279\n",
      "\u001b[32m[1112 12:39:34 @monitor.py:362]\u001b[0m training_auc: 0.54023\n",
      "\u001b[32m[1112 12:39:34 @base.py:257]\u001b[0m Start Epoch 5 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:39:36 @base.py:267]\u001b[0m Epoch 5 (global_step 115) finished, time:1.49 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 6.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:39:37 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-115.\n",
      "\u001b[32m[1112 12:39:37 @saver.py:159]\u001b[0m Model with maximum 'micro_f1' saved.\n",
      "\u001b[32m[1112 12:39:37 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 44.548\n",
      "\u001b[32m[1112 12:39:37 @monitor.py:362]\u001b[0m QueueInput/queue_size: 24.395\n",
      "\u001b[32m[1112 12:39:37 @monitor.py:362]\u001b[0m coverage: 4.5882\n",
      "\u001b[32m[1112 12:39:37 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 1.9687\n",
      "\u001b[32m[1112 12:39:37 @monitor.py:362]\u001b[0m learning_rate: 0.001\n",
      "\u001b[32m[1112 12:39:37 @monitor.py:362]\u001b[0m loss/value: 13.495\n",
      "\u001b[32m[1112 12:39:37 @monitor.py:362]\u001b[0m macro_auc: 0.61991\n",
      "\u001b[32m[1112 12:39:37 @monitor.py:362]\u001b[0m macro_f1: 0.30246\n",
      "\u001b[32m[1112 12:39:37 @monitor.py:362]\u001b[0m mean_average_precision: 0.28015\n",
      "\u001b[32m[1112 12:39:37 @monitor.py:362]\u001b[0m micro_auc: 0.62474\n",
      "\u001b[32m[1112 12:39:37 @monitor.py:362]\u001b[0m micro_f1: 0.32676\n",
      "\u001b[32m[1112 12:39:37 @monitor.py:362]\u001b[0m one_error: 0.72193\n",
      "\u001b[32m[1112 12:39:37 @monitor.py:362]\u001b[0m ranking_loss: 0.38738\n",
      "\u001b[32m[1112 12:39:37 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.43639\n",
      "\u001b[32m[1112 12:39:37 @monitor.py:362]\u001b[0m training_ap: 0.20632\n",
      "\u001b[32m[1112 12:39:37 @monitor.py:362]\u001b[0m training_auc: 0.54812\n",
      "\u001b[32m[1112 12:39:37 @base.py:257]\u001b[0m Start Epoch 6 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:39:39 @base.py:267]\u001b[0m Epoch 6 (global_step 138) finished, time:1.61 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 6.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:39:39 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-138.\n",
      "\u001b[32m[1112 12:39:39 @saver.py:159]\u001b[0m Model with maximum 'micro_f1' saved.\n",
      "\u001b[32m[1112 12:39:39 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 46.308\n",
      "\u001b[32m[1112 12:39:39 @monitor.py:362]\u001b[0m QueueInput/queue_size: 35.145\n",
      "\u001b[32m[1112 12:39:39 @monitor.py:362]\u001b[0m coverage: 4.4225\n",
      "\u001b[32m[1112 12:39:39 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.59132\n",
      "\u001b[32m[1112 12:39:39 @monitor.py:362]\u001b[0m learning_rate: 0.001\n",
      "\u001b[32m[1112 12:39:39 @monitor.py:362]\u001b[0m loss/value: 13.046\n",
      "\u001b[32m[1112 12:39:39 @monitor.py:362]\u001b[0m macro_auc: 0.64003\n",
      "\u001b[32m[1112 12:39:39 @monitor.py:362]\u001b[0m macro_f1: 0.31295\n",
      "\u001b[32m[1112 12:39:39 @monitor.py:362]\u001b[0m mean_average_precision: 0.29261\n",
      "\u001b[32m[1112 12:39:39 @monitor.py:362]\u001b[0m micro_auc: 0.63834\n",
      "\u001b[32m[1112 12:39:39 @monitor.py:362]\u001b[0m micro_f1: 0.33725\n",
      "\u001b[32m[1112 12:39:39 @monitor.py:362]\u001b[0m one_error: 0.70232\n",
      "\u001b[32m[1112 12:39:39 @monitor.py:362]\u001b[0m ranking_loss: 0.37079\n",
      "\u001b[32m[1112 12:39:39 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.44833\n",
      "\u001b[32m[1112 12:39:39 @monitor.py:362]\u001b[0m training_ap: 0.2098\n",
      "\u001b[32m[1112 12:39:39 @monitor.py:362]\u001b[0m training_auc: 0.55451\n",
      "\u001b[32m[1112 12:39:39 @base.py:257]\u001b[0m Start Epoch 7 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:39:41 @base.py:267]\u001b[0m Epoch 7 (global_step 161) finished, time:1.57 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 8.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:39:42 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-161.\n",
      "\u001b[32m[1112 12:39:42 @saver.py:159]\u001b[0m Model with maximum 'micro_f1' saved.\n",
      "\u001b[32m[1112 12:39:42 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 47.459\n",
      "\u001b[32m[1112 12:39:42 @monitor.py:362]\u001b[0m QueueInput/queue_size: 44.134\n",
      "\u001b[32m[1112 12:39:42 @monitor.py:362]\u001b[0m coverage: 4.2246\n",
      "\u001b[32m[1112 12:39:42 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.39106\n",
      "\u001b[32m[1112 12:39:42 @monitor.py:362]\u001b[0m learning_rate: 0.001\n",
      "\u001b[32m[1112 12:39:42 @monitor.py:362]\u001b[0m loss/value: 12.58\n",
      "\u001b[32m[1112 12:39:42 @monitor.py:362]\u001b[0m macro_auc: 0.65208\n",
      "\u001b[32m[1112 12:39:42 @monitor.py:362]\u001b[0m macro_f1: 0.31809\n",
      "\u001b[32m[1112 12:39:42 @monitor.py:362]\u001b[0m mean_average_precision: 0.30122\n",
      "\u001b[32m[1112 12:39:42 @monitor.py:362]\u001b[0m micro_auc: 0.65205\n",
      "\u001b[32m[1112 12:39:42 @monitor.py:362]\u001b[0m micro_f1: 0.34305\n",
      "\u001b[32m[1112 12:39:42 @monitor.py:362]\u001b[0m one_error: 0.69875\n",
      "\u001b[32m[1112 12:39:42 @monitor.py:362]\u001b[0m ranking_loss: 0.3535\n",
      "\u001b[32m[1112 12:39:42 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.46443\n",
      "\u001b[32m[1112 12:39:42 @monitor.py:362]\u001b[0m training_ap: 0.2136\n",
      "\u001b[32m[1112 12:39:42 @monitor.py:362]\u001b[0m training_auc: 0.56149\n",
      "\u001b[32m[1112 12:39:42 @base.py:257]\u001b[0m Start Epoch 8 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:39:43 @base.py:267]\u001b[0m Epoch 8 (global_step 184) finished, time:1.53 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,11.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:39:44 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-184.\n",
      "\u001b[32m[1112 12:39:44 @saver.py:159]\u001b[0m Model with maximum 'micro_f1' saved.\n",
      "\u001b[32m[1112 12:39:44 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 48.17\n",
      "\u001b[32m[1112 12:39:44 @monitor.py:362]\u001b[0m QueueInput/queue_size: 48.008\n",
      "\u001b[32m[1112 12:39:44 @monitor.py:362]\u001b[0m coverage: 4.0856\n",
      "\u001b[32m[1112 12:39:44 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.49059\n",
      "\u001b[32m[1112 12:39:44 @monitor.py:362]\u001b[0m learning_rate: 0.001\n",
      "\u001b[32m[1112 12:39:44 @monitor.py:362]\u001b[0m loss/value: 12.257\n",
      "\u001b[32m[1112 12:39:44 @monitor.py:362]\u001b[0m macro_auc: 0.67736\n",
      "\u001b[32m[1112 12:39:44 @monitor.py:362]\u001b[0m macro_f1: 0.33577\n",
      "\u001b[32m[1112 12:39:44 @monitor.py:362]\u001b[0m mean_average_precision: 0.3298\n",
      "\u001b[32m[1112 12:39:44 @monitor.py:362]\u001b[0m micro_auc: 0.67397\n",
      "\u001b[32m[1112 12:39:44 @monitor.py:362]\u001b[0m micro_f1: 0.35945\n",
      "\u001b[32m[1112 12:39:44 @monitor.py:362]\u001b[0m one_error: 0.65597\n",
      "\u001b[32m[1112 12:39:44 @monitor.py:362]\u001b[0m ranking_loss: 0.3347\n",
      "\u001b[32m[1112 12:39:44 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.49144\n",
      "\u001b[32m[1112 12:39:44 @monitor.py:362]\u001b[0m training_ap: 0.21732\n",
      "\u001b[32m[1112 12:39:44 @monitor.py:362]\u001b[0m training_auc: 0.56899\n",
      "\u001b[32m[1112 12:39:44 @base.py:257]\u001b[0m Start Epoch 9 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:39:45 @base.py:267]\u001b[0m Epoch 9 (global_step 207) finished, time:1.58 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:39:46 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-207.\n",
      "\u001b[32m[1112 12:39:46 @saver.py:159]\u001b[0m Model with maximum 'micro_f1' saved.\n",
      "\u001b[32m[1112 12:39:46 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 48.697\n",
      "\u001b[32m[1112 12:39:46 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.194\n",
      "\u001b[32m[1112 12:39:46 @monitor.py:362]\u001b[0m coverage: 3.8681\n",
      "\u001b[32m[1112 12:39:46 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.94187\n",
      "\u001b[32m[1112 12:39:46 @monitor.py:362]\u001b[0m learning_rate: 0.001\n",
      "\u001b[32m[1112 12:39:46 @monitor.py:362]\u001b[0m loss/value: 12.083\n",
      "\u001b[32m[1112 12:39:46 @monitor.py:362]\u001b[0m macro_auc: 0.69039\n",
      "\u001b[32m[1112 12:39:46 @monitor.py:362]\u001b[0m macro_f1: 0.34531\n",
      "\u001b[32m[1112 12:39:46 @monitor.py:362]\u001b[0m mean_average_precision: 0.33858\n",
      "\u001b[32m[1112 12:39:46 @monitor.py:362]\u001b[0m micro_auc: 0.68944\n",
      "\u001b[32m[1112 12:39:46 @monitor.py:362]\u001b[0m micro_f1: 0.36936\n",
      "\u001b[32m[1112 12:39:46 @monitor.py:362]\u001b[0m one_error: 0.64349\n",
      "\u001b[32m[1112 12:39:46 @monitor.py:362]\u001b[0m ranking_loss: 0.31304\n",
      "\u001b[32m[1112 12:39:46 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.50586\n",
      "\u001b[32m[1112 12:39:46 @monitor.py:362]\u001b[0m training_ap: 0.22065\n",
      "\u001b[32m[1112 12:39:46 @monitor.py:362]\u001b[0m training_auc: 0.57535\n",
      "\u001b[32m[1112 12:39:46 @base.py:257]\u001b[0m Start Epoch 10 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:39:47 @base.py:267]\u001b[0m Epoch 10 (global_step 230) finished, time:1.56 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:39:48 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-230.\n",
      "\u001b[32m[1112 12:39:48 @saver.py:159]\u001b[0m Model with maximum 'micro_f1' saved.\n",
      "\u001b[32m[1112 12:39:48 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.021\n",
      "\u001b[32m[1112 12:39:48 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.69\n",
      "\u001b[32m[1112 12:39:48 @monitor.py:362]\u001b[0m coverage: 3.6506\n",
      "\u001b[32m[1112 12:39:48 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 2.2305\n",
      "\u001b[32m[1112 12:39:48 @monitor.py:362]\u001b[0m learning_rate: 0.001\n",
      "\u001b[32m[1112 12:39:48 @monitor.py:362]\u001b[0m loss/value: 11.947\n",
      "\u001b[32m[1112 12:39:48 @monitor.py:362]\u001b[0m macro_auc: 0.71878\n",
      "\u001b[32m[1112 12:39:48 @monitor.py:362]\u001b[0m macro_f1: 0.36459\n",
      "\u001b[32m[1112 12:39:48 @monitor.py:362]\u001b[0m mean_average_precision: 0.37261\n",
      "\u001b[32m[1112 12:39:48 @monitor.py:362]\u001b[0m micro_auc: 0.7151\n",
      "\u001b[32m[1112 12:39:48 @monitor.py:362]\u001b[0m micro_f1: 0.38831\n",
      "\u001b[32m[1112 12:39:48 @monitor.py:362]\u001b[0m one_error: 0.59537\n",
      "\u001b[32m[1112 12:39:48 @monitor.py:362]\u001b[0m ranking_loss: 0.29091\n",
      "\u001b[32m[1112 12:39:48 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.53566\n",
      "\u001b[32m[1112 12:39:48 @monitor.py:362]\u001b[0m training_ap: 0.22451\n",
      "\u001b[32m[1112 12:39:48 @monitor.py:362]\u001b[0m training_auc: 0.58215\n",
      "\u001b[32m[1112 12:39:48 @base.py:257]\u001b[0m Start Epoch 11 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:39:49 @base.py:267]\u001b[0m Epoch 11 (global_step 253) finished, time:1.56 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:39:50 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-253.\n",
      "\u001b[32m[1112 12:39:50 @saver.py:159]\u001b[0m Model with maximum 'micro_f1' saved.\n",
      "\u001b[32m[1112 12:39:50 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.245\n",
      "\u001b[32m[1112 12:39:50 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.829\n",
      "\u001b[32m[1112 12:39:50 @monitor.py:362]\u001b[0m coverage: 3.7308\n",
      "\u001b[32m[1112 12:39:50 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.94944\n",
      "\u001b[32m[1112 12:39:50 @monitor.py:362]\u001b[0m learning_rate: 0.001\n",
      "\u001b[32m[1112 12:39:50 @monitor.py:362]\u001b[0m loss/value: 11.832\n",
      "\u001b[32m[1112 12:39:50 @monitor.py:362]\u001b[0m macro_auc: 0.71687\n",
      "\u001b[32m[1112 12:39:50 @monitor.py:362]\u001b[0m macro_f1: 0.3664\n",
      "\u001b[32m[1112 12:39:50 @monitor.py:362]\u001b[0m mean_average_precision: 0.39021\n",
      "\u001b[32m[1112 12:39:50 @monitor.py:362]\u001b[0m micro_auc: 0.71479\n",
      "\u001b[32m[1112 12:39:50 @monitor.py:362]\u001b[0m micro_f1: 0.39206\n",
      "\u001b[32m[1112 12:39:50 @monitor.py:362]\u001b[0m one_error: 0.6025\n",
      "\u001b[32m[1112 12:39:50 @monitor.py:362]\u001b[0m ranking_loss: 0.29698\n",
      "\u001b[32m[1112 12:39:50 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.53163\n",
      "\u001b[32m[1112 12:39:50 @monitor.py:362]\u001b[0m training_ap: 0.22824\n",
      "\u001b[32m[1112 12:39:50 @monitor.py:362]\u001b[0m training_auc: 0.58795\n",
      "\u001b[32m[1112 12:39:50 @base.py:257]\u001b[0m Start Epoch 12 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:39:52 @base.py:267]\u001b[0m Epoch 12 (global_step 276) finished, time:1.52 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,11.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:39:52 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-276.\n",
      "\u001b[32m[1112 12:39:52 @saver.py:159]\u001b[0m Model with maximum 'micro_f1' saved.\n",
      "\u001b[32m[1112 12:39:52 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.452\n",
      "\u001b[32m[1112 12:39:52 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.813\n",
      "\u001b[32m[1112 12:39:52 @monitor.py:362]\u001b[0m coverage: 3.5989\n",
      "\u001b[32m[1112 12:39:52 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.44303\n",
      "\u001b[32m[1112 12:39:52 @monitor.py:362]\u001b[0m learning_rate: 0.001\n",
      "\u001b[32m[1112 12:39:52 @monitor.py:362]\u001b[0m loss/value: 11.468\n",
      "\u001b[32m[1112 12:39:52 @monitor.py:362]\u001b[0m macro_auc: 0.72541\n",
      "\u001b[32m[1112 12:39:52 @monitor.py:362]\u001b[0m macro_f1: 0.37237\n",
      "\u001b[32m[1112 12:39:52 @monitor.py:362]\u001b[0m mean_average_precision: 0.37732\n",
      "\u001b[32m[1112 12:39:52 @monitor.py:362]\u001b[0m micro_auc: 0.72178\n",
      "\u001b[32m[1112 12:39:52 @monitor.py:362]\u001b[0m micro_f1: 0.39817\n",
      "\u001b[32m[1112 12:39:52 @monitor.py:362]\u001b[0m one_error: 0.61141\n",
      "\u001b[32m[1112 12:39:52 @monitor.py:362]\u001b[0m ranking_loss: 0.28949\n",
      "\u001b[32m[1112 12:39:52 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.53447\n",
      "\u001b[32m[1112 12:39:52 @monitor.py:362]\u001b[0m training_ap: 0.23197\n",
      "\u001b[32m[1112 12:39:52 @monitor.py:362]\u001b[0m training_auc: 0.59411\n",
      "\u001b[32m[1112 12:39:52 @base.py:257]\u001b[0m Start Epoch 13 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:39:54 @base.py:267]\u001b[0m Epoch 13 (global_step 299) finished, time:1.53 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,11.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:39:54 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-299.\n",
      "\u001b[32m[1112 12:39:54 @saver.py:159]\u001b[0m Model with maximum 'micro_f1' saved.\n",
      "\u001b[32m[1112 12:39:54 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.6\n",
      "\u001b[32m[1112 12:39:54 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.889\n",
      "\u001b[32m[1112 12:39:54 @monitor.py:362]\u001b[0m coverage: 3.4332\n",
      "\u001b[32m[1112 12:39:54 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.63002\n",
      "\u001b[32m[1112 12:39:54 @monitor.py:362]\u001b[0m learning_rate: 0.001\n",
      "\u001b[32m[1112 12:39:54 @monitor.py:362]\u001b[0m loss/value: 11.433\n",
      "\u001b[32m[1112 12:39:54 @monitor.py:362]\u001b[0m macro_auc: 0.75374\n",
      "\u001b[32m[1112 12:39:54 @monitor.py:362]\u001b[0m macro_f1: 0.3907\n",
      "\u001b[32m[1112 12:39:54 @monitor.py:362]\u001b[0m mean_average_precision: 0.41426\n",
      "\u001b[32m[1112 12:39:54 @monitor.py:362]\u001b[0m micro_auc: 0.7438\n",
      "\u001b[32m[1112 12:39:54 @monitor.py:362]\u001b[0m micro_f1: 0.41666\n",
      "\u001b[32m[1112 12:39:54 @monitor.py:362]\u001b[0m one_error: 0.58467\n",
      "\u001b[32m[1112 12:39:54 @monitor.py:362]\u001b[0m ranking_loss: 0.2697\n",
      "\u001b[32m[1112 12:39:54 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.55749\n",
      "\u001b[32m[1112 12:39:54 @monitor.py:362]\u001b[0m training_ap: 0.23623\n",
      "\u001b[32m[1112 12:39:54 @monitor.py:362]\u001b[0m training_auc: 0.60041\n",
      "\u001b[32m[1112 12:39:54 @base.py:257]\u001b[0m Start Epoch 14 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:39:56 @base.py:267]\u001b[0m Epoch 14 (global_step 322) finished, time:1.54 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:39:56 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-322.\n",
      "\u001b[32m[1112 12:39:56 @saver.py:159]\u001b[0m Model with maximum 'micro_f1' saved.\n",
      "\u001b[32m[1112 12:39:56 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.708\n",
      "\u001b[32m[1112 12:39:56 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.865\n",
      "\u001b[32m[1112 12:39:56 @monitor.py:362]\u001b[0m coverage: 3.3583\n",
      "\u001b[32m[1112 12:39:56 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.3122\n",
      "\u001b[32m[1112 12:39:56 @monitor.py:362]\u001b[0m learning_rate: 0.001\n",
      "\u001b[32m[1112 12:39:56 @monitor.py:362]\u001b[0m loss/value: 11.265\n",
      "\u001b[32m[1112 12:39:56 @monitor.py:362]\u001b[0m macro_auc: 0.7638\n",
      "\u001b[32m[1112 12:39:56 @monitor.py:362]\u001b[0m macro_f1: 0.41018\n",
      "\u001b[32m[1112 12:39:56 @monitor.py:362]\u001b[0m mean_average_precision: 0.43552\n",
      "\u001b[32m[1112 12:39:56 @monitor.py:362]\u001b[0m micro_auc: 0.75627\n",
      "\u001b[32m[1112 12:39:56 @monitor.py:362]\u001b[0m micro_f1: 0.43331\n",
      "\u001b[32m[1112 12:39:56 @monitor.py:362]\u001b[0m one_error: 0.55437\n",
      "\u001b[32m[1112 12:39:56 @monitor.py:362]\u001b[0m ranking_loss: 0.25773\n",
      "\u001b[32m[1112 12:39:56 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.57626\n",
      "\u001b[32m[1112 12:39:56 @monitor.py:362]\u001b[0m training_ap: 0.24036\n",
      "\u001b[32m[1112 12:39:56 @monitor.py:362]\u001b[0m training_auc: 0.6064\n",
      "\u001b[32m[1112 12:39:56 @base.py:257]\u001b[0m Start Epoch 15 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:39:58 @base.py:267]\u001b[0m Epoch 15 (global_step 345) finished, time:1.50 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:39:58 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-345.\n",
      "\u001b[32m[1112 12:39:58 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.786\n",
      "\u001b[32m[1112 12:39:58 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.864\n",
      "\u001b[32m[1112 12:39:58 @monitor.py:362]\u001b[0m coverage: 3.2585\n",
      "\u001b[32m[1112 12:39:58 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.40232\n",
      "\u001b[32m[1112 12:39:58 @monitor.py:362]\u001b[0m learning_rate: 0.001\n",
      "\u001b[32m[1112 12:39:58 @monitor.py:362]\u001b[0m loss/value: 11.134\n",
      "\u001b[32m[1112 12:39:58 @monitor.py:362]\u001b[0m macro_auc: 0.76047\n",
      "\u001b[32m[1112 12:39:58 @monitor.py:362]\u001b[0m macro_f1: 0.40183\n",
      "\u001b[32m[1112 12:39:58 @monitor.py:362]\u001b[0m mean_average_precision: 0.43673\n",
      "\u001b[32m[1112 12:39:58 @monitor.py:362]\u001b[0m micro_auc: 0.75949\n",
      "\u001b[32m[1112 12:39:58 @monitor.py:362]\u001b[0m micro_f1: 0.42836\n",
      "\u001b[32m[1112 12:39:58 @monitor.py:362]\u001b[0m one_error: 0.56863\n",
      "\u001b[32m[1112 12:39:58 @monitor.py:362]\u001b[0m ranking_loss: 0.25412\n",
      "\u001b[32m[1112 12:39:58 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.57763\n",
      "\u001b[32m[1112 12:39:58 @monitor.py:362]\u001b[0m training_ap: 0.24423\n",
      "\u001b[32m[1112 12:39:58 @monitor.py:362]\u001b[0m training_auc: 0.61236\n",
      "\u001b[32m[1112 12:39:58 @base.py:257]\u001b[0m Start Epoch 16 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:00 @base.py:267]\u001b[0m Epoch 16 (global_step 368) finished, time:1.52 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:00 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-368.\n",
      "\u001b[32m[1112 12:40:00 @saver.py:159]\u001b[0m Model with maximum 'micro_f1' saved.\n",
      "\u001b[32m[1112 12:40:00 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.8\n",
      "\u001b[32m[1112 12:40:00 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.911\n",
      "\u001b[32m[1112 12:40:01 @monitor.py:362]\u001b[0m coverage: 3.0927\n",
      "\u001b[32m[1112 12:40:01 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.54743\n",
      "\u001b[32m[1112 12:40:01 @monitor.py:362]\u001b[0m learning_rate: 0.001\n",
      "\u001b[32m[1112 12:40:01 @monitor.py:362]\u001b[0m loss/value: 10.897\n",
      "\u001b[32m[1112 12:40:01 @monitor.py:362]\u001b[0m macro_auc: 0.77815\n",
      "\u001b[32m[1112 12:40:01 @monitor.py:362]\u001b[0m macro_f1: 0.42091\n",
      "\u001b[32m[1112 12:40:01 @monitor.py:362]\u001b[0m mean_average_precision: 0.45008\n",
      "\u001b[32m[1112 12:40:01 @monitor.py:362]\u001b[0m micro_auc: 0.77569\n",
      "\u001b[32m[1112 12:40:01 @monitor.py:362]\u001b[0m micro_f1: 0.44871\n",
      "\u001b[32m[1112 12:40:01 @monitor.py:362]\u001b[0m one_error: 0.53654\n",
      "\u001b[32m[1112 12:40:01 @monitor.py:362]\u001b[0m ranking_loss: 0.23815\n",
      "\u001b[32m[1112 12:40:01 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.601\n",
      "\u001b[32m[1112 12:40:01 @monitor.py:362]\u001b[0m training_ap: 0.24816\n",
      "\u001b[32m[1112 12:40:01 @monitor.py:362]\u001b[0m training_auc: 0.61788\n",
      "\u001b[32m[1112 12:40:01 @base.py:257]\u001b[0m Start Epoch 17 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:02 @base.py:267]\u001b[0m Epoch 17 (global_step 391) finished, time:1.55 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:03 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-391.\n",
      "\u001b[32m[1112 12:40:03 @saver.py:159]\u001b[0m Model with maximum 'micro_f1' saved.\n",
      "\u001b[32m[1112 12:40:03 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.854\n",
      "\u001b[32m[1112 12:40:03 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.863\n",
      "\u001b[32m[1112 12:40:03 @monitor.py:362]\u001b[0m coverage: 3.0178\n",
      "\u001b[32m[1112 12:40:03 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.56378\n",
      "\u001b[32m[1112 12:40:03 @monitor.py:362]\u001b[0m learning_rate: 0.001\n",
      "\u001b[32m[1112 12:40:03 @monitor.py:362]\u001b[0m loss/value: 10.83\n",
      "\u001b[32m[1112 12:40:03 @monitor.py:362]\u001b[0m macro_auc: 0.78975\n",
      "\u001b[32m[1112 12:40:03 @monitor.py:362]\u001b[0m macro_f1: 0.4347\n",
      "\u001b[32m[1112 12:40:03 @monitor.py:362]\u001b[0m mean_average_precision: 0.46679\n",
      "\u001b[32m[1112 12:40:03 @monitor.py:362]\u001b[0m micro_auc: 0.78524\n",
      "\u001b[32m[1112 12:40:03 @monitor.py:362]\u001b[0m micro_f1: 0.4625\n",
      "\u001b[32m[1112 12:40:03 @monitor.py:362]\u001b[0m one_error: 0.54902\n",
      "\u001b[32m[1112 12:40:03 @monitor.py:362]\u001b[0m ranking_loss: 0.2313\n",
      "\u001b[32m[1112 12:40:03 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.6004\n",
      "\u001b[32m[1112 12:40:03 @monitor.py:362]\u001b[0m training_ap: 0.25261\n",
      "\u001b[32m[1112 12:40:03 @monitor.py:362]\u001b[0m training_auc: 0.62377\n",
      "\u001b[32m[1112 12:40:03 @base.py:257]\u001b[0m Start Epoch 18 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,13.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:04 @base.py:267]\u001b[0m Epoch 18 (global_step 414) finished, time:1.72 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:05 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-414.\n",
      "\u001b[32m[1112 12:40:05 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.852\n",
      "\u001b[32m[1112 12:40:05 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.925\n",
      "\u001b[32m[1112 12:40:05 @monitor.py:362]\u001b[0m coverage: 2.9447\n",
      "\u001b[32m[1112 12:40:05 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.38013\n",
      "\u001b[32m[1112 12:40:05 @monitor.py:362]\u001b[0m learning_rate: 0.001\n",
      "\u001b[32m[1112 12:40:05 @monitor.py:362]\u001b[0m loss/value: 10.68\n",
      "\u001b[32m[1112 12:40:05 @monitor.py:362]\u001b[0m macro_auc: 0.7966\n",
      "\u001b[32m[1112 12:40:05 @monitor.py:362]\u001b[0m macro_f1: 0.42921\n",
      "\u001b[32m[1112 12:40:05 @monitor.py:362]\u001b[0m mean_average_precision: 0.4729\n",
      "\u001b[32m[1112 12:40:05 @monitor.py:362]\u001b[0m micro_auc: 0.79275\n",
      "\u001b[32m[1112 12:40:05 @monitor.py:362]\u001b[0m micro_f1: 0.45289\n",
      "\u001b[32m[1112 12:40:05 @monitor.py:362]\u001b[0m one_error: 0.5205\n",
      "\u001b[32m[1112 12:40:05 @monitor.py:362]\u001b[0m ranking_loss: 0.22273\n",
      "\u001b[32m[1112 12:40:05 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.61329\n",
      "\u001b[32m[1112 12:40:05 @monitor.py:362]\u001b[0m training_ap: 0.25716\n",
      "\u001b[32m[1112 12:40:05 @monitor.py:362]\u001b[0m training_auc: 0.62934\n",
      "\u001b[32m[1112 12:40:05 @base.py:257]\u001b[0m Start Epoch 19 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:07 @base.py:267]\u001b[0m Epoch 19 (global_step 437) finished, time:1.61 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:07 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-437.\n",
      "\u001b[32m[1112 12:40:07 @saver.py:159]\u001b[0m Model with maximum 'micro_f1' saved.\n",
      "\u001b[32m[1112 12:40:07 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.891\n",
      "\u001b[32m[1112 12:40:07 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.895\n",
      "\u001b[32m[1112 12:40:07 @monitor.py:362]\u001b[0m coverage: 2.7879\n",
      "\u001b[32m[1112 12:40:07 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 1.6106\n",
      "\u001b[32m[1112 12:40:07 @monitor.py:362]\u001b[0m learning_rate: 0.001\n",
      "\u001b[32m[1112 12:40:07 @monitor.py:362]\u001b[0m loss/value: 10.508\n",
      "\u001b[32m[1112 12:40:07 @monitor.py:362]\u001b[0m macro_auc: 0.81195\n",
      "\u001b[32m[1112 12:40:07 @monitor.py:362]\u001b[0m macro_f1: 0.45433\n",
      "\u001b[32m[1112 12:40:07 @monitor.py:362]\u001b[0m mean_average_precision: 0.49792\n",
      "\u001b[32m[1112 12:40:07 @monitor.py:362]\u001b[0m micro_auc: 0.80742\n",
      "\u001b[32m[1112 12:40:07 @monitor.py:362]\u001b[0m micro_f1: 0.48097\n",
      "\u001b[32m[1112 12:40:07 @monitor.py:362]\u001b[0m one_error: 0.49911\n",
      "\u001b[32m[1112 12:40:07 @monitor.py:362]\u001b[0m ranking_loss: 0.20619\n",
      "\u001b[32m[1112 12:40:07 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.63216\n",
      "\u001b[32m[1112 12:40:07 @monitor.py:362]\u001b[0m training_ap: 0.26165\n",
      "\u001b[32m[1112 12:40:07 @monitor.py:362]\u001b[0m training_auc: 0.63515\n",
      "\u001b[32m[1112 12:40:07 @base.py:257]\u001b[0m Start Epoch 20 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:09 @base.py:267]\u001b[0m Epoch 20 (global_step 460) finished, time:1.54 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:09 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-460.\n",
      "\u001b[32m[1112 12:40:09 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.92\n",
      "\u001b[32m[1112 12:40:09 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.901\n",
      "\u001b[32m[1112 12:40:09 @monitor.py:362]\u001b[0m coverage: 2.7986\n",
      "\u001b[32m[1112 12:40:09 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.50381\n",
      "\u001b[32m[1112 12:40:09 @monitor.py:362]\u001b[0m learning_rate: 0.001\n",
      "\u001b[32m[1112 12:40:09 @monitor.py:362]\u001b[0m loss/value: 10.267\n",
      "\u001b[32m[1112 12:40:09 @monitor.py:362]\u001b[0m macro_auc: 0.79498\n",
      "\u001b[32m[1112 12:40:09 @monitor.py:362]\u001b[0m macro_f1: 0.44499\n",
      "\u001b[32m[1112 12:40:09 @monitor.py:362]\u001b[0m mean_average_precision: 0.48053\n",
      "\u001b[32m[1112 12:40:09 @monitor.py:362]\u001b[0m micro_auc: 0.79628\n",
      "\u001b[32m[1112 12:40:09 @monitor.py:362]\u001b[0m micro_f1: 0.47386\n",
      "\u001b[32m[1112 12:40:09 @monitor.py:362]\u001b[0m one_error: 0.49911\n",
      "\u001b[32m[1112 12:40:09 @monitor.py:362]\u001b[0m ranking_loss: 0.20499\n",
      "\u001b[32m[1112 12:40:09 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.63522\n",
      "\u001b[32m[1112 12:40:09 @monitor.py:362]\u001b[0m training_ap: 0.26664\n",
      "\u001b[32m[1112 12:40:09 @monitor.py:362]\u001b[0m training_auc: 0.64107\n",
      "\u001b[32m[1112 12:40:09 @base.py:257]\u001b[0m Start Epoch 21 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,13.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:11 @base.py:267]\u001b[0m Epoch 21 (global_step 483) finished, time:1.74 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:12 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-483.\n",
      "\u001b[32m[1112 12:40:12 @saver.py:159]\u001b[0m Model with maximum 'micro_f1' saved.\n",
      "\u001b[32m[1112 12:40:12 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.941\n",
      "\u001b[32m[1112 12:40:12 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.872\n",
      "\u001b[32m[1112 12:40:12 @monitor.py:362]\u001b[0m coverage: 2.8164\n",
      "\u001b[32m[1112 12:40:12 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.38275\n",
      "\u001b[32m[1112 12:40:12 @monitor.py:362]\u001b[0m learning_rate: 0.001\n",
      "\u001b[32m[1112 12:40:12 @monitor.py:362]\u001b[0m loss/value: 10.239\n",
      "\u001b[32m[1112 12:40:12 @monitor.py:362]\u001b[0m macro_auc: 0.80532\n",
      "\u001b[32m[1112 12:40:12 @monitor.py:362]\u001b[0m macro_f1: 0.45845\n",
      "\u001b[32m[1112 12:40:12 @monitor.py:362]\u001b[0m mean_average_precision: 0.49821\n",
      "\u001b[32m[1112 12:40:12 @monitor.py:362]\u001b[0m micro_auc: 0.80488\n",
      "\u001b[32m[1112 12:40:12 @monitor.py:362]\u001b[0m micro_f1: 0.48606\n",
      "\u001b[32m[1112 12:40:12 @monitor.py:362]\u001b[0m one_error: 0.50802\n",
      "\u001b[32m[1112 12:40:12 @monitor.py:362]\u001b[0m ranking_loss: 0.21062\n",
      "\u001b[32m[1112 12:40:12 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.63134\n",
      "\u001b[32m[1112 12:40:12 @monitor.py:362]\u001b[0m training_ap: 0.27155\n",
      "\u001b[32m[1112 12:40:12 @monitor.py:362]\u001b[0m training_auc: 0.6464\n",
      "\u001b[32m[1112 12:40:12 @base.py:257]\u001b[0m Start Epoch 22 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:13 @base.py:267]\u001b[0m Epoch 22 (global_step 506) finished, time:1.51 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 8.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:14 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-506.\n",
      "\u001b[32m[1112 12:40:14 @saver.py:159]\u001b[0m Model with maximum 'micro_f1' saved.\n",
      "\u001b[32m[1112 12:40:14 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.957\n",
      "\u001b[32m[1112 12:40:14 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.857\n",
      "\u001b[32m[1112 12:40:14 @monitor.py:362]\u001b[0m coverage: 2.7184\n",
      "\u001b[32m[1112 12:40:14 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.64213\n",
      "\u001b[32m[1112 12:40:14 @monitor.py:362]\u001b[0m learning_rate: 0.001\n",
      "\u001b[32m[1112 12:40:14 @monitor.py:362]\u001b[0m loss/value: 10.302\n",
      "\u001b[32m[1112 12:40:14 @monitor.py:362]\u001b[0m macro_auc: 0.81784\n",
      "\u001b[32m[1112 12:40:14 @monitor.py:362]\u001b[0m macro_f1: 0.46101\n",
      "\u001b[32m[1112 12:40:14 @monitor.py:362]\u001b[0m mean_average_precision: 0.5031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:14 @monitor.py:362]\u001b[0m micro_auc: 0.81135\n",
      "\u001b[32m[1112 12:40:14 @monitor.py:362]\u001b[0m micro_f1: 0.48974\n",
      "\u001b[32m[1112 12:40:14 @monitor.py:362]\u001b[0m one_error: 0.50446\n",
      "\u001b[32m[1112 12:40:14 @monitor.py:362]\u001b[0m ranking_loss: 0.20155\n",
      "\u001b[32m[1112 12:40:14 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.637\n",
      "\u001b[32m[1112 12:40:14 @monitor.py:362]\u001b[0m training_ap: 0.27624\n",
      "\u001b[32m[1112 12:40:14 @monitor.py:362]\u001b[0m training_auc: 0.65169\n",
      "\u001b[32m[1112 12:40:14 @base.py:257]\u001b[0m Start Epoch 23 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########|23/23[00:01<00:00,13.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:16 @base.py:267]\u001b[0m Epoch 23 (global_step 529) finished, time:1.68 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:16 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-529.\n",
      "\u001b[32m[1112 12:40:16 @saver.py:159]\u001b[0m Model with maximum 'micro_f1' saved.\n",
      "\u001b[32m[1112 12:40:16 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.925\n",
      "\u001b[32m[1112 12:40:16 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.79\n",
      "\u001b[32m[1112 12:40:16 @monitor.py:362]\u001b[0m coverage: 2.5989\n",
      "\u001b[32m[1112 12:40:16 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 1.033\n",
      "\u001b[32m[1112 12:40:16 @monitor.py:362]\u001b[0m learning_rate: 0.001\n",
      "\u001b[32m[1112 12:40:16 @monitor.py:362]\u001b[0m loss/value: 10.117\n",
      "\u001b[32m[1112 12:40:16 @monitor.py:362]\u001b[0m macro_auc: 0.82343\n",
      "\u001b[32m[1112 12:40:16 @monitor.py:362]\u001b[0m macro_f1: 0.46876\n",
      "\u001b[32m[1112 12:40:16 @monitor.py:362]\u001b[0m mean_average_precision: 0.51718\n",
      "\u001b[32m[1112 12:40:16 @monitor.py:362]\u001b[0m micro_auc: 0.81969\n",
      "\u001b[32m[1112 12:40:16 @monitor.py:362]\u001b[0m micro_f1: 0.4955\n",
      "\u001b[32m[1112 12:40:16 @monitor.py:362]\u001b[0m one_error: 0.46524\n",
      "\u001b[32m[1112 12:40:16 @monitor.py:362]\u001b[0m ranking_loss: 0.18735\n",
      "\u001b[32m[1112 12:40:16 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.66262\n",
      "\u001b[32m[1112 12:40:16 @monitor.py:362]\u001b[0m training_ap: 0.28131\n",
      "\u001b[32m[1112 12:40:16 @monitor.py:362]\u001b[0m training_auc: 0.65706\n",
      "\u001b[32m[1112 12:40:16 @base.py:257]\u001b[0m Start Epoch 24 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:18 @base.py:267]\u001b[0m Epoch 24 (global_step 552) finished, time:1.58 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:18 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-552.\n",
      "\u001b[32m[1112 12:40:18 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.945\n",
      "\u001b[32m[1112 12:40:18 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.776\n",
      "\u001b[32m[1112 12:40:18 @monitor.py:362]\u001b[0m coverage: 2.6346\n",
      "\u001b[32m[1112 12:40:18 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 1.2159\n",
      "\u001b[32m[1112 12:40:18 @monitor.py:362]\u001b[0m learning_rate: 0.001\n",
      "\u001b[32m[1112 12:40:18 @monitor.py:362]\u001b[0m loss/value: 9.9693\n",
      "\u001b[32m[1112 12:40:18 @monitor.py:362]\u001b[0m macro_auc: 0.82067\n",
      "\u001b[32m[1112 12:40:18 @monitor.py:362]\u001b[0m macro_f1: 0.46765\n",
      "\u001b[32m[1112 12:40:18 @monitor.py:362]\u001b[0m mean_average_precision: 0.49777\n",
      "\u001b[32m[1112 12:40:18 @monitor.py:362]\u001b[0m micro_auc: 0.8182\n",
      "\u001b[32m[1112 12:40:18 @monitor.py:362]\u001b[0m micro_f1: 0.493\n",
      "\u001b[32m[1112 12:40:18 @monitor.py:362]\u001b[0m one_error: 0.48307\n",
      "\u001b[32m[1112 12:40:18 @monitor.py:362]\u001b[0m ranking_loss: 0.19352\n",
      "\u001b[32m[1112 12:40:18 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.64797\n",
      "\u001b[32m[1112 12:40:18 @monitor.py:362]\u001b[0m training_ap: 0.28649\n",
      "\u001b[32m[1112 12:40:18 @monitor.py:362]\u001b[0m training_auc: 0.66233\n",
      "\u001b[32m[1112 12:40:18 @base.py:257]\u001b[0m Start Epoch 25 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:20 @base.py:267]\u001b[0m Epoch 25 (global_step 575) finished, time:1.54 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:20 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-575.\n",
      "\u001b[32m[1112 12:40:21 @saver.py:159]\u001b[0m Model with maximum 'micro_f1' saved.\n",
      "\u001b[32m[1112 12:40:21 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.96\n",
      "\u001b[32m[1112 12:40:21 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.837\n",
      "\u001b[32m[1112 12:40:21 @monitor.py:362]\u001b[0m coverage: 2.5811\n",
      "\u001b[32m[1112 12:40:21 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.67098\n",
      "\u001b[32m[1112 12:40:21 @monitor.py:362]\u001b[0m learning_rate: 0.001\n",
      "\u001b[32m[1112 12:40:21 @monitor.py:362]\u001b[0m loss/value: 9.811\n",
      "\u001b[32m[1112 12:40:21 @monitor.py:362]\u001b[0m macro_auc: 0.82276\n",
      "\u001b[32m[1112 12:40:21 @monitor.py:362]\u001b[0m macro_f1: 0.48511\n",
      "\u001b[32m[1112 12:40:21 @monitor.py:362]\u001b[0m mean_average_precision: 0.51613\n",
      "\u001b[32m[1112 12:40:21 @monitor.py:362]\u001b[0m micro_auc: 0.82253\n",
      "\u001b[32m[1112 12:40:21 @monitor.py:362]\u001b[0m micro_f1: 0.50737\n",
      "\u001b[32m[1112 12:40:21 @monitor.py:362]\u001b[0m one_error: 0.48663\n",
      "\u001b[32m[1112 12:40:21 @monitor.py:362]\u001b[0m ranking_loss: 0.1872\n",
      "\u001b[32m[1112 12:40:21 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.65526\n",
      "\u001b[32m[1112 12:40:21 @monitor.py:362]\u001b[0m training_ap: 0.29178\n",
      "\u001b[32m[1112 12:40:21 @monitor.py:362]\u001b[0m training_auc: 0.66751\n",
      "\u001b[32m[1112 12:40:21 @base.py:257]\u001b[0m Start Epoch 26 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:22 @base.py:267]\u001b[0m Epoch 26 (global_step 598) finished, time:1.52 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:23 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-598.\n",
      "\u001b[32m[1112 12:40:23 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.97\n",
      "\u001b[32m[1112 12:40:23 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.901\n",
      "\u001b[32m[1112 12:40:23 @monitor.py:362]\u001b[0m coverage: 2.6203\n",
      "\u001b[32m[1112 12:40:23 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.83636\n",
      "\u001b[32m[1112 12:40:23 @monitor.py:362]\u001b[0m learning_rate: 0.001\n",
      "\u001b[32m[1112 12:40:23 @monitor.py:362]\u001b[0m loss/value: 9.7317\n",
      "\u001b[32m[1112 12:40:23 @monitor.py:362]\u001b[0m macro_auc: 0.82332\n",
      "\u001b[32m[1112 12:40:23 @monitor.py:362]\u001b[0m macro_f1: 0.48079\n",
      "\u001b[32m[1112 12:40:23 @monitor.py:362]\u001b[0m mean_average_precision: 0.51727\n",
      "\u001b[32m[1112 12:40:23 @monitor.py:362]\u001b[0m micro_auc: 0.82249\n",
      "\u001b[32m[1112 12:40:23 @monitor.py:362]\u001b[0m micro_f1: 0.50419\n",
      "\u001b[32m[1112 12:40:23 @monitor.py:362]\u001b[0m one_error: 0.48841\n",
      "\u001b[32m[1112 12:40:23 @monitor.py:362]\u001b[0m ranking_loss: 0.19168\n",
      "\u001b[32m[1112 12:40:23 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.64994\n",
      "\u001b[32m[1112 12:40:23 @monitor.py:362]\u001b[0m training_ap: 0.29685\n",
      "\u001b[32m[1112 12:40:23 @monitor.py:362]\u001b[0m training_auc: 0.67236\n",
      "\u001b[32m[1112 12:40:23 @base.py:257]\u001b[0m Start Epoch 27 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,13.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:24 @base.py:267]\u001b[0m Epoch 27 (global_step 621) finished, time:1.73 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,11.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:25 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-621.\n",
      "\u001b[32m[1112 12:40:25 @saver.py:159]\u001b[0m Model with maximum 'micro_f1' saved.\n",
      "\u001b[32m[1112 12:40:25 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.978\n",
      "\u001b[32m[1112 12:40:25 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.969\n",
      "\u001b[32m[1112 12:40:25 @monitor.py:362]\u001b[0m coverage: 2.5758\n",
      "\u001b[32m[1112 12:40:25 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.5926\n",
      "\u001b[32m[1112 12:40:25 @monitor.py:362]\u001b[0m learning_rate: 0.001\n",
      "\u001b[32m[1112 12:40:25 @monitor.py:362]\u001b[0m loss/value: 9.6516\n",
      "\u001b[32m[1112 12:40:25 @monitor.py:362]\u001b[0m macro_auc: 0.82522\n",
      "\u001b[32m[1112 12:40:25 @monitor.py:362]\u001b[0m macro_f1: 0.48581\n",
      "\u001b[32m[1112 12:40:25 @monitor.py:362]\u001b[0m mean_average_precision: 0.52426\n",
      "\u001b[32m[1112 12:40:25 @monitor.py:362]\u001b[0m micro_auc: 0.82651\n",
      "\u001b[32m[1112 12:40:25 @monitor.py:362]\u001b[0m micro_f1: 0.50836\n",
      "\u001b[32m[1112 12:40:25 @monitor.py:362]\u001b[0m one_error: 0.46346\n",
      "\u001b[32m[1112 12:40:25 @monitor.py:362]\u001b[0m ranking_loss: 0.18605\n",
      "\u001b[32m[1112 12:40:25 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.66388\n",
      "\u001b[32m[1112 12:40:25 @monitor.py:362]\u001b[0m training_ap: 0.302\n",
      "\u001b[32m[1112 12:40:25 @monitor.py:362]\u001b[0m training_auc: 0.677\n",
      "\u001b[32m[1112 12:40:25 @base.py:257]\u001b[0m Start Epoch 28 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:27 @base.py:267]\u001b[0m Epoch 28 (global_step 644) finished, time:1.58 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:27 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-644.\n",
      "\u001b[32m[1112 12:40:27 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.984\n",
      "\u001b[32m[1112 12:40:27 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.851\n",
      "\u001b[32m[1112 12:40:27 @monitor.py:362]\u001b[0m coverage: 2.5187\n",
      "\u001b[32m[1112 12:40:27 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.71291\n",
      "\u001b[32m[1112 12:40:27 @monitor.py:362]\u001b[0m learning_rate: 0.001\n",
      "\u001b[32m[1112 12:40:27 @monitor.py:362]\u001b[0m loss/value: 9.5763\n",
      "\u001b[32m[1112 12:40:27 @monitor.py:362]\u001b[0m macro_auc: 0.82826\n",
      "\u001b[32m[1112 12:40:27 @monitor.py:362]\u001b[0m macro_f1: 0.48542\n",
      "\u001b[32m[1112 12:40:27 @monitor.py:362]\u001b[0m mean_average_precision: 0.52754\n",
      "\u001b[32m[1112 12:40:27 @monitor.py:362]\u001b[0m micro_auc: 0.8283\n",
      "\u001b[32m[1112 12:40:27 @monitor.py:362]\u001b[0m micro_f1: 0.50606\n",
      "\u001b[32m[1112 12:40:27 @monitor.py:362]\u001b[0m one_error: 0.47059\n",
      "\u001b[32m[1112 12:40:27 @monitor.py:362]\u001b[0m ranking_loss: 0.18339\n",
      "\u001b[32m[1112 12:40:27 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.66339\n",
      "\u001b[32m[1112 12:40:27 @monitor.py:362]\u001b[0m training_ap: 0.30709\n",
      "\u001b[32m[1112 12:40:27 @monitor.py:362]\u001b[0m training_auc: 0.68164\n",
      "\u001b[32m[1112 12:40:27 @base.py:257]\u001b[0m Start Epoch 29 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:29 @base.py:267]\u001b[0m Epoch 29 (global_step 667) finished, time:1.49 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:29 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-667.\n",
      "\u001b[32m[1112 12:40:29 @saver.py:159]\u001b[0m Model with maximum 'micro_f1' saved.\n",
      "\u001b[32m[1112 12:40:29 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.988\n",
      "\u001b[32m[1112 12:40:29 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.799\n",
      "\u001b[32m[1112 12:40:29 @monitor.py:362]\u001b[0m coverage: 2.5027\n",
      "\u001b[32m[1112 12:40:29 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.34534\n",
      "\u001b[32m[1112 12:40:29 @monitor.py:362]\u001b[0m learning_rate: 0.001\n",
      "\u001b[32m[1112 12:40:29 @monitor.py:362]\u001b[0m loss/value: 9.5652\n",
      "\u001b[32m[1112 12:40:29 @monitor.py:362]\u001b[0m macro_auc: 0.83338\n",
      "\u001b[32m[1112 12:40:29 @monitor.py:362]\u001b[0m macro_f1: 0.49854\n",
      "\u001b[32m[1112 12:40:29 @monitor.py:362]\u001b[0m mean_average_precision: 0.52179\n",
      "\u001b[32m[1112 12:40:29 @monitor.py:362]\u001b[0m micro_auc: 0.83181\n",
      "\u001b[32m[1112 12:40:29 @monitor.py:362]\u001b[0m micro_f1: 0.517\n",
      "\u001b[32m[1112 12:40:29 @monitor.py:362]\u001b[0m one_error: 0.48485\n",
      "\u001b[32m[1112 12:40:29 @monitor.py:362]\u001b[0m ranking_loss: 0.18257\n",
      "\u001b[32m[1112 12:40:29 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.65631\n",
      "\u001b[32m[1112 12:40:29 @monitor.py:362]\u001b[0m training_ap: 0.31192\n",
      "\u001b[32m[1112 12:40:29 @monitor.py:362]\u001b[0m training_auc: 0.68607\n",
      "\u001b[32m[1112 12:40:29 @base.py:257]\u001b[0m Start Epoch 30 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,13.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:31 @base.py:267]\u001b[0m Epoch 30 (global_step 690) finished, time:1.65 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:31 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-690.\n",
      "\u001b[32m[1112 12:40:31 @saver.py:159]\u001b[0m Model with maximum 'micro_f1' saved.\n",
      "\u001b[32m[1112 12:40:32 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.946\n",
      "\u001b[32m[1112 12:40:32 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.938\n",
      "\u001b[32m[1112 12:40:32 @monitor.py:362]\u001b[0m coverage: 2.4831\n",
      "\u001b[32m[1112 12:40:32 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.88693\n",
      "\u001b[32m[1112 12:40:32 @monitor.py:362]\u001b[0m learning_rate: 0.001\n",
      "\u001b[32m[1112 12:40:32 @monitor.py:362]\u001b[0m loss/value: 9.487\n",
      "\u001b[32m[1112 12:40:32 @monitor.py:362]\u001b[0m macro_auc: 0.83521\n",
      "\u001b[32m[1112 12:40:32 @monitor.py:362]\u001b[0m macro_f1: 0.49743\n",
      "\u001b[32m[1112 12:40:32 @monitor.py:362]\u001b[0m mean_average_precision: 0.53768\n",
      "\u001b[32m[1112 12:40:32 @monitor.py:362]\u001b[0m micro_auc: 0.83481\n",
      "\u001b[32m[1112 12:40:32 @monitor.py:362]\u001b[0m micro_f1: 0.52273\n",
      "\u001b[32m[1112 12:40:32 @monitor.py:362]\u001b[0m one_error: 0.47059\n",
      "\u001b[32m[1112 12:40:32 @monitor.py:362]\u001b[0m ranking_loss: 0.18011\n",
      "\u001b[32m[1112 12:40:32 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.66367\n",
      "\u001b[32m[1112 12:40:32 @monitor.py:362]\u001b[0m training_ap: 0.31701\n",
      "\u001b[32m[1112 12:40:32 @monitor.py:362]\u001b[0m training_auc: 0.6903\n",
      "\u001b[32m[1112 12:40:32 @base.py:257]\u001b[0m Start Epoch 31 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:33 @base.py:267]\u001b[0m Epoch 31 (global_step 713) finished, time:1.61 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:34 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-713.\n",
      "\u001b[32m[1112 12:40:34 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.96\n",
      "\u001b[32m[1112 12:40:34 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.896\n",
      "\u001b[32m[1112 12:40:34 @monitor.py:362]\u001b[0m coverage: 2.426\n",
      "\u001b[32m[1112 12:40:34 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.74929\n",
      "\u001b[32m[1112 12:40:34 @monitor.py:362]\u001b[0m learning_rate: 0.001\n",
      "\u001b[32m[1112 12:40:34 @monitor.py:362]\u001b[0m loss/value: 9.3071\n",
      "\u001b[32m[1112 12:40:34 @monitor.py:362]\u001b[0m macro_auc: 0.83975\n",
      "\u001b[32m[1112 12:40:34 @monitor.py:362]\u001b[0m macro_f1: 0.4984\n",
      "\u001b[32m[1112 12:40:34 @monitor.py:362]\u001b[0m mean_average_precision: 0.53317\n",
      "\u001b[32m[1112 12:40:34 @monitor.py:362]\u001b[0m micro_auc: 0.83858\n",
      "\u001b[32m[1112 12:40:34 @monitor.py:362]\u001b[0m micro_f1: 0.52069\n",
      "\u001b[32m[1112 12:40:34 @monitor.py:362]\u001b[0m one_error: 0.45098\n",
      "\u001b[32m[1112 12:40:34 @monitor.py:362]\u001b[0m ranking_loss: 0.17386\n",
      "\u001b[32m[1112 12:40:34 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.67753\n",
      "\u001b[32m[1112 12:40:34 @monitor.py:362]\u001b[0m training_ap: 0.32207\n",
      "\u001b[32m[1112 12:40:34 @monitor.py:362]\u001b[0m training_auc: 0.69451\n",
      "\u001b[32m[1112 12:40:34 @base.py:257]\u001b[0m Start Epoch 32 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:35 @base.py:267]\u001b[0m Epoch 32 (global_step 736) finished, time:1.57 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:36 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-736.\n",
      "\u001b[32m[1112 12:40:36 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.93\n",
      "\u001b[32m[1112 12:40:36 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.949\n",
      "\u001b[32m[1112 12:40:36 @monitor.py:362]\u001b[0m coverage: 2.4189\n",
      "\u001b[32m[1112 12:40:36 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.23933\n",
      "\u001b[32m[1112 12:40:36 @monitor.py:362]\u001b[0m learning_rate: 0.001\n",
      "\u001b[32m[1112 12:40:36 @monitor.py:362]\u001b[0m loss/value: 9.166\n",
      "\u001b[32m[1112 12:40:36 @monitor.py:362]\u001b[0m macro_auc: 0.82832\n",
      "\u001b[32m[1112 12:40:36 @monitor.py:362]\u001b[0m macro_f1: 0.49073\n",
      "\u001b[32m[1112 12:40:36 @monitor.py:362]\u001b[0m mean_average_precision: 0.52381\n",
      "\u001b[32m[1112 12:40:36 @monitor.py:362]\u001b[0m micro_auc: 0.83365\n",
      "\u001b[32m[1112 12:40:36 @monitor.py:362]\u001b[0m micro_f1: 0.52184\n",
      "\u001b[32m[1112 12:40:36 @monitor.py:362]\u001b[0m one_error: 0.45455\n",
      "\u001b[32m[1112 12:40:36 @monitor.py:362]\u001b[0m ranking_loss: 0.17305\n",
      "\u001b[32m[1112 12:40:36 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.67858\n",
      "\u001b[32m[1112 12:40:36 @monitor.py:362]\u001b[0m training_ap: 0.32727\n",
      "\u001b[32m[1112 12:40:36 @monitor.py:362]\u001b[0m training_auc: 0.69869\n",
      "\u001b[32m[1112 12:40:36 @base.py:257]\u001b[0m Start Epoch 33 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:37 @base.py:267]\u001b[0m Epoch 33 (global_step 759) finished, time:1.56 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:38 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-759.\n",
      "\u001b[32m[1112 12:40:38 @saver.py:159]\u001b[0m Model with maximum 'micro_f1' saved.\n",
      "\u001b[32m[1112 12:40:38 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.949\n",
      "\u001b[32m[1112 12:40:38 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.853\n",
      "\u001b[32m[1112 12:40:38 @monitor.py:362]\u001b[0m coverage: 2.3975\n",
      "\u001b[32m[1112 12:40:38 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.91813\n",
      "\u001b[32m[1112 12:40:38 @monitor.py:362]\u001b[0m learning_rate: 0.001\n",
      "\u001b[32m[1112 12:40:38 @monitor.py:362]\u001b[0m loss/value: 9.0396\n",
      "\u001b[32m[1112 12:40:38 @monitor.py:362]\u001b[0m macro_auc: 0.83744\n",
      "\u001b[32m[1112 12:40:38 @monitor.py:362]\u001b[0m macro_f1: 0.51018\n",
      "\u001b[32m[1112 12:40:38 @monitor.py:362]\u001b[0m mean_average_precision: 0.53113\n",
      "\u001b[32m[1112 12:40:38 @monitor.py:362]\u001b[0m micro_auc: 0.83911\n",
      "\u001b[32m[1112 12:40:38 @monitor.py:362]\u001b[0m micro_f1: 0.53554\n",
      "\u001b[32m[1112 12:40:38 @monitor.py:362]\u001b[0m one_error: 0.46881\n",
      "\u001b[32m[1112 12:40:38 @monitor.py:362]\u001b[0m ranking_loss: 0.17416\n",
      "\u001b[32m[1112 12:40:38 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.67352\n",
      "\u001b[32m[1112 12:40:38 @monitor.py:362]\u001b[0m training_ap: 0.33277\n",
      "\u001b[32m[1112 12:40:38 @monitor.py:362]\u001b[0m training_auc: 0.70275\n",
      "\u001b[32m[1112 12:40:38 @base.py:257]\u001b[0m Start Epoch 34 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:40 @base.py:267]\u001b[0m Epoch 34 (global_step 782) finished, time:1.63 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:40 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-782.\n",
      "\u001b[32m[1112 12:40:40 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.922\n",
      "\u001b[32m[1112 12:40:40 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.892\n",
      "\u001b[32m[1112 12:40:40 @monitor.py:362]\u001b[0m coverage: 2.4189\n",
      "\u001b[32m[1112 12:40:40 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.90445\n",
      "\u001b[32m[1112 12:40:40 @monitor.py:362]\u001b[0m learning_rate: 0.001\n",
      "\u001b[32m[1112 12:40:40 @monitor.py:362]\u001b[0m loss/value: 8.9312\n",
      "\u001b[32m[1112 12:40:40 @monitor.py:362]\u001b[0m macro_auc: 0.83546\n",
      "\u001b[32m[1112 12:40:40 @monitor.py:362]\u001b[0m macro_f1: 0.5062\n",
      "\u001b[32m[1112 12:40:40 @monitor.py:362]\u001b[0m mean_average_precision: 0.53282\n",
      "\u001b[32m[1112 12:40:40 @monitor.py:362]\u001b[0m micro_auc: 0.83401\n",
      "\u001b[32m[1112 12:40:40 @monitor.py:362]\u001b[0m micro_f1: 0.52502\n",
      "\u001b[32m[1112 12:40:40 @monitor.py:362]\u001b[0m one_error: 0.46702\n",
      "\u001b[32m[1112 12:40:40 @monitor.py:362]\u001b[0m ranking_loss: 0.17612\n",
      "\u001b[32m[1112 12:40:40 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.6692\n",
      "\u001b[32m[1112 12:40:40 @monitor.py:362]\u001b[0m training_ap: 0.33803\n",
      "\u001b[32m[1112 12:40:40 @monitor.py:362]\u001b[0m training_auc: 0.70672\n",
      "\u001b[32m[1112 12:40:40 @base.py:257]\u001b[0m Start Epoch 35 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,16.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:42 @base.py:267]\u001b[0m Epoch 35 (global_step 805) finished, time:1.42 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:42 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-805.\n",
      "\u001b[32m[1112 12:40:42 @saver.py:159]\u001b[0m Model with maximum 'micro_f1' saved.\n",
      "\u001b[32m[1112 12:40:42 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.942\n",
      "\u001b[32m[1112 12:40:42 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.909\n",
      "\u001b[32m[1112 12:40:42 @monitor.py:362]\u001b[0m coverage: 2.262\n",
      "\u001b[32m[1112 12:40:42 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 1.1412\n",
      "\u001b[32m[1112 12:40:42 @monitor.py:362]\u001b[0m learning_rate: 0.001\n",
      "\u001b[32m[1112 12:40:42 @monitor.py:362]\u001b[0m loss/value: 8.9766\n",
      "\u001b[32m[1112 12:40:42 @monitor.py:362]\u001b[0m macro_auc: 0.84643\n",
      "\u001b[32m[1112 12:40:42 @monitor.py:362]\u001b[0m macro_f1: 0.51787\n",
      "\u001b[32m[1112 12:40:42 @monitor.py:362]\u001b[0m mean_average_precision: 0.55443\n",
      "\u001b[32m[1112 12:40:42 @monitor.py:362]\u001b[0m micro_auc: 0.84789\n",
      "\u001b[32m[1112 12:40:42 @monitor.py:362]\u001b[0m micro_f1: 0.54294\n",
      "\u001b[32m[1112 12:40:42 @monitor.py:362]\u001b[0m one_error: 0.44385\n",
      "\u001b[32m[1112 12:40:42 @monitor.py:362]\u001b[0m ranking_loss: 0.15939\n",
      "\u001b[32m[1112 12:40:42 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.69035\n",
      "\u001b[32m[1112 12:40:42 @monitor.py:362]\u001b[0m training_ap: 0.34359\n",
      "\u001b[32m[1112 12:40:42 @monitor.py:362]\u001b[0m training_auc: 0.71078\n",
      "\u001b[32m[1112 12:40:42 @base.py:257]\u001b[0m Start Epoch 36 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:44 @base.py:267]\u001b[0m Epoch 36 (global_step 828) finished, time:1.47 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:44 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-828.\n",
      "\u001b[32m[1112 12:40:44 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.958\n",
      "\u001b[32m[1112 12:40:44 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.922\n",
      "\u001b[32m[1112 12:40:44 @monitor.py:362]\u001b[0m coverage: 2.3743\n",
      "\u001b[32m[1112 12:40:44 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.92354\n",
      "\u001b[32m[1112 12:40:44 @monitor.py:362]\u001b[0m learning_rate: 0.001\n",
      "\u001b[32m[1112 12:40:44 @monitor.py:362]\u001b[0m loss/value: 8.9025\n",
      "\u001b[32m[1112 12:40:44 @monitor.py:362]\u001b[0m macro_auc: 0.8389\n",
      "\u001b[32m[1112 12:40:44 @monitor.py:362]\u001b[0m macro_f1: 0.51447\n",
      "\u001b[32m[1112 12:40:44 @monitor.py:362]\u001b[0m mean_average_precision: 0.55171\n",
      "\u001b[32m[1112 12:40:44 @monitor.py:362]\u001b[0m micro_auc: 0.8439\n",
      "\u001b[32m[1112 12:40:44 @monitor.py:362]\u001b[0m micro_f1: 0.53947\n",
      "\u001b[32m[1112 12:40:44 @monitor.py:362]\u001b[0m one_error: 0.45098\n",
      "\u001b[32m[1112 12:40:44 @monitor.py:362]\u001b[0m ranking_loss: 0.16971\n",
      "\u001b[32m[1112 12:40:44 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.68235\n",
      "\u001b[32m[1112 12:40:44 @monitor.py:362]\u001b[0m training_ap: 0.34899\n",
      "\u001b[32m[1112 12:40:44 @monitor.py:362]\u001b[0m training_auc: 0.71471\n",
      "\u001b[32m[1112 12:40:44 @base.py:257]\u001b[0m Start Epoch 37 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:46 @base.py:267]\u001b[0m Epoch 37 (global_step 851) finished, time:1.53 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:46 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-851.\n",
      "\u001b[32m[1112 12:40:46 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.969\n",
      "\u001b[32m[1112 12:40:46 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.856\n",
      "\u001b[32m[1112 12:40:46 @monitor.py:362]\u001b[0m coverage: 2.3405\n",
      "\u001b[32m[1112 12:40:46 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.50926\n",
      "\u001b[32m[1112 12:40:46 @monitor.py:362]\u001b[0m learning_rate: 0.001\n",
      "\u001b[32m[1112 12:40:46 @monitor.py:362]\u001b[0m loss/value: 8.7674\n",
      "\u001b[32m[1112 12:40:46 @monitor.py:362]\u001b[0m macro_auc: 0.84194\n",
      "\u001b[32m[1112 12:40:46 @monitor.py:362]\u001b[0m macro_f1: 0.51705\n",
      "\u001b[32m[1112 12:40:46 @monitor.py:362]\u001b[0m mean_average_precision: 0.54817\n",
      "\u001b[32m[1112 12:40:46 @monitor.py:362]\u001b[0m micro_auc: 0.84594\n",
      "\u001b[32m[1112 12:40:46 @monitor.py:362]\u001b[0m micro_f1: 0.54241\n",
      "\u001b[32m[1112 12:40:46 @monitor.py:362]\u001b[0m one_error: 0.42068\n",
      "\u001b[32m[1112 12:40:46 @monitor.py:362]\u001b[0m ranking_loss: 0.16335\n",
      "\u001b[32m[1112 12:40:46 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.69517\n",
      "\u001b[32m[1112 12:40:46 @monitor.py:362]\u001b[0m training_ap: 0.35407\n",
      "\u001b[32m[1112 12:40:46 @monitor.py:362]\u001b[0m training_auc: 0.71845\n",
      "\u001b[32m[1112 12:40:46 @base.py:257]\u001b[0m Start Epoch 38 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:48 @base.py:267]\u001b[0m Epoch 38 (global_step 874) finished, time:1.58 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,11.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:48 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-874.\n",
      "\u001b[32m[1112 12:40:48 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.977\n",
      "\u001b[32m[1112 12:40:48 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.823\n",
      "\u001b[32m[1112 12:40:48 @monitor.py:362]\u001b[0m coverage: 2.3387\n",
      "\u001b[32m[1112 12:40:48 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.18487\n",
      "\u001b[32m[1112 12:40:48 @monitor.py:362]\u001b[0m learning_rate: 0.001\n",
      "\u001b[32m[1112 12:40:48 @monitor.py:362]\u001b[0m loss/value: 8.688\n",
      "\u001b[32m[1112 12:40:48 @monitor.py:362]\u001b[0m macro_auc: 0.83902\n",
      "\u001b[32m[1112 12:40:48 @monitor.py:362]\u001b[0m macro_f1: 0.50962\n",
      "\u001b[32m[1112 12:40:48 @monitor.py:362]\u001b[0m mean_average_precision: 0.56115\n",
      "\u001b[32m[1112 12:40:48 @monitor.py:362]\u001b[0m micro_auc: 0.84589\n",
      "\u001b[32m[1112 12:40:48 @monitor.py:362]\u001b[0m micro_f1: 0.53077\n",
      "\u001b[32m[1112 12:40:48 @monitor.py:362]\u001b[0m one_error: 0.45633\n",
      "\u001b[32m[1112 12:40:48 @monitor.py:362]\u001b[0m ranking_loss: 0.16675\n",
      "\u001b[32m[1112 12:40:48 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.68353\n",
      "\u001b[32m[1112 12:40:48 @monitor.py:362]\u001b[0m training_ap: 0.35894\n",
      "\u001b[32m[1112 12:40:48 @monitor.py:362]\u001b[0m training_auc: 0.72189\n",
      "\u001b[32m[1112 12:40:48 @base.py:257]\u001b[0m Start Epoch 39 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:50 @base.py:267]\u001b[0m Epoch 39 (global_step 897) finished, time:1.51 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:50 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-897.\n",
      "\u001b[32m[1112 12:40:50 @saver.py:159]\u001b[0m Model with maximum 'micro_f1' saved.\n",
      "\u001b[32m[1112 12:40:50 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.983\n",
      "\u001b[32m[1112 12:40:51 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.843\n",
      "\u001b[32m[1112 12:40:51 @monitor.py:362]\u001b[0m coverage: 2.3066\n",
      "\u001b[32m[1112 12:40:51 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.28136\n",
      "\u001b[32m[1112 12:40:51 @monitor.py:362]\u001b[0m learning_rate: 0.001\n",
      "\u001b[32m[1112 12:40:51 @monitor.py:362]\u001b[0m loss/value: 8.6048\n",
      "\u001b[32m[1112 12:40:51 @monitor.py:362]\u001b[0m macro_auc: 0.84502\n",
      "\u001b[32m[1112 12:40:51 @monitor.py:362]\u001b[0m macro_f1: 0.51934\n",
      "\u001b[32m[1112 12:40:51 @monitor.py:362]\u001b[0m mean_average_precision: 0.54519\n",
      "\u001b[32m[1112 12:40:51 @monitor.py:362]\u001b[0m micro_auc: 0.84671\n",
      "\u001b[32m[1112 12:40:51 @monitor.py:362]\u001b[0m micro_f1: 0.54735\n",
      "\u001b[32m[1112 12:40:51 @monitor.py:362]\u001b[0m one_error: 0.45098\n",
      "\u001b[32m[1112 12:40:51 @monitor.py:362]\u001b[0m ranking_loss: 0.16323\n",
      "\u001b[32m[1112 12:40:51 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.68454\n",
      "\u001b[32m[1112 12:40:51 @monitor.py:362]\u001b[0m training_ap: 0.3638\n",
      "\u001b[32m[1112 12:40:51 @monitor.py:362]\u001b[0m training_auc: 0.72541\n",
      "\u001b[32m[1112 12:40:51 @base.py:257]\u001b[0m Start Epoch 40 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,13.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:52 @base.py:267]\u001b[0m Epoch 40 (global_step 920) finished, time:1.66 sec.\n",
      "\u001b[32m[1112 12:40:52 @param.py:144]\u001b[0m After epoch 40, learning_rate will change to 0.00010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:53 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-920.\n",
      "\u001b[32m[1112 12:40:53 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.988\n",
      "\u001b[32m[1112 12:40:53 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.909\n",
      "\u001b[32m[1112 12:40:53 @monitor.py:362]\u001b[0m coverage: 2.2692\n",
      "\u001b[32m[1112 12:40:53 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.36656\n",
      "\u001b[32m[1112 12:40:53 @monitor.py:362]\u001b[0m learning_rate: 0.001\n",
      "\u001b[32m[1112 12:40:53 @monitor.py:362]\u001b[0m loss/value: 8.5931\n",
      "\u001b[32m[1112 12:40:53 @monitor.py:362]\u001b[0m macro_auc: 0.83942\n",
      "\u001b[32m[1112 12:40:53 @monitor.py:362]\u001b[0m macro_f1: 0.51572\n",
      "\u001b[32m[1112 12:40:53 @monitor.py:362]\u001b[0m mean_average_precision: 0.53661\n",
      "\u001b[32m[1112 12:40:53 @monitor.py:362]\u001b[0m micro_auc: 0.84654\n",
      "\u001b[32m[1112 12:40:53 @monitor.py:362]\u001b[0m micro_f1: 0.53958\n",
      "\u001b[32m[1112 12:40:53 @monitor.py:362]\u001b[0m one_error: 0.44385\n",
      "\u001b[32m[1112 12:40:53 @monitor.py:362]\u001b[0m ranking_loss: 0.15841\n",
      "\u001b[32m[1112 12:40:53 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.6935\n",
      "\u001b[32m[1112 12:40:53 @monitor.py:362]\u001b[0m training_ap: 0.36862\n",
      "\u001b[32m[1112 12:40:53 @monitor.py:362]\u001b[0m training_auc: 0.72867\n",
      "\u001b[32m[1112 12:40:53 @base.py:257]\u001b[0m Start Epoch 41 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:54 @base.py:267]\u001b[0m Epoch 41 (global_step 943) finished, time:1.59 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:55 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-943.\n",
      "\u001b[32m[1112 12:40:55 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.991\n",
      "\u001b[32m[1112 12:40:55 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.951\n",
      "\u001b[32m[1112 12:40:55 @monitor.py:362]\u001b[0m coverage: 2.2638\n",
      "\u001b[32m[1112 12:40:55 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.63376\n",
      "\u001b[32m[1112 12:40:55 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:40:55 @monitor.py:362]\u001b[0m loss/value: 8.4749\n",
      "\u001b[32m[1112 12:40:55 @monitor.py:362]\u001b[0m macro_auc: 0.84135\n",
      "\u001b[32m[1112 12:40:55 @monitor.py:362]\u001b[0m macro_f1: 0.52217\n",
      "\u001b[32m[1112 12:40:55 @monitor.py:362]\u001b[0m mean_average_precision: 0.54535\n",
      "\u001b[32m[1112 12:40:55 @monitor.py:362]\u001b[0m micro_auc: 0.84729\n",
      "\u001b[32m[1112 12:40:55 @monitor.py:362]\u001b[0m micro_f1: 0.54289\n",
      "\u001b[32m[1112 12:40:55 @monitor.py:362]\u001b[0m one_error: 0.43672\n",
      "\u001b[32m[1112 12:40:55 @monitor.py:362]\u001b[0m ranking_loss: 0.15669\n",
      "\u001b[32m[1112 12:40:55 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.69773\n",
      "\u001b[32m[1112 12:40:55 @monitor.py:362]\u001b[0m training_ap: 0.37349\n",
      "\u001b[32m[1112 12:40:55 @monitor.py:362]\u001b[0m training_auc: 0.73191\n",
      "\u001b[32m[1112 12:40:55 @base.py:257]\u001b[0m Start Epoch 42 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:56 @base.py:267]\u001b[0m Epoch 42 (global_step 966) finished, time:1.64 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:57 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-966.\n",
      "\u001b[32m[1112 12:40:57 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.993\n",
      "\u001b[32m[1112 12:40:57 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.907\n",
      "\u001b[32m[1112 12:40:57 @monitor.py:362]\u001b[0m coverage: 2.2371\n",
      "\u001b[32m[1112 12:40:57 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.94349\n",
      "\u001b[32m[1112 12:40:57 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:40:57 @monitor.py:362]\u001b[0m loss/value: 8.6083\n",
      "\u001b[32m[1112 12:40:57 @monitor.py:362]\u001b[0m macro_auc: 0.83893\n",
      "\u001b[32m[1112 12:40:57 @monitor.py:362]\u001b[0m macro_f1: 0.51987\n",
      "\u001b[32m[1112 12:40:57 @monitor.py:362]\u001b[0m mean_average_precision: 0.54236\n",
      "\u001b[32m[1112 12:40:57 @monitor.py:362]\u001b[0m micro_auc: 0.84601\n",
      "\u001b[32m[1112 12:40:57 @monitor.py:362]\u001b[0m micro_f1: 0.54552\n",
      "\u001b[32m[1112 12:40:57 @monitor.py:362]\u001b[0m one_error: 0.42781\n",
      "\u001b[32m[1112 12:40:57 @monitor.py:362]\u001b[0m ranking_loss: 0.15551\n",
      "\u001b[32m[1112 12:40:57 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70374\n",
      "\u001b[32m[1112 12:40:57 @monitor.py:362]\u001b[0m training_ap: 0.37804\n",
      "\u001b[32m[1112 12:40:57 @monitor.py:362]\u001b[0m training_auc: 0.73503\n",
      "\u001b[32m[1112 12:40:57 @base.py:257]\u001b[0m Start Epoch 43 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:59 @base.py:267]\u001b[0m Epoch 43 (global_step 989) finished, time:1.51 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:40:59 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-989.\n",
      "\u001b[32m[1112 12:40:59 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.954\n",
      "\u001b[32m[1112 12:40:59 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.954\n",
      "\u001b[32m[1112 12:40:59 @monitor.py:362]\u001b[0m coverage: 2.2727\n",
      "\u001b[32m[1112 12:40:59 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.96519\n",
      "\u001b[32m[1112 12:40:59 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:40:59 @monitor.py:362]\u001b[0m loss/value: 8.6343\n",
      "\u001b[32m[1112 12:40:59 @monitor.py:362]\u001b[0m macro_auc: 0.84163\n",
      "\u001b[32m[1112 12:40:59 @monitor.py:362]\u001b[0m macro_f1: 0.51906\n",
      "\u001b[32m[1112 12:40:59 @monitor.py:362]\u001b[0m mean_average_precision: 0.53486\n",
      "\u001b[32m[1112 12:40:59 @monitor.py:362]\u001b[0m micro_auc: 0.84772\n",
      "\u001b[32m[1112 12:40:59 @monitor.py:362]\u001b[0m micro_f1: 0.54452\n",
      "\u001b[32m[1112 12:40:59 @monitor.py:362]\u001b[0m one_error: 0.44563\n",
      "\u001b[32m[1112 12:40:59 @monitor.py:362]\u001b[0m ranking_loss: 0.16214\n",
      "\u001b[32m[1112 12:40:59 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.69157\n",
      "\u001b[32m[1112 12:40:59 @monitor.py:362]\u001b[0m training_ap: 0.38244\n",
      "\u001b[32m[1112 12:40:59 @monitor.py:362]\u001b[0m training_auc: 0.73787\n",
      "\u001b[32m[1112 12:40:59 @base.py:257]\u001b[0m Start Epoch 44 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:01 @base.py:267]\u001b[0m Epoch 44 (global_step 1012) finished, time:1.58 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:01 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-1012.\n",
      "\u001b[32m[1112 12:41:01 @saver.py:159]\u001b[0m Model with maximum 'micro_f1' saved.\n",
      "\u001b[32m[1112 12:41:01 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.966\n",
      "\u001b[32m[1112 12:41:01 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.86\n",
      "\u001b[32m[1112 12:41:01 @monitor.py:362]\u001b[0m coverage: 2.2852\n",
      "\u001b[32m[1112 12:41:01 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 1.2922\n",
      "\u001b[32m[1112 12:41:01 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:41:01 @monitor.py:362]\u001b[0m loss/value: 8.4757\n",
      "\u001b[32m[1112 12:41:01 @monitor.py:362]\u001b[0m macro_auc: 0.84586\n",
      "\u001b[32m[1112 12:41:01 @monitor.py:362]\u001b[0m macro_f1: 0.52501\n",
      "\u001b[32m[1112 12:41:01 @monitor.py:362]\u001b[0m mean_average_precision: 0.55405\n",
      "\u001b[32m[1112 12:41:01 @monitor.py:362]\u001b[0m micro_auc: 0.85155\n",
      "\u001b[32m[1112 12:41:01 @monitor.py:362]\u001b[0m micro_f1: 0.54849\n",
      "\u001b[32m[1112 12:41:01 @monitor.py:362]\u001b[0m one_error: 0.44563\n",
      "\u001b[32m[1112 12:41:01 @monitor.py:362]\u001b[0m ranking_loss: 0.15944\n",
      "\u001b[32m[1112 12:41:01 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.69121\n",
      "\u001b[32m[1112 12:41:01 @monitor.py:362]\u001b[0m training_ap: 0.38677\n",
      "\u001b[32m[1112 12:41:01 @monitor.py:362]\u001b[0m training_auc: 0.7408\n",
      "\u001b[32m[1112 12:41:01 @base.py:257]\u001b[0m Start Epoch 45 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,13.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:03 @base.py:267]\u001b[0m Epoch 45 (global_step 1035) finished, time:1.70 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,11.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:04 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-1035.\n",
      "\u001b[32m[1112 12:41:04 @saver.py:159]\u001b[0m Model with maximum 'micro_f1' saved.\n",
      "\u001b[32m[1112 12:41:04 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.975\n",
      "\u001b[32m[1112 12:41:04 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.92\n",
      "\u001b[32m[1112 12:41:04 @monitor.py:362]\u001b[0m coverage: 2.2513\n",
      "\u001b[32m[1112 12:41:04 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.51632\n",
      "\u001b[32m[1112 12:41:04 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:41:04 @monitor.py:362]\u001b[0m loss/value: 8.4033\n",
      "\u001b[32m[1112 12:41:04 @monitor.py:362]\u001b[0m macro_auc: 0.85326\n",
      "\u001b[32m[1112 12:41:04 @monitor.py:362]\u001b[0m macro_f1: 0.53011\n",
      "\u001b[32m[1112 12:41:04 @monitor.py:362]\u001b[0m mean_average_precision: 0.56217\n",
      "\u001b[32m[1112 12:41:04 @monitor.py:362]\u001b[0m micro_auc: 0.8522\n",
      "\u001b[32m[1112 12:41:04 @monitor.py:362]\u001b[0m micro_f1: 0.55115\n",
      "\u001b[32m[1112 12:41:04 @monitor.py:362]\u001b[0m one_error: 0.42781\n",
      "\u001b[32m[1112 12:41:04 @monitor.py:362]\u001b[0m ranking_loss: 0.1555\n",
      "\u001b[32m[1112 12:41:04 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.69974\n",
      "\u001b[32m[1112 12:41:04 @monitor.py:362]\u001b[0m training_ap: 0.39092\n",
      "\u001b[32m[1112 12:41:04 @monitor.py:362]\u001b[0m training_auc: 0.74363\n",
      "\u001b[32m[1112 12:41:04 @base.py:257]\u001b[0m Start Epoch 46 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,13.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:05 @base.py:267]\u001b[0m Epoch 46 (global_step 1058) finished, time:1.66 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:06 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-1058.\n",
      "\u001b[32m[1112 12:41:06 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.982\n",
      "\u001b[32m[1112 12:41:06 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.957\n",
      "\u001b[32m[1112 12:41:06 @monitor.py:362]\u001b[0m coverage: 2.2246\n",
      "\u001b[32m[1112 12:41:06 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 1.1229\n",
      "\u001b[32m[1112 12:41:06 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:41:06 @monitor.py:362]\u001b[0m loss/value: 8.4487\n",
      "\u001b[32m[1112 12:41:06 @monitor.py:362]\u001b[0m macro_auc: 0.85041\n",
      "\u001b[32m[1112 12:41:06 @monitor.py:362]\u001b[0m macro_f1: 0.52455\n",
      "\u001b[32m[1112 12:41:06 @monitor.py:362]\u001b[0m mean_average_precision: 0.57069\n",
      "\u001b[32m[1112 12:41:06 @monitor.py:362]\u001b[0m micro_auc: 0.85252\n",
      "\u001b[32m[1112 12:41:06 @monitor.py:362]\u001b[0m micro_f1: 0.54825\n",
      "\u001b[32m[1112 12:41:06 @monitor.py:362]\u001b[0m one_error: 0.44385\n",
      "\u001b[32m[1112 12:41:06 @monitor.py:362]\u001b[0m ranking_loss: 0.15208\n",
      "\u001b[32m[1112 12:41:06 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.69777\n",
      "\u001b[32m[1112 12:41:06 @monitor.py:362]\u001b[0m training_ap: 0.39513\n",
      "\u001b[32m[1112 12:41:06 @monitor.py:362]\u001b[0m training_auc: 0.74638\n",
      "\u001b[32m[1112 12:41:06 @base.py:257]\u001b[0m Start Epoch 47 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:07 @base.py:267]\u001b[0m Epoch 47 (global_step 1081) finished, time:1.60 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:08 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-1081.\n",
      "\u001b[32m[1112 12:41:08 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.987\n",
      "\u001b[32m[1112 12:41:08 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.97\n",
      "\u001b[32m[1112 12:41:08 @monitor.py:362]\u001b[0m coverage: 2.2282\n",
      "\u001b[32m[1112 12:41:08 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.69791\n",
      "\u001b[32m[1112 12:41:08 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:41:08 @monitor.py:362]\u001b[0m loss/value: 8.4472\n",
      "\u001b[32m[1112 12:41:08 @monitor.py:362]\u001b[0m macro_auc: 0.84787\n",
      "\u001b[32m[1112 12:41:08 @monitor.py:362]\u001b[0m macro_f1: 0.52087\n",
      "\u001b[32m[1112 12:41:08 @monitor.py:362]\u001b[0m mean_average_precision: 0.56207\n",
      "\u001b[32m[1112 12:41:08 @monitor.py:362]\u001b[0m micro_auc: 0.85185\n",
      "\u001b[32m[1112 12:41:08 @monitor.py:362]\u001b[0m micro_f1: 0.54174\n",
      "\u001b[32m[1112 12:41:08 @monitor.py:362]\u001b[0m one_error: 0.45276\n",
      "\u001b[32m[1112 12:41:08 @monitor.py:362]\u001b[0m ranking_loss: 0.15546\n",
      "\u001b[32m[1112 12:41:08 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.69119\n",
      "\u001b[32m[1112 12:41:08 @monitor.py:362]\u001b[0m training_ap: 0.39884\n",
      "\u001b[32m[1112 12:41:08 @monitor.py:362]\u001b[0m training_auc: 0.7488\n",
      "\u001b[32m[1112 12:41:08 @base.py:257]\u001b[0m Start Epoch 48 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:10 @base.py:267]\u001b[0m Epoch 48 (global_step 1104) finished, time:1.55 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:10 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-1104.\n",
      "\u001b[32m[1112 12:41:10 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.99\n",
      "\u001b[32m[1112 12:41:10 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.818\n",
      "\u001b[32m[1112 12:41:10 @monitor.py:362]\u001b[0m coverage: 2.246\n",
      "\u001b[32m[1112 12:41:10 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.56201\n",
      "\u001b[32m[1112 12:41:10 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:41:10 @monitor.py:362]\u001b[0m loss/value: 8.4616\n",
      "\u001b[32m[1112 12:41:10 @monitor.py:362]\u001b[0m macro_auc: 0.8503\n",
      "\u001b[32m[1112 12:41:10 @monitor.py:362]\u001b[0m macro_f1: 0.52981\n",
      "\u001b[32m[1112 12:41:10 @monitor.py:362]\u001b[0m mean_average_precision: 0.55505\n",
      "\u001b[32m[1112 12:41:10 @monitor.py:362]\u001b[0m micro_auc: 0.85355\n",
      "\u001b[32m[1112 12:41:10 @monitor.py:362]\u001b[0m micro_f1: 0.55081\n",
      "\u001b[32m[1112 12:41:10 @monitor.py:362]\u001b[0m one_error: 0.45455\n",
      "\u001b[32m[1112 12:41:10 @monitor.py:362]\u001b[0m ranking_loss: 0.15732\n",
      "\u001b[32m[1112 12:41:10 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.68918\n",
      "\u001b[32m[1112 12:41:10 @monitor.py:362]\u001b[0m training_ap: 0.4026\n",
      "\u001b[32m[1112 12:41:10 @monitor.py:362]\u001b[0m training_auc: 0.75117\n",
      "\u001b[32m[1112 12:41:10 @base.py:257]\u001b[0m Start Epoch 49 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:12 @base.py:267]\u001b[0m Epoch 49 (global_step 1127) finished, time:1.58 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:12 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-1127.\n",
      "\u001b[32m[1112 12:41:12 @saver.py:159]\u001b[0m Model with maximum 'micro_f1' saved.\n",
      "\u001b[32m[1112 12:41:12 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.907\n",
      "\u001b[32m[1112 12:41:12 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.917\n",
      "\u001b[32m[1112 12:41:12 @monitor.py:362]\u001b[0m coverage: 2.1729\n",
      "\u001b[32m[1112 12:41:12 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.751\n",
      "\u001b[32m[1112 12:41:12 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:41:12 @monitor.py:362]\u001b[0m loss/value: 8.4813\n",
      "\u001b[32m[1112 12:41:12 @monitor.py:362]\u001b[0m macro_auc: 0.84837\n",
      "\u001b[32m[1112 12:41:12 @monitor.py:362]\u001b[0m macro_f1: 0.54095\n",
      "\u001b[32m[1112 12:41:12 @monitor.py:362]\u001b[0m mean_average_precision: 0.56065\n",
      "\u001b[32m[1112 12:41:12 @monitor.py:362]\u001b[0m micro_auc: 0.85472\n",
      "\u001b[32m[1112 12:41:12 @monitor.py:362]\u001b[0m micro_f1: 0.56079\n",
      "\u001b[32m[1112 12:41:12 @monitor.py:362]\u001b[0m one_error: 0.43672\n",
      "\u001b[32m[1112 12:41:12 @monitor.py:362]\u001b[0m ranking_loss: 0.14944\n",
      "\u001b[32m[1112 12:41:12 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70116\n",
      "\u001b[32m[1112 12:41:12 @monitor.py:362]\u001b[0m training_ap: 0.40597\n",
      "\u001b[32m[1112 12:41:12 @monitor.py:362]\u001b[0m training_auc: 0.75339\n",
      "\u001b[32m[1112 12:41:12 @base.py:257]\u001b[0m Start Epoch 50 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:14 @base.py:267]\u001b[0m Epoch 50 (global_step 1150) finished, time:1.55 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:14 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-1150.\n",
      "\u001b[32m[1112 12:41:14 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.886\n",
      "\u001b[32m[1112 12:41:14 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.917\n",
      "\u001b[32m[1112 12:41:14 @monitor.py:362]\u001b[0m coverage: 2.1462\n",
      "\u001b[32m[1112 12:41:14 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.7949\n",
      "\u001b[32m[1112 12:41:14 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:41:14 @monitor.py:362]\u001b[0m loss/value: 8.4805\n",
      "\u001b[32m[1112 12:41:14 @monitor.py:362]\u001b[0m macro_auc: 0.85739\n",
      "\u001b[32m[1112 12:41:14 @monitor.py:362]\u001b[0m macro_f1: 0.53779\n",
      "\u001b[32m[1112 12:41:14 @monitor.py:362]\u001b[0m mean_average_precision: 0.58227\n",
      "\u001b[32m[1112 12:41:14 @monitor.py:362]\u001b[0m micro_auc: 0.86292\n",
      "\u001b[32m[1112 12:41:14 @monitor.py:362]\u001b[0m micro_f1: 0.55615\n",
      "\u001b[32m[1112 12:41:14 @monitor.py:362]\u001b[0m one_error: 0.42602\n",
      "\u001b[32m[1112 12:41:14 @monitor.py:362]\u001b[0m ranking_loss: 0.14589\n",
      "\u001b[32m[1112 12:41:14 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70702\n",
      "\u001b[32m[1112 12:41:14 @monitor.py:362]\u001b[0m training_ap: 0.40926\n",
      "\u001b[32m[1112 12:41:14 @monitor.py:362]\u001b[0m training_auc: 0.75547\n",
      "\u001b[32m[1112 12:41:14 @base.py:257]\u001b[0m Start Epoch 51 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:16 @base.py:267]\u001b[0m Epoch 51 (global_step 1173) finished, time:1.53 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:16 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-1173.\n",
      "\u001b[32m[1112 12:41:16 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.876\n",
      "\u001b[32m[1112 12:41:16 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.902\n",
      "\u001b[32m[1112 12:41:16 @monitor.py:362]\u001b[0m coverage: 2.2709\n",
      "\u001b[32m[1112 12:41:16 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.83544\n",
      "\u001b[32m[1112 12:41:16 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:41:16 @monitor.py:362]\u001b[0m loss/value: 8.4131\n",
      "\u001b[32m[1112 12:41:16 @monitor.py:362]\u001b[0m macro_auc: 0.8353\n",
      "\u001b[32m[1112 12:41:16 @monitor.py:362]\u001b[0m macro_f1: 0.51835\n",
      "\u001b[32m[1112 12:41:16 @monitor.py:362]\u001b[0m mean_average_precision: 0.54329\n",
      "\u001b[32m[1112 12:41:16 @monitor.py:362]\u001b[0m micro_auc: 0.84568\n",
      "\u001b[32m[1112 12:41:16 @monitor.py:362]\u001b[0m micro_f1: 0.54532\n",
      "\u001b[32m[1112 12:41:16 @monitor.py:362]\u001b[0m one_error: 0.43672\n",
      "\u001b[32m[1112 12:41:16 @monitor.py:362]\u001b[0m ranking_loss: 0.15973\n",
      "\u001b[32m[1112 12:41:16 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.69699\n",
      "\u001b[32m[1112 12:41:16 @monitor.py:362]\u001b[0m training_ap: 0.41251\n",
      "\u001b[32m[1112 12:41:16 @monitor.py:362]\u001b[0m training_auc: 0.75755\n",
      "\u001b[32m[1112 12:41:16 @base.py:257]\u001b[0m Start Epoch 52 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,16.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:18 @base.py:267]\u001b[0m Epoch 52 (global_step 1196) finished, time:1.44 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:18 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-1196.\n",
      "\u001b[32m[1112 12:41:18 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.868\n",
      "\u001b[32m[1112 12:41:18 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.837\n",
      "\u001b[32m[1112 12:41:18 @monitor.py:362]\u001b[0m coverage: 2.1836\n",
      "\u001b[32m[1112 12:41:18 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.29857\n",
      "\u001b[32m[1112 12:41:18 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:41:18 @monitor.py:362]\u001b[0m loss/value: 8.3998\n",
      "\u001b[32m[1112 12:41:18 @monitor.py:362]\u001b[0m macro_auc: 0.84839\n",
      "\u001b[32m[1112 12:41:18 @monitor.py:362]\u001b[0m macro_f1: 0.52654\n",
      "\u001b[32m[1112 12:41:18 @monitor.py:362]\u001b[0m mean_average_precision: 0.55246\n",
      "\u001b[32m[1112 12:41:18 @monitor.py:362]\u001b[0m micro_auc: 0.85254\n",
      "\u001b[32m[1112 12:41:18 @monitor.py:362]\u001b[0m micro_f1: 0.54929\n",
      "\u001b[32m[1112 12:41:18 @monitor.py:362]\u001b[0m one_error: 0.4385\n",
      "\u001b[32m[1112 12:41:18 @monitor.py:362]\u001b[0m ranking_loss: 0.14957\n",
      "\u001b[32m[1112 12:41:18 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.69864\n",
      "\u001b[32m[1112 12:41:18 @monitor.py:362]\u001b[0m training_ap: 0.41572\n",
      "\u001b[32m[1112 12:41:18 @monitor.py:362]\u001b[0m training_auc: 0.7596\n",
      "\u001b[32m[1112 12:41:18 @base.py:257]\u001b[0m Start Epoch 53 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:20 @base.py:267]\u001b[0m Epoch 53 (global_step 1219) finished, time:1.54 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:21 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-1219.\n",
      "\u001b[32m[1112 12:41:21 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.862\n",
      "\u001b[32m[1112 12:41:21 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.75\n",
      "\u001b[32m[1112 12:41:21 @monitor.py:362]\u001b[0m coverage: 2.2531\n",
      "\u001b[32m[1112 12:41:21 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 1.6868\n",
      "\u001b[32m[1112 12:41:21 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:41:21 @monitor.py:362]\u001b[0m loss/value: 8.3997\n",
      "\u001b[32m[1112 12:41:21 @monitor.py:362]\u001b[0m macro_auc: 0.83401\n",
      "\u001b[32m[1112 12:41:21 @monitor.py:362]\u001b[0m macro_f1: 0.51299\n",
      "\u001b[32m[1112 12:41:21 @monitor.py:362]\u001b[0m mean_average_precision: 0.53393\n",
      "\u001b[32m[1112 12:41:21 @monitor.py:362]\u001b[0m micro_auc: 0.84612\n",
      "\u001b[32m[1112 12:41:21 @monitor.py:362]\u001b[0m micro_f1: 0.53876\n",
      "\u001b[32m[1112 12:41:21 @monitor.py:362]\u001b[0m one_error: 0.45633\n",
      "\u001b[32m[1112 12:41:21 @monitor.py:362]\u001b[0m ranking_loss: 0.15881\n",
      "\u001b[32m[1112 12:41:21 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.6906\n",
      "\u001b[32m[1112 12:41:21 @monitor.py:362]\u001b[0m training_ap: 0.4188\n",
      "\u001b[32m[1112 12:41:21 @monitor.py:362]\u001b[0m training_auc: 0.76154\n",
      "\u001b[32m[1112 12:41:21 @base.py:257]\u001b[0m Start Epoch 54 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:22 @base.py:267]\u001b[0m Epoch 54 (global_step 1242) finished, time:1.54 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:23 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-1242.\n",
      "\u001b[32m[1112 12:41:23 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.858\n",
      "\u001b[32m[1112 12:41:23 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.831\n",
      "\u001b[32m[1112 12:41:23 @monitor.py:362]\u001b[0m coverage: 2.2299\n",
      "\u001b[32m[1112 12:41:23 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.53033\n",
      "\u001b[32m[1112 12:41:23 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:41:23 @monitor.py:362]\u001b[0m loss/value: 8.2905\n",
      "\u001b[32m[1112 12:41:23 @monitor.py:362]\u001b[0m macro_auc: 0.8398\n",
      "\u001b[32m[1112 12:41:23 @monitor.py:362]\u001b[0m macro_f1: 0.53351\n",
      "\u001b[32m[1112 12:41:23 @monitor.py:362]\u001b[0m mean_average_precision: 0.55515\n",
      "\u001b[32m[1112 12:41:23 @monitor.py:362]\u001b[0m micro_auc: 0.84813\n",
      "\u001b[32m[1112 12:41:23 @monitor.py:362]\u001b[0m micro_f1: 0.55441\n",
      "\u001b[32m[1112 12:41:23 @monitor.py:362]\u001b[0m one_error: 0.42959\n",
      "\u001b[32m[1112 12:41:23 @monitor.py:362]\u001b[0m ranking_loss: 0.15449\n",
      "\u001b[32m[1112 12:41:23 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.69872\n",
      "\u001b[32m[1112 12:41:23 @monitor.py:362]\u001b[0m training_ap: 0.42191\n",
      "\u001b[32m[1112 12:41:23 @monitor.py:362]\u001b[0m training_auc: 0.76356\n",
      "\u001b[32m[1112 12:41:23 @base.py:257]\u001b[0m Start Epoch 55 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:24 @base.py:267]\u001b[0m Epoch 55 (global_step 1265) finished, time:1.60 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,11.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:25 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-1265.\n",
      "\u001b[32m[1112 12:41:25 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.896\n",
      "\u001b[32m[1112 12:41:25 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.873\n",
      "\u001b[32m[1112 12:41:25 @monitor.py:362]\u001b[0m coverage: 2.2389\n",
      "\u001b[32m[1112 12:41:25 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.37596\n",
      "\u001b[32m[1112 12:41:25 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:41:25 @monitor.py:362]\u001b[0m loss/value: 8.3294\n",
      "\u001b[32m[1112 12:41:25 @monitor.py:362]\u001b[0m macro_auc: 0.84128\n",
      "\u001b[32m[1112 12:41:25 @monitor.py:362]\u001b[0m macro_f1: 0.51383\n",
      "\u001b[32m[1112 12:41:25 @monitor.py:362]\u001b[0m mean_average_precision: 0.54215\n",
      "\u001b[32m[1112 12:41:25 @monitor.py:362]\u001b[0m micro_auc: 0.84981\n",
      "\u001b[32m[1112 12:41:25 @monitor.py:362]\u001b[0m micro_f1: 0.5461\n",
      "\u001b[32m[1112 12:41:25 @monitor.py:362]\u001b[0m one_error: 0.44563\n",
      "\u001b[32m[1112 12:41:25 @monitor.py:362]\u001b[0m ranking_loss: 0.15616\n",
      "\u001b[32m[1112 12:41:25 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.69421\n",
      "\u001b[32m[1112 12:41:25 @monitor.py:362]\u001b[0m training_ap: 0.42495\n",
      "\u001b[32m[1112 12:41:25 @monitor.py:362]\u001b[0m training_auc: 0.76552\n",
      "\u001b[32m[1112 12:41:25 @base.py:257]\u001b[0m Start Epoch 56 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:26 @base.py:267]\u001b[0m Epoch 56 (global_step 1288) finished, time:1.54 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:27 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-1288.\n",
      "\u001b[32m[1112 12:41:27 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.883\n",
      "\u001b[32m[1112 12:41:27 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.892\n",
      "\u001b[32m[1112 12:41:27 @monitor.py:362]\u001b[0m coverage: 2.2513\n",
      "\u001b[32m[1112 12:41:27 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.425\n",
      "\u001b[32m[1112 12:41:27 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:41:27 @monitor.py:362]\u001b[0m loss/value: 8.4837\n",
      "\u001b[32m[1112 12:41:27 @monitor.py:362]\u001b[0m macro_auc: 0.84751\n",
      "\u001b[32m[1112 12:41:27 @monitor.py:362]\u001b[0m macro_f1: 0.53024\n",
      "\u001b[32m[1112 12:41:27 @monitor.py:362]\u001b[0m mean_average_precision: 0.54589\n",
      "\u001b[32m[1112 12:41:27 @monitor.py:362]\u001b[0m micro_auc: 0.85134\n",
      "\u001b[32m[1112 12:41:27 @monitor.py:362]\u001b[0m micro_f1: 0.547\n",
      "\u001b[32m[1112 12:41:27 @monitor.py:362]\u001b[0m one_error: 0.44742\n",
      "\u001b[32m[1112 12:41:27 @monitor.py:362]\u001b[0m ranking_loss: 0.15851\n",
      "\u001b[32m[1112 12:41:27 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.69105\n",
      "\u001b[32m[1112 12:41:27 @monitor.py:362]\u001b[0m training_ap: 0.42755\n",
      "\u001b[32m[1112 12:41:27 @monitor.py:362]\u001b[0m training_auc: 0.76716\n",
      "\u001b[32m[1112 12:41:27 @base.py:257]\u001b[0m Start Epoch 57 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,13.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:29 @base.py:267]\u001b[0m Epoch 57 (global_step 1311) finished, time:1.67 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:29 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-1311.\n",
      "\u001b[32m[1112 12:41:29 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.914\n",
      "\u001b[32m[1112 12:41:29 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.778\n",
      "\u001b[32m[1112 12:41:29 @monitor.py:362]\u001b[0m coverage: 2.2389\n",
      "\u001b[32m[1112 12:41:29 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.55776\n",
      "\u001b[32m[1112 12:41:29 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:41:29 @monitor.py:362]\u001b[0m loss/value: 8.3878\n",
      "\u001b[32m[1112 12:41:29 @monitor.py:362]\u001b[0m macro_auc: 0.84373\n",
      "\u001b[32m[1112 12:41:29 @monitor.py:362]\u001b[0m macro_f1: 0.51871\n",
      "\u001b[32m[1112 12:41:29 @monitor.py:362]\u001b[0m mean_average_precision: 0.55775\n",
      "\u001b[32m[1112 12:41:29 @monitor.py:362]\u001b[0m micro_auc: 0.85073\n",
      "\u001b[32m[1112 12:41:29 @monitor.py:362]\u001b[0m micro_f1: 0.54309\n",
      "\u001b[32m[1112 12:41:29 @monitor.py:362]\u001b[0m one_error: 0.45098\n",
      "\u001b[32m[1112 12:41:29 @monitor.py:362]\u001b[0m ranking_loss: 0.15539\n",
      "\u001b[32m[1112 12:41:29 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.69089\n",
      "\u001b[32m[1112 12:41:29 @monitor.py:362]\u001b[0m training_ap: 0.43023\n",
      "\u001b[32m[1112 12:41:29 @monitor.py:362]\u001b[0m training_auc: 0.76881\n",
      "\u001b[32m[1112 12:41:29 @base.py:257]\u001b[0m Start Epoch 58 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:31 @base.py:267]\u001b[0m Epoch 58 (global_step 1334) finished, time:1.51 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:31 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-1334.\n",
      "\u001b[32m[1112 12:41:31 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.891\n",
      "\u001b[32m[1112 12:41:31 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.833\n",
      "\u001b[32m[1112 12:41:31 @monitor.py:362]\u001b[0m coverage: 2.1444\n",
      "\u001b[32m[1112 12:41:31 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.34312\n",
      "\u001b[32m[1112 12:41:31 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:41:31 @monitor.py:362]\u001b[0m loss/value: 8.2457\n",
      "\u001b[32m[1112 12:41:31 @monitor.py:362]\u001b[0m macro_auc: 0.84867\n",
      "\u001b[32m[1112 12:41:31 @monitor.py:362]\u001b[0m macro_f1: 0.53221\n",
      "\u001b[32m[1112 12:41:31 @monitor.py:362]\u001b[0m mean_average_precision: 0.55705\n",
      "\u001b[32m[1112 12:41:31 @monitor.py:362]\u001b[0m micro_auc: 0.85612\n",
      "\u001b[32m[1112 12:41:31 @monitor.py:362]\u001b[0m micro_f1: 0.55688\n",
      "\u001b[32m[1112 12:41:31 @monitor.py:362]\u001b[0m one_error: 0.44563\n",
      "\u001b[32m[1112 12:41:31 @monitor.py:362]\u001b[0m ranking_loss: 0.14833\n",
      "\u001b[32m[1112 12:41:31 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.7011\n",
      "\u001b[32m[1112 12:41:31 @monitor.py:362]\u001b[0m training_ap: 0.43304\n",
      "\u001b[32m[1112 12:41:31 @monitor.py:362]\u001b[0m training_auc: 0.77053\n",
      "\u001b[32m[1112 12:41:31 @base.py:257]\u001b[0m Start Epoch 59 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:33 @base.py:267]\u001b[0m Epoch 59 (global_step 1357) finished, time:1.47 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:33 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-1357.\n",
      "\u001b[32m[1112 12:41:33 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.92\n",
      "\u001b[32m[1112 12:41:33 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.865\n",
      "\u001b[32m[1112 12:41:33 @monitor.py:362]\u001b[0m coverage: 2.164\n",
      "\u001b[32m[1112 12:41:33 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.45042\n",
      "\u001b[32m[1112 12:41:33 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:41:33 @monitor.py:362]\u001b[0m loss/value: 8.2462\n",
      "\u001b[32m[1112 12:41:33 @monitor.py:362]\u001b[0m macro_auc: 0.85564\n",
      "\u001b[32m[1112 12:41:33 @monitor.py:362]\u001b[0m macro_f1: 0.53368\n",
      "\u001b[32m[1112 12:41:33 @monitor.py:362]\u001b[0m mean_average_precision: 0.5681\n",
      "\u001b[32m[1112 12:41:33 @monitor.py:362]\u001b[0m micro_auc: 0.85821\n",
      "\u001b[32m[1112 12:41:33 @monitor.py:362]\u001b[0m micro_f1: 0.55565\n",
      "\u001b[32m[1112 12:41:33 @monitor.py:362]\u001b[0m one_error: 0.4082\n",
      "\u001b[32m[1112 12:41:33 @monitor.py:362]\u001b[0m ranking_loss: 0.14765\n",
      "\u001b[32m[1112 12:41:33 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.71597\n",
      "\u001b[32m[1112 12:41:33 @monitor.py:362]\u001b[0m training_ap: 0.43559\n",
      "\u001b[32m[1112 12:41:33 @monitor.py:362]\u001b[0m training_auc: 0.77213\n",
      "\u001b[32m[1112 12:41:33 @base.py:257]\u001b[0m Start Epoch 60 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:35 @base.py:267]\u001b[0m Epoch 60 (global_step 1380) finished, time:1.60 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:35 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-1380.\n",
      "\u001b[32m[1112 12:41:35 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.941\n",
      "\u001b[32m[1112 12:41:35 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.883\n",
      "\u001b[32m[1112 12:41:35 @monitor.py:362]\u001b[0m coverage: 2.205\n",
      "\u001b[32m[1112 12:41:35 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 1.4539\n",
      "\u001b[32m[1112 12:41:35 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:41:35 @monitor.py:362]\u001b[0m loss/value: 8.2273\n",
      "\u001b[32m[1112 12:41:35 @monitor.py:362]\u001b[0m macro_auc: 0.8517\n",
      "\u001b[32m[1112 12:41:35 @monitor.py:362]\u001b[0m macro_f1: 0.5349\n",
      "\u001b[32m[1112 12:41:35 @monitor.py:362]\u001b[0m mean_average_precision: 0.56489\n",
      "\u001b[32m[1112 12:41:35 @monitor.py:362]\u001b[0m micro_auc: 0.85484\n",
      "\u001b[32m[1112 12:41:35 @monitor.py:362]\u001b[0m micro_f1: 0.55381\n",
      "\u001b[32m[1112 12:41:35 @monitor.py:362]\u001b[0m one_error: 0.41176\n",
      "\u001b[32m[1112 12:41:35 @monitor.py:362]\u001b[0m ranking_loss: 0.15053\n",
      "\u001b[32m[1112 12:41:35 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.71156\n",
      "\u001b[32m[1112 12:41:35 @monitor.py:362]\u001b[0m training_ap: 0.43818\n",
      "\u001b[32m[1112 12:41:35 @monitor.py:362]\u001b[0m training_auc: 0.77379\n",
      "\u001b[32m[1112 12:41:35 @base.py:257]\u001b[0m Start Epoch 61 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:37 @base.py:267]\u001b[0m Epoch 61 (global_step 1403) finished, time:1.56 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:38 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-1403.\n",
      "\u001b[32m[1112 12:41:38 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.957\n",
      "\u001b[32m[1112 12:41:38 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.83\n",
      "\u001b[32m[1112 12:41:38 @monitor.py:362]\u001b[0m coverage: 2.1551\n",
      "\u001b[32m[1112 12:41:38 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.36527\n",
      "\u001b[32m[1112 12:41:38 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:41:38 @monitor.py:362]\u001b[0m loss/value: 8.2835\n",
      "\u001b[32m[1112 12:41:38 @monitor.py:362]\u001b[0m macro_auc: 0.85274\n",
      "\u001b[32m[1112 12:41:38 @monitor.py:362]\u001b[0m macro_f1: 0.53136\n",
      "\u001b[32m[1112 12:41:38 @monitor.py:362]\u001b[0m mean_average_precision: 0.56067\n",
      "\u001b[32m[1112 12:41:38 @monitor.py:362]\u001b[0m micro_auc: 0.85442\n",
      "\u001b[32m[1112 12:41:38 @monitor.py:362]\u001b[0m micro_f1: 0.55234\n",
      "\u001b[32m[1112 12:41:38 @monitor.py:362]\u001b[0m one_error: 0.43316\n",
      "\u001b[32m[1112 12:41:38 @monitor.py:362]\u001b[0m ranking_loss: 0.14631\n",
      "\u001b[32m[1112 12:41:38 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70329\n",
      "\u001b[32m[1112 12:41:38 @monitor.py:362]\u001b[0m training_ap: 0.44042\n",
      "\u001b[32m[1112 12:41:38 @monitor.py:362]\u001b[0m training_auc: 0.77525\n",
      "\u001b[32m[1112 12:41:38 @base.py:257]\u001b[0m Start Epoch 62 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:39 @base.py:267]\u001b[0m Epoch 62 (global_step 1426) finished, time:1.62 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,11.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:40 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-1426.\n",
      "\u001b[32m[1112 12:41:40 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.928\n",
      "\u001b[32m[1112 12:41:40 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.948\n",
      "\u001b[32m[1112 12:41:40 @monitor.py:362]\u001b[0m coverage: 2.2086\n",
      "\u001b[32m[1112 12:41:40 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.45272\n",
      "\u001b[32m[1112 12:41:40 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:41:40 @monitor.py:362]\u001b[0m loss/value: 8.2729\n",
      "\u001b[32m[1112 12:41:40 @monitor.py:362]\u001b[0m macro_auc: 0.84286\n",
      "\u001b[32m[1112 12:41:40 @monitor.py:362]\u001b[0m macro_f1: 0.52911\n",
      "\u001b[32m[1112 12:41:40 @monitor.py:362]\u001b[0m mean_average_precision: 0.56451\n",
      "\u001b[32m[1112 12:41:40 @monitor.py:362]\u001b[0m micro_auc: 0.85251\n",
      "\u001b[32m[1112 12:41:40 @monitor.py:362]\u001b[0m micro_f1: 0.55188\n",
      "\u001b[32m[1112 12:41:40 @monitor.py:362]\u001b[0m one_error: 0.43137\n",
      "\u001b[32m[1112 12:41:40 @monitor.py:362]\u001b[0m ranking_loss: 0.15598\n",
      "\u001b[32m[1112 12:41:40 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.7006\n",
      "\u001b[32m[1112 12:41:40 @monitor.py:362]\u001b[0m training_ap: 0.4427\n",
      "\u001b[32m[1112 12:41:40 @monitor.py:362]\u001b[0m training_auc: 0.77669\n",
      "\u001b[32m[1112 12:41:40 @base.py:257]\u001b[0m Start Epoch 63 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:41 @base.py:267]\u001b[0m Epoch 63 (global_step 1449) finished, time:1.47 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:42 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-1449.\n",
      "\u001b[32m[1112 12:41:42 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.906\n",
      "\u001b[32m[1112 12:41:42 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.786\n",
      "\u001b[32m[1112 12:41:42 @monitor.py:362]\u001b[0m coverage: 2.2175\n",
      "\u001b[32m[1112 12:41:42 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.71417\n",
      "\u001b[32m[1112 12:41:42 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:41:42 @monitor.py:362]\u001b[0m loss/value: 8.3593\n",
      "\u001b[32m[1112 12:41:42 @monitor.py:362]\u001b[0m macro_auc: 0.8465\n",
      "\u001b[32m[1112 12:41:42 @monitor.py:362]\u001b[0m macro_f1: 0.52748\n",
      "\u001b[32m[1112 12:41:42 @monitor.py:362]\u001b[0m mean_average_precision: 0.55176\n",
      "\u001b[32m[1112 12:41:42 @monitor.py:362]\u001b[0m micro_auc: 0.85369\n",
      "\u001b[32m[1112 12:41:42 @monitor.py:362]\u001b[0m micro_f1: 0.55298\n",
      "\u001b[32m[1112 12:41:42 @monitor.py:362]\u001b[0m one_error: 0.44385\n",
      "\u001b[32m[1112 12:41:42 @monitor.py:362]\u001b[0m ranking_loss: 0.15396\n",
      "\u001b[32m[1112 12:41:42 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.69591\n",
      "\u001b[32m[1112 12:41:42 @monitor.py:362]\u001b[0m training_ap: 0.44498\n",
      "\u001b[32m[1112 12:41:42 @monitor.py:362]\u001b[0m training_auc: 0.778\n",
      "\u001b[32m[1112 12:41:42 @base.py:257]\u001b[0m Start Epoch 64 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:43 @base.py:267]\u001b[0m Epoch 64 (global_step 1472) finished, time:1.61 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,12.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:44 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-1472.\n",
      "\u001b[32m[1112 12:41:44 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.931\n",
      "\u001b[32m[1112 12:41:44 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.901\n",
      "\u001b[32m[1112 12:41:44 @monitor.py:362]\u001b[0m coverage: 2.1176\n",
      "\u001b[32m[1112 12:41:44 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.28736\n",
      "\u001b[32m[1112 12:41:44 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:41:44 @monitor.py:362]\u001b[0m loss/value: 8.3128\n",
      "\u001b[32m[1112 12:41:44 @monitor.py:362]\u001b[0m macro_auc: 0.85004\n",
      "\u001b[32m[1112 12:41:44 @monitor.py:362]\u001b[0m macro_f1: 0.53184\n",
      "\u001b[32m[1112 12:41:44 @monitor.py:362]\u001b[0m mean_average_precision: 0.5591\n",
      "\u001b[32m[1112 12:41:44 @monitor.py:362]\u001b[0m micro_auc: 0.85558\n",
      "\u001b[32m[1112 12:41:44 @monitor.py:362]\u001b[0m micro_f1: 0.55441\n",
      "\u001b[32m[1112 12:41:44 @monitor.py:362]\u001b[0m one_error: 0.42246\n",
      "\u001b[32m[1112 12:41:44 @monitor.py:362]\u001b[0m ranking_loss: 0.14592\n",
      "\u001b[32m[1112 12:41:44 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.71172\n",
      "\u001b[32m[1112 12:41:44 @monitor.py:362]\u001b[0m training_ap: 0.44716\n",
      "\u001b[32m[1112 12:41:44 @monitor.py:362]\u001b[0m training_auc: 0.77936\n",
      "\u001b[32m[1112 12:41:44 @base.py:257]\u001b[0m Start Epoch 65 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:45 @base.py:267]\u001b[0m Epoch 65 (global_step 1495) finished, time:1.59 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:46 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-1495.\n",
      "\u001b[32m[1112 12:41:46 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.949\n",
      "\u001b[32m[1112 12:41:46 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.854\n",
      "\u001b[32m[1112 12:41:46 @monitor.py:362]\u001b[0m coverage: 2.2068\n",
      "\u001b[32m[1112 12:41:46 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.64674\n",
      "\u001b[32m[1112 12:41:46 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:41:46 @monitor.py:362]\u001b[0m loss/value: 8.3355\n",
      "\u001b[32m[1112 12:41:46 @monitor.py:362]\u001b[0m macro_auc: 0.84256\n",
      "\u001b[32m[1112 12:41:46 @monitor.py:362]\u001b[0m macro_f1: 0.52185\n",
      "\u001b[32m[1112 12:41:46 @monitor.py:362]\u001b[0m mean_average_precision: 0.56309\n",
      "\u001b[32m[1112 12:41:46 @monitor.py:362]\u001b[0m micro_auc: 0.85357\n",
      "\u001b[32m[1112 12:41:46 @monitor.py:362]\u001b[0m micro_f1: 0.54911\n",
      "\u001b[32m[1112 12:41:46 @monitor.py:362]\u001b[0m one_error: 0.43316\n",
      "\u001b[32m[1112 12:41:46 @monitor.py:362]\u001b[0m ranking_loss: 0.15296\n",
      "\u001b[32m[1112 12:41:46 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70271\n",
      "\u001b[32m[1112 12:41:46 @monitor.py:362]\u001b[0m training_ap: 0.44936\n",
      "\u001b[32m[1112 12:41:46 @monitor.py:362]\u001b[0m training_auc: 0.78066\n",
      "\u001b[32m[1112 12:41:46 @base.py:257]\u001b[0m Start Epoch 66 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:48 @base.py:267]\u001b[0m Epoch 66 (global_step 1518) finished, time:1.57 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:48 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-1518.\n",
      "\u001b[32m[1112 12:41:48 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.963\n",
      "\u001b[32m[1112 12:41:48 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.862\n",
      "\u001b[32m[1112 12:41:48 @monitor.py:362]\u001b[0m coverage: 2.2852\n",
      "\u001b[32m[1112 12:41:48 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 1.0606\n",
      "\u001b[32m[1112 12:41:48 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:41:48 @monitor.py:362]\u001b[0m loss/value: 8.218\n",
      "\u001b[32m[1112 12:41:48 @monitor.py:362]\u001b[0m macro_auc: 0.84399\n",
      "\u001b[32m[1112 12:41:48 @monitor.py:362]\u001b[0m macro_f1: 0.53035\n",
      "\u001b[32m[1112 12:41:48 @monitor.py:362]\u001b[0m mean_average_precision: 0.55318\n",
      "\u001b[32m[1112 12:41:48 @monitor.py:362]\u001b[0m micro_auc: 0.85037\n",
      "\u001b[32m[1112 12:41:48 @monitor.py:362]\u001b[0m micro_f1: 0.55163\n",
      "\u001b[32m[1112 12:41:48 @monitor.py:362]\u001b[0m one_error: 0.44563\n",
      "\u001b[32m[1112 12:41:48 @monitor.py:362]\u001b[0m ranking_loss: 0.16008\n",
      "\u001b[32m[1112 12:41:48 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.69175\n",
      "\u001b[32m[1112 12:41:48 @monitor.py:362]\u001b[0m training_ap: 0.4518\n",
      "\u001b[32m[1112 12:41:48 @monitor.py:362]\u001b[0m training_auc: 0.78209\n",
      "\u001b[32m[1112 12:41:48 @base.py:257]\u001b[0m Start Epoch 67 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:50 @base.py:267]\u001b[0m Epoch 67 (global_step 1541) finished, time:1.55 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:50 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-1541.\n",
      "\u001b[32m[1112 12:41:50 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.973\n",
      "\u001b[32m[1112 12:41:50 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.862\n",
      "\u001b[32m[1112 12:41:50 @monitor.py:362]\u001b[0m coverage: 2.2139\n",
      "\u001b[32m[1112 12:41:50 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.80403\n",
      "\u001b[32m[1112 12:41:50 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:41:50 @monitor.py:362]\u001b[0m loss/value: 8.2498\n",
      "\u001b[32m[1112 12:41:50 @monitor.py:362]\u001b[0m macro_auc: 0.84725\n",
      "\u001b[32m[1112 12:41:50 @monitor.py:362]\u001b[0m macro_f1: 0.52777\n",
      "\u001b[32m[1112 12:41:50 @monitor.py:362]\u001b[0m mean_average_precision: 0.55699\n",
      "\u001b[32m[1112 12:41:50 @monitor.py:362]\u001b[0m micro_auc: 0.85698\n",
      "\u001b[32m[1112 12:41:50 @monitor.py:362]\u001b[0m micro_f1: 0.5526\n",
      "\u001b[32m[1112 12:41:50 @monitor.py:362]\u001b[0m one_error: 0.43672\n",
      "\u001b[32m[1112 12:41:50 @monitor.py:362]\u001b[0m ranking_loss: 0.15118\n",
      "\u001b[32m[1112 12:41:50 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.69837\n",
      "\u001b[32m[1112 12:41:50 @monitor.py:362]\u001b[0m training_ap: 0.45369\n",
      "\u001b[32m[1112 12:41:50 @monitor.py:362]\u001b[0m training_auc: 0.78325\n",
      "\u001b[32m[1112 12:41:50 @base.py:257]\u001b[0m Start Epoch 68 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:52 @base.py:267]\u001b[0m Epoch 68 (global_step 1564) finished, time:1.57 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:52 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-1564.\n",
      "\u001b[32m[1112 12:41:52 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.939\n",
      "\u001b[32m[1112 12:41:52 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.936\n",
      "\u001b[32m[1112 12:41:52 @monitor.py:362]\u001b[0m coverage: 2.2282\n",
      "\u001b[32m[1112 12:41:52 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.77913\n",
      "\u001b[32m[1112 12:41:52 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:41:52 @monitor.py:362]\u001b[0m loss/value: 8.1285\n",
      "\u001b[32m[1112 12:41:52 @monitor.py:362]\u001b[0m macro_auc: 0.8383\n",
      "\u001b[32m[1112 12:41:52 @monitor.py:362]\u001b[0m macro_f1: 0.52036\n",
      "\u001b[32m[1112 12:41:52 @monitor.py:362]\u001b[0m mean_average_precision: 0.54817\n",
      "\u001b[32m[1112 12:41:52 @monitor.py:362]\u001b[0m micro_auc: 0.84712\n",
      "\u001b[32m[1112 12:41:52 @monitor.py:362]\u001b[0m micro_f1: 0.54631\n",
      "\u001b[32m[1112 12:41:52 @monitor.py:362]\u001b[0m one_error: 0.42959\n",
      "\u001b[32m[1112 12:41:52 @monitor.py:362]\u001b[0m ranking_loss: 0.15249\n",
      "\u001b[32m[1112 12:41:52 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70362\n",
      "\u001b[32m[1112 12:41:52 @monitor.py:362]\u001b[0m training_ap: 0.45584\n",
      "\u001b[32m[1112 12:41:52 @monitor.py:362]\u001b[0m training_auc: 0.78457\n",
      "\u001b[32m[1112 12:41:52 @base.py:257]\u001b[0m Start Epoch 69 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:54 @base.py:267]\u001b[0m Epoch 69 (global_step 1587) finished, time:1.54 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:54 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-1587.\n",
      "\u001b[32m[1112 12:41:54 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.955\n",
      "\u001b[32m[1112 12:41:54 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.893\n",
      "\u001b[32m[1112 12:41:54 @monitor.py:362]\u001b[0m coverage: 2.2175\n",
      "\u001b[32m[1112 12:41:54 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.62126\n",
      "\u001b[32m[1112 12:41:54 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:41:54 @monitor.py:362]\u001b[0m loss/value: 8.1605\n",
      "\u001b[32m[1112 12:41:54 @monitor.py:362]\u001b[0m macro_auc: 0.84818\n",
      "\u001b[32m[1112 12:41:54 @monitor.py:362]\u001b[0m macro_f1: 0.52776\n",
      "\u001b[32m[1112 12:41:54 @monitor.py:362]\u001b[0m mean_average_precision: 0.56175\n",
      "\u001b[32m[1112 12:41:54 @monitor.py:362]\u001b[0m micro_auc: 0.85764\n",
      "\u001b[32m[1112 12:41:54 @monitor.py:362]\u001b[0m micro_f1: 0.55227\n",
      "\u001b[32m[1112 12:41:54 @monitor.py:362]\u001b[0m one_error: 0.44029\n",
      "\u001b[32m[1112 12:41:54 @monitor.py:362]\u001b[0m ranking_loss: 0.15421\n",
      "\u001b[32m[1112 12:41:54 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.69642\n",
      "\u001b[32m[1112 12:41:54 @monitor.py:362]\u001b[0m training_ap: 0.4577\n",
      "\u001b[32m[1112 12:41:54 @monitor.py:362]\u001b[0m training_auc: 0.78573\n",
      "\u001b[32m[1112 12:41:54 @base.py:257]\u001b[0m Start Epoch 70 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:56 @base.py:267]\u001b[0m Epoch 70 (global_step 1610) finished, time:1.56 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 8.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:57 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-1610.\n",
      "\u001b[32m[1112 12:41:57 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.926\n",
      "\u001b[32m[1112 12:41:57 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.797\n",
      "\u001b[32m[1112 12:41:57 @monitor.py:362]\u001b[0m coverage: 2.18\n",
      "\u001b[32m[1112 12:41:57 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.61199\n",
      "\u001b[32m[1112 12:41:57 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:41:57 @monitor.py:362]\u001b[0m loss/value: 8.2983\n",
      "\u001b[32m[1112 12:41:57 @monitor.py:362]\u001b[0m macro_auc: 0.84844\n",
      "\u001b[32m[1112 12:41:57 @monitor.py:362]\u001b[0m macro_f1: 0.52577\n",
      "\u001b[32m[1112 12:41:57 @monitor.py:362]\u001b[0m mean_average_precision: 0.55576\n",
      "\u001b[32m[1112 12:41:57 @monitor.py:362]\u001b[0m micro_auc: 0.85514\n",
      "\u001b[32m[1112 12:41:57 @monitor.py:362]\u001b[0m micro_f1: 0.54584\n",
      "\u001b[32m[1112 12:41:57 @monitor.py:362]\u001b[0m one_error: 0.44563\n",
      "\u001b[32m[1112 12:41:57 @monitor.py:362]\u001b[0m ranking_loss: 0.15176\n",
      "\u001b[32m[1112 12:41:57 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.69704\n",
      "\u001b[32m[1112 12:41:57 @monitor.py:362]\u001b[0m training_ap: 0.45951\n",
      "\u001b[32m[1112 12:41:57 @monitor.py:362]\u001b[0m training_auc: 0.78686\n",
      "\u001b[32m[1112 12:41:57 @base.py:257]\u001b[0m Start Epoch 71 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:58 @base.py:267]\u001b[0m Epoch 71 (global_step 1633) finished, time:1.45 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:41:59 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-1633.\n",
      "\u001b[32m[1112 12:41:59 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.905\n",
      "\u001b[32m[1112 12:41:59 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.85\n",
      "\u001b[32m[1112 12:41:59 @monitor.py:362]\u001b[0m coverage: 2.221\n",
      "\u001b[32m[1112 12:41:59 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.37\n",
      "\u001b[32m[1112 12:41:59 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:41:59 @monitor.py:362]\u001b[0m loss/value: 8.1667\n",
      "\u001b[32m[1112 12:41:59 @monitor.py:362]\u001b[0m macro_auc: 0.84774\n",
      "\u001b[32m[1112 12:41:59 @monitor.py:362]\u001b[0m macro_f1: 0.53257\n",
      "\u001b[32m[1112 12:41:59 @monitor.py:362]\u001b[0m mean_average_precision: 0.5441\n",
      "\u001b[32m[1112 12:41:59 @monitor.py:362]\u001b[0m micro_auc: 0.85244\n",
      "\u001b[32m[1112 12:41:59 @monitor.py:362]\u001b[0m micro_f1: 0.55003\n",
      "\u001b[32m[1112 12:41:59 @monitor.py:362]\u001b[0m one_error: 0.44207\n",
      "\u001b[32m[1112 12:41:59 @monitor.py:362]\u001b[0m ranking_loss: 0.1529\n",
      "\u001b[32m[1112 12:41:59 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.69469\n",
      "\u001b[32m[1112 12:41:59 @monitor.py:362]\u001b[0m training_ap: 0.46135\n",
      "\u001b[32m[1112 12:41:59 @monitor.py:362]\u001b[0m training_auc: 0.78795\n",
      "\u001b[32m[1112 12:41:59 @base.py:257]\u001b[0m Start Epoch 72 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:00 @base.py:267]\u001b[0m Epoch 72 (global_step 1656) finished, time:1.56 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:01 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-1656.\n",
      "\u001b[32m[1112 12:42:01 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.93\n",
      "\u001b[32m[1112 12:42:01 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.851\n",
      "\u001b[32m[1112 12:42:01 @monitor.py:362]\u001b[0m coverage: 2.205\n",
      "\u001b[32m[1112 12:42:01 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.79247\n",
      "\u001b[32m[1112 12:42:01 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:42:01 @monitor.py:362]\u001b[0m loss/value: 8.1981\n",
      "\u001b[32m[1112 12:42:01 @monitor.py:362]\u001b[0m macro_auc: 0.84966\n",
      "\u001b[32m[1112 12:42:01 @monitor.py:362]\u001b[0m macro_f1: 0.53744\n",
      "\u001b[32m[1112 12:42:01 @monitor.py:362]\u001b[0m mean_average_precision: 0.55671\n",
      "\u001b[32m[1112 12:42:01 @monitor.py:362]\u001b[0m micro_auc: 0.85632\n",
      "\u001b[32m[1112 12:42:01 @monitor.py:362]\u001b[0m micro_f1: 0.5557\n",
      "\u001b[32m[1112 12:42:01 @monitor.py:362]\u001b[0m one_error: 0.44385\n",
      "\u001b[32m[1112 12:42:01 @monitor.py:362]\u001b[0m ranking_loss: 0.153\n",
      "\u001b[32m[1112 12:42:01 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.69509\n",
      "\u001b[32m[1112 12:42:01 @monitor.py:362]\u001b[0m training_ap: 0.46324\n",
      "\u001b[32m[1112 12:42:01 @monitor.py:362]\u001b[0m training_auc: 0.78901\n",
      "\u001b[32m[1112 12:42:01 @base.py:257]\u001b[0m Start Epoch 73 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:02 @base.py:267]\u001b[0m Epoch 73 (global_step 1679) finished, time:1.45 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,11.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:03 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-1679.\n",
      "\u001b[32m[1112 12:42:03 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.949\n",
      "\u001b[32m[1112 12:42:03 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.912\n",
      "\u001b[32m[1112 12:42:03 @monitor.py:362]\u001b[0m coverage: 2.1783\n",
      "\u001b[32m[1112 12:42:03 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.36163\n",
      "\u001b[32m[1112 12:42:03 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:42:03 @monitor.py:362]\u001b[0m loss/value: 8.2119\n",
      "\u001b[32m[1112 12:42:03 @monitor.py:362]\u001b[0m macro_auc: 0.84844\n",
      "\u001b[32m[1112 12:42:03 @monitor.py:362]\u001b[0m macro_f1: 0.52218\n",
      "\u001b[32m[1112 12:42:03 @monitor.py:362]\u001b[0m mean_average_precision: 0.55532\n",
      "\u001b[32m[1112 12:42:03 @monitor.py:362]\u001b[0m micro_auc: 0.85797\n",
      "\u001b[32m[1112 12:42:03 @monitor.py:362]\u001b[0m micro_f1: 0.55079\n",
      "\u001b[32m[1112 12:42:03 @monitor.py:362]\u001b[0m one_error: 0.43316\n",
      "\u001b[32m[1112 12:42:03 @monitor.py:362]\u001b[0m ranking_loss: 0.14996\n",
      "\u001b[32m[1112 12:42:03 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70309\n",
      "\u001b[32m[1112 12:42:03 @monitor.py:362]\u001b[0m training_ap: 0.46503\n",
      "\u001b[32m[1112 12:42:03 @monitor.py:362]\u001b[0m training_auc: 0.79005\n",
      "\u001b[32m[1112 12:42:03 @base.py:257]\u001b[0m Start Epoch 74 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:04 @base.py:267]\u001b[0m Epoch 74 (global_step 1702) finished, time:1.56 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:05 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-1702.\n",
      "\u001b[32m[1112 12:42:05 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.962\n",
      "\u001b[32m[1112 12:42:05 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.84\n",
      "\u001b[32m[1112 12:42:05 @monitor.py:362]\u001b[0m coverage: 2.1551\n",
      "\u001b[32m[1112 12:42:05 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.57947\n",
      "\u001b[32m[1112 12:42:05 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:42:05 @monitor.py:362]\u001b[0m loss/value: 8.1153\n",
      "\u001b[32m[1112 12:42:05 @monitor.py:362]\u001b[0m macro_auc: 0.85216\n",
      "\u001b[32m[1112 12:42:05 @monitor.py:362]\u001b[0m macro_f1: 0.53995\n",
      "\u001b[32m[1112 12:42:05 @monitor.py:362]\u001b[0m mean_average_precision: 0.54802\n",
      "\u001b[32m[1112 12:42:05 @monitor.py:362]\u001b[0m micro_auc: 0.85819\n",
      "\u001b[32m[1112 12:42:05 @monitor.py:362]\u001b[0m micro_f1: 0.5578\n",
      "\u001b[32m[1112 12:42:05 @monitor.py:362]\u001b[0m one_error: 0.44029\n",
      "\u001b[32m[1112 12:42:05 @monitor.py:362]\u001b[0m ranking_loss: 0.14968\n",
      "\u001b[32m[1112 12:42:05 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.69822\n",
      "\u001b[32m[1112 12:42:05 @monitor.py:362]\u001b[0m training_ap: 0.467\n",
      "\u001b[32m[1112 12:42:05 @monitor.py:362]\u001b[0m training_auc: 0.79115\n",
      "\u001b[32m[1112 12:42:05 @base.py:257]\u001b[0m Start Epoch 75 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:06 @base.py:267]\u001b[0m Epoch 75 (global_step 1725) finished, time:1.60 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:07 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-1725.\n",
      "\u001b[32m[1112 12:42:07 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.972\n",
      "\u001b[32m[1112 12:42:07 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.815\n",
      "\u001b[32m[1112 12:42:07 @monitor.py:362]\u001b[0m coverage: 2.1907\n",
      "\u001b[32m[1112 12:42:07 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 1.1544\n",
      "\u001b[32m[1112 12:42:07 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:42:07 @monitor.py:362]\u001b[0m loss/value: 8.1544\n",
      "\u001b[32m[1112 12:42:07 @monitor.py:362]\u001b[0m macro_auc: 0.84489\n",
      "\u001b[32m[1112 12:42:07 @monitor.py:362]\u001b[0m macro_f1: 0.52722\n",
      "\u001b[32m[1112 12:42:07 @monitor.py:362]\u001b[0m mean_average_precision: 0.54724\n",
      "\u001b[32m[1112 12:42:07 @monitor.py:362]\u001b[0m micro_auc: 0.85469\n",
      "\u001b[32m[1112 12:42:07 @monitor.py:362]\u001b[0m micro_f1: 0.55356\n",
      "\u001b[32m[1112 12:42:07 @monitor.py:362]\u001b[0m one_error: 0.43494\n",
      "\u001b[32m[1112 12:42:07 @monitor.py:362]\u001b[0m ranking_loss: 0.14903\n",
      "\u001b[32m[1112 12:42:07 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.69994\n",
      "\u001b[32m[1112 12:42:07 @monitor.py:362]\u001b[0m training_ap: 0.46865\n",
      "\u001b[32m[1112 12:42:07 @monitor.py:362]\u001b[0m training_auc: 0.7921\n",
      "\u001b[32m[1112 12:42:07 @base.py:257]\u001b[0m Start Epoch 76 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:09 @base.py:267]\u001b[0m Epoch 76 (global_step 1748) finished, time:1.58 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:09 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-1748.\n",
      "\u001b[32m[1112 12:42:09 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.98\n",
      "\u001b[32m[1112 12:42:09 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.894\n",
      "\u001b[32m[1112 12:42:09 @monitor.py:362]\u001b[0m coverage: 2.2103\n",
      "\u001b[32m[1112 12:42:09 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.56191\n",
      "\u001b[32m[1112 12:42:09 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:42:09 @monitor.py:362]\u001b[0m loss/value: 8.1719\n",
      "\u001b[32m[1112 12:42:09 @monitor.py:362]\u001b[0m macro_auc: 0.83791\n",
      "\u001b[32m[1112 12:42:09 @monitor.py:362]\u001b[0m macro_f1: 0.52991\n",
      "\u001b[32m[1112 12:42:09 @monitor.py:362]\u001b[0m mean_average_precision: 0.55517\n",
      "\u001b[32m[1112 12:42:09 @monitor.py:362]\u001b[0m micro_auc: 0.85195\n",
      "\u001b[32m[1112 12:42:09 @monitor.py:362]\u001b[0m micro_f1: 0.55253\n",
      "\u001b[32m[1112 12:42:09 @monitor.py:362]\u001b[0m one_error: 0.42068\n",
      "\u001b[32m[1112 12:42:09 @monitor.py:362]\u001b[0m ranking_loss: 0.15312\n",
      "\u001b[32m[1112 12:42:09 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70452\n",
      "\u001b[32m[1112 12:42:09 @monitor.py:362]\u001b[0m training_ap: 0.47042\n",
      "\u001b[32m[1112 12:42:09 @monitor.py:362]\u001b[0m training_auc: 0.79314\n",
      "\u001b[32m[1112 12:42:09 @base.py:257]\u001b[0m Start Epoch 77 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:11 @base.py:267]\u001b[0m Epoch 77 (global_step 1771) finished, time:1.61 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 8.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:11 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-1771.\n",
      "\u001b[32m[1112 12:42:11 @saver.py:159]\u001b[0m Model with maximum 'micro_f1' saved.\n",
      "\u001b[32m[1112 12:42:11 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.944\n",
      "\u001b[32m[1112 12:42:11 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.913\n",
      "\u001b[32m[1112 12:42:11 @monitor.py:362]\u001b[0m coverage: 2.1497\n",
      "\u001b[32m[1112 12:42:11 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.42853\n",
      "\u001b[32m[1112 12:42:11 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:42:11 @monitor.py:362]\u001b[0m loss/value: 8.0366\n",
      "\u001b[32m[1112 12:42:11 @monitor.py:362]\u001b[0m macro_auc: 0.85773\n",
      "\u001b[32m[1112 12:42:11 @monitor.py:362]\u001b[0m macro_f1: 0.54181\n",
      "\u001b[32m[1112 12:42:11 @monitor.py:362]\u001b[0m mean_average_precision: 0.5665\n",
      "\u001b[32m[1112 12:42:11 @monitor.py:362]\u001b[0m micro_auc: 0.86032\n",
      "\u001b[32m[1112 12:42:11 @monitor.py:362]\u001b[0m micro_f1: 0.56351\n",
      "\u001b[32m[1112 12:42:11 @monitor.py:362]\u001b[0m one_error: 0.41176\n",
      "\u001b[32m[1112 12:42:11 @monitor.py:362]\u001b[0m ranking_loss: 0.14712\n",
      "\u001b[32m[1112 12:42:11 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.71219\n",
      "\u001b[32m[1112 12:42:11 @monitor.py:362]\u001b[0m training_ap: 0.47221\n",
      "\u001b[32m[1112 12:42:11 @monitor.py:362]\u001b[0m training_auc: 0.79423\n",
      "\u001b[32m[1112 12:42:11 @base.py:257]\u001b[0m Start Epoch 78 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:13 @base.py:267]\u001b[0m Epoch 78 (global_step 1794) finished, time:1.60 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:14 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-1794.\n",
      "\u001b[32m[1112 12:42:14 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.918\n",
      "\u001b[32m[1112 12:42:14 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.936\n",
      "\u001b[32m[1112 12:42:14 @monitor.py:362]\u001b[0m coverage: 2.1729\n",
      "\u001b[32m[1112 12:42:14 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.43416\n",
      "\u001b[32m[1112 12:42:14 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:42:14 @monitor.py:362]\u001b[0m loss/value: 8.1232\n",
      "\u001b[32m[1112 12:42:14 @monitor.py:362]\u001b[0m macro_auc: 0.85304\n",
      "\u001b[32m[1112 12:42:14 @monitor.py:362]\u001b[0m macro_f1: 0.53969\n",
      "\u001b[32m[1112 12:42:14 @monitor.py:362]\u001b[0m mean_average_precision: 0.55936\n",
      "\u001b[32m[1112 12:42:14 @monitor.py:362]\u001b[0m micro_auc: 0.86094\n",
      "\u001b[32m[1112 12:42:14 @monitor.py:362]\u001b[0m micro_f1: 0.55962\n",
      "\u001b[32m[1112 12:42:14 @monitor.py:362]\u001b[0m one_error: 0.44742\n",
      "\u001b[32m[1112 12:42:14 @monitor.py:362]\u001b[0m ranking_loss: 0.15067\n",
      "\u001b[32m[1112 12:42:14 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.69567\n",
      "\u001b[32m[1112 12:42:14 @monitor.py:362]\u001b[0m training_ap: 0.47388\n",
      "\u001b[32m[1112 12:42:14 @monitor.py:362]\u001b[0m training_auc: 0.79518\n",
      "\u001b[32m[1112 12:42:14 @base.py:257]\u001b[0m Start Epoch 79 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:15 @base.py:267]\u001b[0m Epoch 79 (global_step 1817) finished, time:1.54 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:16 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-1817.\n",
      "\u001b[32m[1112 12:42:16 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.899\n",
      "\u001b[32m[1112 12:42:16 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.888\n",
      "\u001b[32m[1112 12:42:16 @monitor.py:362]\u001b[0m coverage: 2.1141\n",
      "\u001b[32m[1112 12:42:16 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.92078\n",
      "\u001b[32m[1112 12:42:16 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:42:16 @monitor.py:362]\u001b[0m loss/value: 8.0443\n",
      "\u001b[32m[1112 12:42:16 @monitor.py:362]\u001b[0m macro_auc: 0.8476\n",
      "\u001b[32m[1112 12:42:16 @monitor.py:362]\u001b[0m macro_f1: 0.53792\n",
      "\u001b[32m[1112 12:42:16 @monitor.py:362]\u001b[0m mean_average_precision: 0.5582\n",
      "\u001b[32m[1112 12:42:16 @monitor.py:362]\u001b[0m micro_auc: 0.85645\n",
      "\u001b[32m[1112 12:42:16 @monitor.py:362]\u001b[0m micro_f1: 0.55756\n",
      "\u001b[32m[1112 12:42:16 @monitor.py:362]\u001b[0m one_error: 0.42246\n",
      "\u001b[32m[1112 12:42:16 @monitor.py:362]\u001b[0m ranking_loss: 0.14112\n",
      "\u001b[32m[1112 12:42:16 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.71288\n",
      "\u001b[32m[1112 12:42:16 @monitor.py:362]\u001b[0m training_ap: 0.47558\n",
      "\u001b[32m[1112 12:42:16 @monitor.py:362]\u001b[0m training_auc: 0.79616\n",
      "\u001b[32m[1112 12:42:16 @base.py:257]\u001b[0m Start Epoch 80 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,13.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:17 @base.py:267]\u001b[0m Epoch 80 (global_step 1840) finished, time:1.67 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:18 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-1840.\n",
      "\u001b[32m[1112 12:42:18 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.926\n",
      "\u001b[32m[1112 12:42:18 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.915\n",
      "\u001b[32m[1112 12:42:18 @monitor.py:362]\u001b[0m coverage: 2.1676\n",
      "\u001b[32m[1112 12:42:18 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 1.2612\n",
      "\u001b[32m[1112 12:42:18 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:42:18 @monitor.py:362]\u001b[0m loss/value: 8.2499\n",
      "\u001b[32m[1112 12:42:18 @monitor.py:362]\u001b[0m macro_auc: 0.84988\n",
      "\u001b[32m[1112 12:42:18 @monitor.py:362]\u001b[0m macro_f1: 0.53307\n",
      "\u001b[32m[1112 12:42:18 @monitor.py:362]\u001b[0m mean_average_precision: 0.55924\n",
      "\u001b[32m[1112 12:42:18 @monitor.py:362]\u001b[0m micro_auc: 0.85564\n",
      "\u001b[32m[1112 12:42:18 @monitor.py:362]\u001b[0m micro_f1: 0.55492\n",
      "\u001b[32m[1112 12:42:18 @monitor.py:362]\u001b[0m one_error: 0.43494\n",
      "\u001b[32m[1112 12:42:18 @monitor.py:362]\u001b[0m ranking_loss: 0.14742\n",
      "\u001b[32m[1112 12:42:18 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70185\n",
      "\u001b[32m[1112 12:42:18 @monitor.py:362]\u001b[0m training_ap: 0.47711\n",
      "\u001b[32m[1112 12:42:18 @monitor.py:362]\u001b[0m training_auc: 0.79705\n",
      "\u001b[32m[1112 12:42:18 @base.py:257]\u001b[0m Start Epoch 81 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:19 @base.py:267]\u001b[0m Epoch 81 (global_step 1863) finished, time:1.51 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:20 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-1863.\n",
      "\u001b[32m[1112 12:42:20 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.905\n",
      "\u001b[32m[1112 12:42:20 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.957\n",
      "\u001b[32m[1112 12:42:20 @monitor.py:362]\u001b[0m coverage: 2.1622\n",
      "\u001b[32m[1112 12:42:20 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 1.2222\n",
      "\u001b[32m[1112 12:42:20 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:42:20 @monitor.py:362]\u001b[0m loss/value: 8.1452\n",
      "\u001b[32m[1112 12:42:20 @monitor.py:362]\u001b[0m macro_auc: 0.85547\n",
      "\u001b[32m[1112 12:42:20 @monitor.py:362]\u001b[0m macro_f1: 0.53318\n",
      "\u001b[32m[1112 12:42:20 @monitor.py:362]\u001b[0m mean_average_precision: 0.56029\n",
      "\u001b[32m[1112 12:42:20 @monitor.py:362]\u001b[0m micro_auc: 0.86109\n",
      "\u001b[32m[1112 12:42:20 @monitor.py:362]\u001b[0m micro_f1: 0.55548\n",
      "\u001b[32m[1112 12:42:20 @monitor.py:362]\u001b[0m one_error: 0.42959\n",
      "\u001b[32m[1112 12:42:20 @monitor.py:362]\u001b[0m ranking_loss: 0.14709\n",
      "\u001b[32m[1112 12:42:20 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70618\n",
      "\u001b[32m[1112 12:42:20 @monitor.py:362]\u001b[0m training_ap: 0.47851\n",
      "\u001b[32m[1112 12:42:20 @monitor.py:362]\u001b[0m training_auc: 0.79792\n",
      "\u001b[32m[1112 12:42:20 @base.py:257]\u001b[0m Start Epoch 82 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:22 @base.py:267]\u001b[0m Epoch 82 (global_step 1886) finished, time:1.55 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:22 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-1886.\n",
      "\u001b[32m[1112 12:42:22 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.93\n",
      "\u001b[32m[1112 12:42:22 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.824\n",
      "\u001b[32m[1112 12:42:22 @monitor.py:362]\u001b[0m coverage: 2.1586\n",
      "\u001b[32m[1112 12:42:22 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.22454\n",
      "\u001b[32m[1112 12:42:22 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:42:22 @monitor.py:362]\u001b[0m loss/value: 8.0308\n",
      "\u001b[32m[1112 12:42:22 @monitor.py:362]\u001b[0m macro_auc: 0.84699\n",
      "\u001b[32m[1112 12:42:22 @monitor.py:362]\u001b[0m macro_f1: 0.53533\n",
      "\u001b[32m[1112 12:42:22 @monitor.py:362]\u001b[0m mean_average_precision: 0.54364\n",
      "\u001b[32m[1112 12:42:22 @monitor.py:362]\u001b[0m micro_auc: 0.8541\n",
      "\u001b[32m[1112 12:42:22 @monitor.py:362]\u001b[0m micro_f1: 0.55551\n",
      "\u001b[32m[1112 12:42:22 @monitor.py:362]\u001b[0m one_error: 0.41711\n",
      "\u001b[32m[1112 12:42:22 @monitor.py:362]\u001b[0m ranking_loss: 0.14502\n",
      "\u001b[32m[1112 12:42:22 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70914\n",
      "\u001b[32m[1112 12:42:22 @monitor.py:362]\u001b[0m training_ap: 0.48008\n",
      "\u001b[32m[1112 12:42:22 @monitor.py:362]\u001b[0m training_auc: 0.79885\n",
      "\u001b[32m[1112 12:42:22 @base.py:257]\u001b[0m Start Epoch 83 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:24 @base.py:267]\u001b[0m Epoch 83 (global_step 1909) finished, time:1.49 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:24 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-1909.\n",
      "\u001b[32m[1112 12:42:24 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.908\n",
      "\u001b[32m[1112 12:42:24 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.669\n",
      "\u001b[32m[1112 12:42:24 @monitor.py:362]\u001b[0m coverage: 2.2513\n",
      "\u001b[32m[1112 12:42:24 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.57261\n",
      "\u001b[32m[1112 12:42:24 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:42:24 @monitor.py:362]\u001b[0m loss/value: 7.9771\n",
      "\u001b[32m[1112 12:42:24 @monitor.py:362]\u001b[0m macro_auc: 0.84274\n",
      "\u001b[32m[1112 12:42:24 @monitor.py:362]\u001b[0m macro_f1: 0.51914\n",
      "\u001b[32m[1112 12:42:24 @monitor.py:362]\u001b[0m mean_average_precision: 0.55881\n",
      "\u001b[32m[1112 12:42:24 @monitor.py:362]\u001b[0m micro_auc: 0.85083\n",
      "\u001b[32m[1112 12:42:24 @monitor.py:362]\u001b[0m micro_f1: 0.54163\n",
      "\u001b[32m[1112 12:42:24 @monitor.py:362]\u001b[0m one_error: 0.45633\n",
      "\u001b[32m[1112 12:42:24 @monitor.py:362]\u001b[0m ranking_loss: 0.15932\n",
      "\u001b[32m[1112 12:42:24 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.68557\n",
      "\u001b[32m[1112 12:42:24 @monitor.py:362]\u001b[0m training_ap: 0.4817\n",
      "\u001b[32m[1112 12:42:24 @monitor.py:362]\u001b[0m training_auc: 0.79975\n",
      "\u001b[32m[1112 12:42:24 @base.py:257]\u001b[0m Start Epoch 84 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:26 @base.py:267]\u001b[0m Epoch 84 (global_step 1932) finished, time:1.58 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:26 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-1932.\n",
      "\u001b[32m[1112 12:42:26 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.932\n",
      "\u001b[32m[1112 12:42:26 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.818\n",
      "\u001b[32m[1112 12:42:26 @monitor.py:362]\u001b[0m coverage: 2.1854\n",
      "\u001b[32m[1112 12:42:26 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.5277\n",
      "\u001b[32m[1112 12:42:26 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:42:26 @monitor.py:362]\u001b[0m loss/value: 8.0052\n",
      "\u001b[32m[1112 12:42:26 @monitor.py:362]\u001b[0m macro_auc: 0.84951\n",
      "\u001b[32m[1112 12:42:26 @monitor.py:362]\u001b[0m macro_f1: 0.53579\n",
      "\u001b[32m[1112 12:42:26 @monitor.py:362]\u001b[0m mean_average_precision: 0.54165\n",
      "\u001b[32m[1112 12:42:26 @monitor.py:362]\u001b[0m micro_auc: 0.8567\n",
      "\u001b[32m[1112 12:42:26 @monitor.py:362]\u001b[0m micro_f1: 0.56124\n",
      "\u001b[32m[1112 12:42:26 @monitor.py:362]\u001b[0m one_error: 0.43672\n",
      "\u001b[32m[1112 12:42:26 @monitor.py:362]\u001b[0m ranking_loss: 0.15171\n",
      "\u001b[32m[1112 12:42:26 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.7015\n",
      "\u001b[32m[1112 12:42:26 @monitor.py:362]\u001b[0m training_ap: 0.48325\n",
      "\u001b[32m[1112 12:42:26 @monitor.py:362]\u001b[0m training_auc: 0.80061\n",
      "\u001b[32m[1112 12:42:26 @base.py:257]\u001b[0m Start Epoch 85 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:28 @base.py:267]\u001b[0m Epoch 85 (global_step 1955) finished, time:1.63 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:29 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-1955.\n",
      "\u001b[32m[1112 12:42:29 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.95\n",
      "\u001b[32m[1112 12:42:29 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.875\n",
      "\u001b[32m[1112 12:42:29 @monitor.py:362]\u001b[0m coverage: 2.1355\n",
      "\u001b[32m[1112 12:42:29 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.31007\n",
      "\u001b[32m[1112 12:42:29 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:42:29 @monitor.py:362]\u001b[0m loss/value: 7.9019\n",
      "\u001b[32m[1112 12:42:29 @monitor.py:362]\u001b[0m macro_auc: 0.85385\n",
      "\u001b[32m[1112 12:42:29 @monitor.py:362]\u001b[0m macro_f1: 0.54167\n",
      "\u001b[32m[1112 12:42:29 @monitor.py:362]\u001b[0m mean_average_precision: 0.56866\n",
      "\u001b[32m[1112 12:42:29 @monitor.py:362]\u001b[0m micro_auc: 0.85853\n",
      "\u001b[32m[1112 12:42:29 @monitor.py:362]\u001b[0m micro_f1: 0.56207\n",
      "\u001b[32m[1112 12:42:29 @monitor.py:362]\u001b[0m one_error: 0.42068\n",
      "\u001b[32m[1112 12:42:29 @monitor.py:362]\u001b[0m ranking_loss: 0.1463\n",
      "\u001b[32m[1112 12:42:29 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.7105\n",
      "\u001b[32m[1112 12:42:29 @monitor.py:362]\u001b[0m training_ap: 0.48474\n",
      "\u001b[32m[1112 12:42:29 @monitor.py:362]\u001b[0m training_auc: 0.80148\n",
      "\u001b[32m[1112 12:42:29 @base.py:257]\u001b[0m Start Epoch 86 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:30 @base.py:267]\u001b[0m Epoch 86 (global_step 1978) finished, time:1.45 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:31 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-1978.\n",
      "\u001b[32m[1112 12:42:31 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.963\n",
      "\u001b[32m[1112 12:42:31 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.895\n",
      "\u001b[32m[1112 12:42:31 @monitor.py:362]\u001b[0m coverage: 2.1604\n",
      "\u001b[32m[1112 12:42:31 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 1.392\n",
      "\u001b[32m[1112 12:42:31 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:42:31 @monitor.py:362]\u001b[0m loss/value: 7.9869\n",
      "\u001b[32m[1112 12:42:31 @monitor.py:362]\u001b[0m macro_auc: 0.85639\n",
      "\u001b[32m[1112 12:42:31 @monitor.py:362]\u001b[0m macro_f1: 0.54259\n",
      "\u001b[32m[1112 12:42:31 @monitor.py:362]\u001b[0m mean_average_precision: 0.56198\n",
      "\u001b[32m[1112 12:42:31 @monitor.py:362]\u001b[0m micro_auc: 0.85843\n",
      "\u001b[32m[1112 12:42:31 @monitor.py:362]\u001b[0m micro_f1: 0.55798\n",
      "\u001b[32m[1112 12:42:31 @monitor.py:362]\u001b[0m one_error: 0.42781\n",
      "\u001b[32m[1112 12:42:31 @monitor.py:362]\u001b[0m ranking_loss: 0.148\n",
      "\u001b[32m[1112 12:42:31 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70565\n",
      "\u001b[32m[1112 12:42:31 @monitor.py:362]\u001b[0m training_ap: 0.48627\n",
      "\u001b[32m[1112 12:42:31 @monitor.py:362]\u001b[0m training_auc: 0.80237\n",
      "\u001b[32m[1112 12:42:31 @base.py:257]\u001b[0m Start Epoch 87 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########|23/23[00:01<00:00,14.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:33 @base.py:267]\u001b[0m Epoch 87 (global_step 2001) finished, time:1.61 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:33 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-2001.\n",
      "\u001b[32m[1112 12:42:33 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.973\n",
      "\u001b[32m[1112 12:42:33 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.951\n",
      "\u001b[32m[1112 12:42:33 @monitor.py:362]\u001b[0m coverage: 2.2086\n",
      "\u001b[32m[1112 12:42:33 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.8024\n",
      "\u001b[32m[1112 12:42:33 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:42:33 @monitor.py:362]\u001b[0m loss/value: 7.965\n",
      "\u001b[32m[1112 12:42:33 @monitor.py:362]\u001b[0m macro_auc: 0.84344\n",
      "\u001b[32m[1112 12:42:33 @monitor.py:362]\u001b[0m macro_f1: 0.52523\n",
      "\u001b[32m[1112 12:42:33 @monitor.py:362]\u001b[0m mean_average_precision: 0.54047\n",
      "\u001b[32m[1112 12:42:33 @monitor.py:362]\u001b[0m micro_auc: 0.85371\n",
      "\u001b[32m[1112 12:42:33 @monitor.py:362]\u001b[0m micro_f1: 0.55311\n",
      "\u001b[32m[1112 12:42:33 @monitor.py:362]\u001b[0m one_error: 0.44029\n",
      "\u001b[32m[1112 12:42:33 @monitor.py:362]\u001b[0m ranking_loss: 0.15345\n",
      "\u001b[32m[1112 12:42:33 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.69478\n",
      "\u001b[32m[1112 12:42:33 @monitor.py:362]\u001b[0m training_ap: 0.48783\n",
      "\u001b[32m[1112 12:42:33 @monitor.py:362]\u001b[0m training_auc: 0.8032\n",
      "\u001b[32m[1112 12:42:33 @base.py:257]\u001b[0m Start Epoch 88 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:35 @base.py:267]\u001b[0m Epoch 88 (global_step 2024) finished, time:1.50 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:35 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-2024.\n",
      "\u001b[32m[1112 12:42:35 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.98\n",
      "\u001b[32m[1112 12:42:35 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.877\n",
      "\u001b[32m[1112 12:42:35 @monitor.py:362]\u001b[0m coverage: 2.1586\n",
      "\u001b[32m[1112 12:42:35 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.26808\n",
      "\u001b[32m[1112 12:42:35 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:42:35 @monitor.py:362]\u001b[0m loss/value: 7.9798\n",
      "\u001b[32m[1112 12:42:35 @monitor.py:362]\u001b[0m macro_auc: 0.8401\n",
      "\u001b[32m[1112 12:42:35 @monitor.py:362]\u001b[0m macro_f1: 0.53329\n",
      "\u001b[32m[1112 12:42:35 @monitor.py:362]\u001b[0m mean_average_precision: 0.55131\n",
      "\u001b[32m[1112 12:42:35 @monitor.py:362]\u001b[0m micro_auc: 0.85365\n",
      "\u001b[32m[1112 12:42:35 @monitor.py:362]\u001b[0m micro_f1: 0.55549\n",
      "\u001b[32m[1112 12:42:35 @monitor.py:362]\u001b[0m one_error: 0.42246\n",
      "\u001b[32m[1112 12:42:35 @monitor.py:362]\u001b[0m ranking_loss: 0.14525\n",
      "\u001b[32m[1112 12:42:35 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70893\n",
      "\u001b[32m[1112 12:42:35 @monitor.py:362]\u001b[0m training_ap: 0.48928\n",
      "\u001b[32m[1112 12:42:35 @monitor.py:362]\u001b[0m training_auc: 0.80404\n",
      "\u001b[32m[1112 12:42:35 @base.py:257]\u001b[0m Start Epoch 89 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:37 @base.py:267]\u001b[0m Epoch 89 (global_step 2047) finished, time:1.54 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:37 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-2047.\n",
      "\u001b[32m[1112 12:42:37 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.985\n",
      "\u001b[32m[1112 12:42:37 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.811\n",
      "\u001b[32m[1112 12:42:37 @monitor.py:362]\u001b[0m coverage: 2.2032\n",
      "\u001b[32m[1112 12:42:37 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.65001\n",
      "\u001b[32m[1112 12:42:37 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:42:37 @monitor.py:362]\u001b[0m loss/value: 7.9155\n",
      "\u001b[32m[1112 12:42:37 @monitor.py:362]\u001b[0m macro_auc: 0.8469\n",
      "\u001b[32m[1112 12:42:37 @monitor.py:362]\u001b[0m macro_f1: 0.52449\n",
      "\u001b[32m[1112 12:42:37 @monitor.py:362]\u001b[0m mean_average_precision: 0.54581\n",
      "\u001b[32m[1112 12:42:37 @monitor.py:362]\u001b[0m micro_auc: 0.85413\n",
      "\u001b[32m[1112 12:42:37 @monitor.py:362]\u001b[0m micro_f1: 0.54847\n",
      "\u001b[32m[1112 12:42:37 @monitor.py:362]\u001b[0m one_error: 0.41533\n",
      "\u001b[32m[1112 12:42:37 @monitor.py:362]\u001b[0m ranking_loss: 0.15336\n",
      "\u001b[32m[1112 12:42:37 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70507\n",
      "\u001b[32m[1112 12:42:37 @monitor.py:362]\u001b[0m training_ap: 0.49071\n",
      "\u001b[32m[1112 12:42:37 @monitor.py:362]\u001b[0m training_auc: 0.80484\n",
      "\u001b[32m[1112 12:42:37 @base.py:257]\u001b[0m Start Epoch 90 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:39 @base.py:267]\u001b[0m Epoch 90 (global_step 2070) finished, time:1.55 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,11.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:39 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-2070.\n",
      "\u001b[32m[1112 12:42:39 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.989\n",
      "\u001b[32m[1112 12:42:39 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.847\n",
      "\u001b[32m[1112 12:42:39 @monitor.py:362]\u001b[0m coverage: 2.1996\n",
      "\u001b[32m[1112 12:42:39 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.42821\n",
      "\u001b[32m[1112 12:42:39 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:42:39 @monitor.py:362]\u001b[0m loss/value: 7.9908\n",
      "\u001b[32m[1112 12:42:39 @monitor.py:362]\u001b[0m macro_auc: 0.84425\n",
      "\u001b[32m[1112 12:42:39 @monitor.py:362]\u001b[0m macro_f1: 0.52611\n",
      "\u001b[32m[1112 12:42:39 @monitor.py:362]\u001b[0m mean_average_precision: 0.54409\n",
      "\u001b[32m[1112 12:42:39 @monitor.py:362]\u001b[0m micro_auc: 0.85348\n",
      "\u001b[32m[1112 12:42:39 @monitor.py:362]\u001b[0m micro_f1: 0.54866\n",
      "\u001b[32m[1112 12:42:39 @monitor.py:362]\u001b[0m one_error: 0.41533\n",
      "\u001b[32m[1112 12:42:39 @monitor.py:362]\u001b[0m ranking_loss: 0.15145\n",
      "\u001b[32m[1112 12:42:39 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70915\n",
      "\u001b[32m[1112 12:42:39 @monitor.py:362]\u001b[0m training_ap: 0.49208\n",
      "\u001b[32m[1112 12:42:39 @monitor.py:362]\u001b[0m training_auc: 0.80562\n",
      "\u001b[32m[1112 12:42:39 @base.py:257]\u001b[0m Start Epoch 91 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:41 @base.py:267]\u001b[0m Epoch 91 (global_step 2093) finished, time:1.56 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:42 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-2093.\n",
      "\u001b[32m[1112 12:42:42 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.992\n",
      "\u001b[32m[1112 12:42:42 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.8\n",
      "\u001b[32m[1112 12:42:42 @monitor.py:362]\u001b[0m coverage: 2.1658\n",
      "\u001b[32m[1112 12:42:42 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.28659\n",
      "\u001b[32m[1112 12:42:42 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:42:42 @monitor.py:362]\u001b[0m loss/value: 7.882\n",
      "\u001b[32m[1112 12:42:42 @monitor.py:362]\u001b[0m macro_auc: 0.84887\n",
      "\u001b[32m[1112 12:42:42 @monitor.py:362]\u001b[0m macro_f1: 0.53195\n",
      "\u001b[32m[1112 12:42:42 @monitor.py:362]\u001b[0m mean_average_precision: 0.56043\n",
      "\u001b[32m[1112 12:42:42 @monitor.py:362]\u001b[0m micro_auc: 0.85622\n",
      "\u001b[32m[1112 12:42:42 @monitor.py:362]\u001b[0m micro_f1: 0.55307\n",
      "\u001b[32m[1112 12:42:42 @monitor.py:362]\u001b[0m one_error: 0.43137\n",
      "\u001b[32m[1112 12:42:42 @monitor.py:362]\u001b[0m ranking_loss: 0.14727\n",
      "\u001b[32m[1112 12:42:42 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70337\n",
      "\u001b[32m[1112 12:42:42 @monitor.py:362]\u001b[0m training_ap: 0.49344\n",
      "\u001b[32m[1112 12:42:42 @monitor.py:362]\u001b[0m training_auc: 0.80638\n",
      "\u001b[32m[1112 12:42:42 @base.py:257]\u001b[0m Start Epoch 92 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:43 @base.py:267]\u001b[0m Epoch 92 (global_step 2116) finished, time:1.64 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 8.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:44 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-2116.\n",
      "\u001b[32m[1112 12:42:44 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.994\n",
      "\u001b[32m[1112 12:42:44 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.873\n",
      "\u001b[32m[1112 12:42:44 @monitor.py:362]\u001b[0m coverage: 2.1105\n",
      "\u001b[32m[1112 12:42:44 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.77823\n",
      "\u001b[32m[1112 12:42:44 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:42:44 @monitor.py:362]\u001b[0m loss/value: 7.8986\n",
      "\u001b[32m[1112 12:42:44 @monitor.py:362]\u001b[0m macro_auc: 0.86349\n",
      "\u001b[32m[1112 12:42:44 @monitor.py:362]\u001b[0m macro_f1: 0.54231\n",
      "\u001b[32m[1112 12:42:44 @monitor.py:362]\u001b[0m mean_average_precision: 0.55378\n",
      "\u001b[32m[1112 12:42:44 @monitor.py:362]\u001b[0m micro_auc: 0.8662\n",
      "\u001b[32m[1112 12:42:44 @monitor.py:362]\u001b[0m micro_f1: 0.5634\n",
      "\u001b[32m[1112 12:42:44 @monitor.py:362]\u001b[0m one_error: 0.42781\n",
      "\u001b[32m[1112 12:42:44 @monitor.py:362]\u001b[0m ranking_loss: 0.14057\n",
      "\u001b[32m[1112 12:42:44 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70867\n",
      "\u001b[32m[1112 12:42:44 @monitor.py:362]\u001b[0m training_ap: 0.49473\n",
      "\u001b[32m[1112 12:42:44 @monitor.py:362]\u001b[0m training_auc: 0.80717\n",
      "\u001b[32m[1112 12:42:44 @base.py:257]\u001b[0m Start Epoch 93 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:45 @base.py:267]\u001b[0m Epoch 93 (global_step 2139) finished, time:1.60 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:46 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-2139.\n",
      "\u001b[32m[1112 12:42:46 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.996\n",
      "\u001b[32m[1112 12:42:46 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.911\n",
      "\u001b[32m[1112 12:42:46 @monitor.py:362]\u001b[0m coverage: 2.1961\n",
      "\u001b[32m[1112 12:42:46 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 1.5195\n",
      "\u001b[32m[1112 12:42:46 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:42:46 @monitor.py:362]\u001b[0m loss/value: 7.975\n",
      "\u001b[32m[1112 12:42:46 @monitor.py:362]\u001b[0m macro_auc: 0.84155\n",
      "\u001b[32m[1112 12:42:46 @monitor.py:362]\u001b[0m macro_f1: 0.53267\n",
      "\u001b[32m[1112 12:42:46 @monitor.py:362]\u001b[0m mean_average_precision: 0.54129\n",
      "\u001b[32m[1112 12:42:46 @monitor.py:362]\u001b[0m micro_auc: 0.85472\n",
      "\u001b[32m[1112 12:42:46 @monitor.py:362]\u001b[0m micro_f1: 0.55506\n",
      "\u001b[32m[1112 12:42:46 @monitor.py:362]\u001b[0m one_error: 0.42781\n",
      "\u001b[32m[1112 12:42:46 @monitor.py:362]\u001b[0m ranking_loss: 0.15213\n",
      "\u001b[32m[1112 12:42:46 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70125\n",
      "\u001b[32m[1112 12:42:46 @monitor.py:362]\u001b[0m training_ap: 0.49595\n",
      "\u001b[32m[1112 12:42:46 @monitor.py:362]\u001b[0m training_auc: 0.80786\n",
      "\u001b[32m[1112 12:42:46 @base.py:257]\u001b[0m Start Epoch 94 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:48 @base.py:267]\u001b[0m Epoch 94 (global_step 2162) finished, time:1.54 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:48 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-2162.\n",
      "\u001b[32m[1112 12:42:48 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.997\n",
      "\u001b[32m[1112 12:42:48 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.956\n",
      "\u001b[32m[1112 12:42:48 @monitor.py:362]\u001b[0m coverage: 2.1854\n",
      "\u001b[32m[1112 12:42:48 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.41686\n",
      "\u001b[32m[1112 12:42:48 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:42:48 @monitor.py:362]\u001b[0m loss/value: 8.0082\n",
      "\u001b[32m[1112 12:42:48 @monitor.py:362]\u001b[0m macro_auc: 0.84694\n",
      "\u001b[32m[1112 12:42:48 @monitor.py:362]\u001b[0m macro_f1: 0.53408\n",
      "\u001b[32m[1112 12:42:48 @monitor.py:362]\u001b[0m mean_average_precision: 0.57091\n",
      "\u001b[32m[1112 12:42:48 @monitor.py:362]\u001b[0m micro_auc: 0.85241\n",
      "\u001b[32m[1112 12:42:48 @monitor.py:362]\u001b[0m micro_f1: 0.55538\n",
      "\u001b[32m[1112 12:42:48 @monitor.py:362]\u001b[0m one_error: 0.41355\n",
      "\u001b[32m[1112 12:42:48 @monitor.py:362]\u001b[0m ranking_loss: 0.15012\n",
      "\u001b[32m[1112 12:42:48 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.71098\n",
      "\u001b[32m[1112 12:42:48 @monitor.py:362]\u001b[0m training_ap: 0.49712\n",
      "\u001b[32m[1112 12:42:48 @monitor.py:362]\u001b[0m training_auc: 0.80852\n",
      "\u001b[32m[1112 12:42:48 @base.py:257]\u001b[0m Start Epoch 95 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:50 @base.py:267]\u001b[0m Epoch 95 (global_step 2185) finished, time:1.54 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,12.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:50 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-2185.\n",
      "\u001b[32m[1112 12:42:50 @saver.py:159]\u001b[0m Model with maximum 'micro_f1' saved.\n",
      "\u001b[32m[1112 12:42:50 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.957\n",
      "\u001b[32m[1112 12:42:50 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.986\n",
      "\u001b[32m[1112 12:42:50 @monitor.py:362]\u001b[0m coverage: 2.0713\n",
      "\u001b[32m[1112 12:42:50 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.95621\n",
      "\u001b[32m[1112 12:42:50 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:42:50 @monitor.py:362]\u001b[0m loss/value: 7.941\n",
      "\u001b[32m[1112 12:42:50 @monitor.py:362]\u001b[0m macro_auc: 0.86579\n",
      "\u001b[32m[1112 12:42:50 @monitor.py:362]\u001b[0m macro_f1: 0.55863\n",
      "\u001b[32m[1112 12:42:50 @monitor.py:362]\u001b[0m mean_average_precision: 0.56987\n",
      "\u001b[32m[1112 12:42:50 @monitor.py:362]\u001b[0m micro_auc: 0.87028\n",
      "\u001b[32m[1112 12:42:50 @monitor.py:362]\u001b[0m micro_f1: 0.57931\n",
      "\u001b[32m[1112 12:42:50 @monitor.py:362]\u001b[0m one_error: 0.40998\n",
      "\u001b[32m[1112 12:42:50 @monitor.py:362]\u001b[0m ranking_loss: 0.13766\n",
      "\u001b[32m[1112 12:42:50 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.71783\n",
      "\u001b[32m[1112 12:42:50 @monitor.py:362]\u001b[0m training_ap: 0.49836\n",
      "\u001b[32m[1112 12:42:50 @monitor.py:362]\u001b[0m training_auc: 0.80921\n",
      "\u001b[32m[1112 12:42:50 @base.py:257]\u001b[0m Start Epoch 96 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:52 @base.py:267]\u001b[0m Epoch 96 (global_step 2208) finished, time:1.53 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:52 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-2208.\n",
      "\u001b[32m[1112 12:42:52 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.968\n",
      "\u001b[32m[1112 12:42:52 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.91\n",
      "\u001b[32m[1112 12:42:52 @monitor.py:362]\u001b[0m coverage: 2.1943\n",
      "\u001b[32m[1112 12:42:52 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.83751\n",
      "\u001b[32m[1112 12:42:52 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:42:52 @monitor.py:362]\u001b[0m loss/value: 7.8875\n",
      "\u001b[32m[1112 12:42:52 @monitor.py:362]\u001b[0m macro_auc: 0.8452\n",
      "\u001b[32m[1112 12:42:52 @monitor.py:362]\u001b[0m macro_f1: 0.53319\n",
      "\u001b[32m[1112 12:42:52 @monitor.py:362]\u001b[0m mean_average_precision: 0.53923\n",
      "\u001b[32m[1112 12:42:52 @monitor.py:362]\u001b[0m micro_auc: 0.85504\n",
      "\u001b[32m[1112 12:42:52 @monitor.py:362]\u001b[0m micro_f1: 0.55212\n",
      "\u001b[32m[1112 12:42:52 @monitor.py:362]\u001b[0m one_error: 0.43494\n",
      "\u001b[32m[1112 12:42:52 @monitor.py:362]\u001b[0m ranking_loss: 0.15249\n",
      "\u001b[32m[1112 12:42:52 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70092\n",
      "\u001b[32m[1112 12:42:52 @monitor.py:362]\u001b[0m training_ap: 0.49959\n",
      "\u001b[32m[1112 12:42:52 @monitor.py:362]\u001b[0m training_auc: 0.80992\n",
      "\u001b[32m[1112 12:42:52 @base.py:257]\u001b[0m Start Epoch 97 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:54 @base.py:267]\u001b[0m Epoch 97 (global_step 2231) finished, time:1.52 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 8.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:55 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-2231.\n",
      "\u001b[32m[1112 12:42:55 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.936\n",
      "\u001b[32m[1112 12:42:55 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.868\n",
      "\u001b[32m[1112 12:42:55 @monitor.py:362]\u001b[0m coverage: 2.0963\n",
      "\u001b[32m[1112 12:42:55 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.509\n",
      "\u001b[32m[1112 12:42:55 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:42:55 @monitor.py:362]\u001b[0m loss/value: 8.0241\n",
      "\u001b[32m[1112 12:42:55 @monitor.py:362]\u001b[0m macro_auc: 0.85579\n",
      "\u001b[32m[1112 12:42:55 @monitor.py:362]\u001b[0m macro_f1: 0.53588\n",
      "\u001b[32m[1112 12:42:55 @monitor.py:362]\u001b[0m mean_average_precision: 0.55422\n",
      "\u001b[32m[1112 12:42:55 @monitor.py:362]\u001b[0m micro_auc: 0.86094\n",
      "\u001b[32m[1112 12:42:55 @monitor.py:362]\u001b[0m micro_f1: 0.55651\n",
      "\u001b[32m[1112 12:42:55 @monitor.py:362]\u001b[0m one_error: 0.42246\n",
      "\u001b[32m[1112 12:42:55 @monitor.py:362]\u001b[0m ranking_loss: 0.14136\n",
      "\u001b[32m[1112 12:42:55 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70884\n",
      "\u001b[32m[1112 12:42:55 @monitor.py:362]\u001b[0m training_ap: 0.50081\n",
      "\u001b[32m[1112 12:42:55 @monitor.py:362]\u001b[0m training_auc: 0.81062\n",
      "\u001b[32m[1112 12:42:55 @base.py:257]\u001b[0m Start Epoch 98 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:56 @base.py:267]\u001b[0m Epoch 98 (global_step 2254) finished, time:1.55 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,11.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:57 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-2254.\n",
      "\u001b[32m[1112 12:42:57 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.869\n",
      "\u001b[32m[1112 12:42:57 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.959\n",
      "\u001b[32m[1112 12:42:57 @monitor.py:362]\u001b[0m coverage: 2.2086\n",
      "\u001b[32m[1112 12:42:57 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.19983\n",
      "\u001b[32m[1112 12:42:57 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:42:57 @monitor.py:362]\u001b[0m loss/value: 7.8298\n",
      "\u001b[32m[1112 12:42:57 @monitor.py:362]\u001b[0m macro_auc: 0.84964\n",
      "\u001b[32m[1112 12:42:57 @monitor.py:362]\u001b[0m macro_f1: 0.53415\n",
      "\u001b[32m[1112 12:42:57 @monitor.py:362]\u001b[0m mean_average_precision: 0.54776\n",
      "\u001b[32m[1112 12:42:57 @monitor.py:362]\u001b[0m micro_auc: 0.85635\n",
      "\u001b[32m[1112 12:42:57 @monitor.py:362]\u001b[0m micro_f1: 0.55146\n",
      "\u001b[32m[1112 12:42:57 @monitor.py:362]\u001b[0m one_error: 0.44563\n",
      "\u001b[32m[1112 12:42:57 @monitor.py:362]\u001b[0m ranking_loss: 0.15243\n",
      "\u001b[32m[1112 12:42:57 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.69279\n",
      "\u001b[32m[1112 12:42:57 @monitor.py:362]\u001b[0m training_ap: 0.50203\n",
      "\u001b[32m[1112 12:42:57 @monitor.py:362]\u001b[0m training_auc: 0.81128\n",
      "\u001b[32m[1112 12:42:57 @base.py:257]\u001b[0m Start Epoch 99 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:58 @base.py:267]\u001b[0m Epoch 99 (global_step 2277) finished, time:1.63 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:42:59 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-2277.\n",
      "\u001b[32m[1112 12:42:59 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.863\n",
      "\u001b[32m[1112 12:42:59 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.958\n",
      "\u001b[32m[1112 12:42:59 @monitor.py:362]\u001b[0m coverage: 2.1836\n",
      "\u001b[32m[1112 12:42:59 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.782\n",
      "\u001b[32m[1112 12:42:59 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:42:59 @monitor.py:362]\u001b[0m loss/value: 7.8031\n",
      "\u001b[32m[1112 12:42:59 @monitor.py:362]\u001b[0m macro_auc: 0.84702\n",
      "\u001b[32m[1112 12:42:59 @monitor.py:362]\u001b[0m macro_f1: 0.52638\n",
      "\u001b[32m[1112 12:42:59 @monitor.py:362]\u001b[0m mean_average_precision: 0.55086\n",
      "\u001b[32m[1112 12:42:59 @monitor.py:362]\u001b[0m micro_auc: 0.85974\n",
      "\u001b[32m[1112 12:42:59 @monitor.py:362]\u001b[0m micro_f1: 0.5508\n",
      "\u001b[32m[1112 12:42:59 @monitor.py:362]\u001b[0m one_error: 0.43672\n",
      "\u001b[32m[1112 12:42:59 @monitor.py:362]\u001b[0m ranking_loss: 0.14971\n",
      "\u001b[32m[1112 12:42:59 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.69893\n",
      "\u001b[32m[1112 12:42:59 @monitor.py:362]\u001b[0m training_ap: 0.50342\n",
      "\u001b[32m[1112 12:42:59 @monitor.py:362]\u001b[0m training_auc: 0.81199\n",
      "\u001b[32m[1112 12:42:59 @base.py:257]\u001b[0m Start Epoch 100 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:00 @base.py:267]\u001b[0m Epoch 100 (global_step 2300) finished, time:1.63 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:01 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-2300.\n",
      "\u001b[32m[1112 12:43:01 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.899\n",
      "\u001b[32m[1112 12:43:01 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.851\n",
      "\u001b[32m[1112 12:43:01 @monitor.py:362]\u001b[0m coverage: 2.1943\n",
      "\u001b[32m[1112 12:43:01 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.48665\n",
      "\u001b[32m[1112 12:43:01 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:43:01 @monitor.py:362]\u001b[0m loss/value: 7.7983\n",
      "\u001b[32m[1112 12:43:01 @monitor.py:362]\u001b[0m macro_auc: 0.84554\n",
      "\u001b[32m[1112 12:43:01 @monitor.py:362]\u001b[0m macro_f1: 0.53417\n",
      "\u001b[32m[1112 12:43:01 @monitor.py:362]\u001b[0m mean_average_precision: 0.55174\n",
      "\u001b[32m[1112 12:43:01 @monitor.py:362]\u001b[0m micro_auc: 0.85217\n",
      "\u001b[32m[1112 12:43:01 @monitor.py:362]\u001b[0m micro_f1: 0.5555\n",
      "\u001b[32m[1112 12:43:01 @monitor.py:362]\u001b[0m one_error: 0.41889\n",
      "\u001b[32m[1112 12:43:01 @monitor.py:362]\u001b[0m ranking_loss: 0.15117\n",
      "\u001b[32m[1112 12:43:01 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70441\n",
      "\u001b[32m[1112 12:43:01 @monitor.py:362]\u001b[0m training_ap: 0.50459\n",
      "\u001b[32m[1112 12:43:01 @monitor.py:362]\u001b[0m training_auc: 0.81267\n",
      "\u001b[32m[1112 12:43:01 @base.py:257]\u001b[0m Start Epoch 101 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:03 @base.py:267]\u001b[0m Epoch 101 (global_step 2323) finished, time:1.64 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,11.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:03 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-2323.\n",
      "\u001b[32m[1112 12:43:03 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.926\n",
      "\u001b[32m[1112 12:43:03 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.9\n",
      "\u001b[32m[1112 12:43:03 @monitor.py:362]\u001b[0m coverage: 2.2068\n",
      "\u001b[32m[1112 12:43:03 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 1.0932\n",
      "\u001b[32m[1112 12:43:03 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:43:03 @monitor.py:362]\u001b[0m loss/value: 7.8322\n",
      "\u001b[32m[1112 12:43:03 @monitor.py:362]\u001b[0m macro_auc: 0.8507\n",
      "\u001b[32m[1112 12:43:03 @monitor.py:362]\u001b[0m macro_f1: 0.52775\n",
      "\u001b[32m[1112 12:43:03 @monitor.py:362]\u001b[0m mean_average_precision: 0.55837\n",
      "\u001b[32m[1112 12:43:03 @monitor.py:362]\u001b[0m micro_auc: 0.85443\n",
      "\u001b[32m[1112 12:43:03 @monitor.py:362]\u001b[0m micro_f1: 0.5456\n",
      "\u001b[32m[1112 12:43:03 @monitor.py:362]\u001b[0m one_error: 0.43672\n",
      "\u001b[32m[1112 12:43:03 @monitor.py:362]\u001b[0m ranking_loss: 0.15202\n",
      "\u001b[32m[1112 12:43:03 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.69612\n",
      "\u001b[32m[1112 12:43:03 @monitor.py:362]\u001b[0m training_ap: 0.5058\n",
      "\u001b[32m[1112 12:43:03 @monitor.py:362]\u001b[0m training_auc: 0.81337\n",
      "\u001b[32m[1112 12:43:03 @base.py:257]\u001b[0m Start Epoch 102 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:05 @base.py:267]\u001b[0m Epoch 102 (global_step 2346) finished, time:1.59 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,11.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:05 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-2346.\n",
      "\u001b[32m[1112 12:43:05 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.905\n",
      "\u001b[32m[1112 12:43:05 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.801\n",
      "\u001b[32m[1112 12:43:05 @monitor.py:362]\u001b[0m coverage: 2.1693\n",
      "\u001b[32m[1112 12:43:05 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.9162\n",
      "\u001b[32m[1112 12:43:05 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:43:05 @monitor.py:362]\u001b[0m loss/value: 7.8881\n",
      "\u001b[32m[1112 12:43:05 @monitor.py:362]\u001b[0m macro_auc: 0.84828\n",
      "\u001b[32m[1112 12:43:05 @monitor.py:362]\u001b[0m macro_f1: 0.52925\n",
      "\u001b[32m[1112 12:43:05 @monitor.py:362]\u001b[0m mean_average_precision: 0.53892\n",
      "\u001b[32m[1112 12:43:05 @monitor.py:362]\u001b[0m micro_auc: 0.85752\n",
      "\u001b[32m[1112 12:43:05 @monitor.py:362]\u001b[0m micro_f1: 0.55263\n",
      "\u001b[32m[1112 12:43:05 @monitor.py:362]\u001b[0m one_error: 0.42602\n",
      "\u001b[32m[1112 12:43:05 @monitor.py:362]\u001b[0m ranking_loss: 0.14799\n",
      "\u001b[32m[1112 12:43:05 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70488\n",
      "\u001b[32m[1112 12:43:05 @monitor.py:362]\u001b[0m training_ap: 0.50702\n",
      "\u001b[32m[1112 12:43:05 @monitor.py:362]\u001b[0m training_auc: 0.81405\n",
      "\u001b[32m[1112 12:43:05 @base.py:257]\u001b[0m Start Epoch 103 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:07 @base.py:267]\u001b[0m Epoch 103 (global_step 2369) finished, time:1.52 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:07 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-2369.\n",
      "\u001b[32m[1112 12:43:07 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.93\n",
      "\u001b[32m[1112 12:43:07 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.816\n",
      "\u001b[32m[1112 12:43:07 @monitor.py:362]\u001b[0m coverage: 2.1818\n",
      "\u001b[32m[1112 12:43:07 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.34935\n",
      "\u001b[32m[1112 12:43:07 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:43:07 @monitor.py:362]\u001b[0m loss/value: 7.7821\n",
      "\u001b[32m[1112 12:43:07 @monitor.py:362]\u001b[0m macro_auc: 0.84689\n",
      "\u001b[32m[1112 12:43:07 @monitor.py:362]\u001b[0m macro_f1: 0.53412\n",
      "\u001b[32m[1112 12:43:07 @monitor.py:362]\u001b[0m mean_average_precision: 0.55668\n",
      "\u001b[32m[1112 12:43:07 @monitor.py:362]\u001b[0m micro_auc: 0.8569\n",
      "\u001b[32m[1112 12:43:07 @monitor.py:362]\u001b[0m micro_f1: 0.55442\n",
      "\u001b[32m[1112 12:43:07 @monitor.py:362]\u001b[0m one_error: 0.42068\n",
      "\u001b[32m[1112 12:43:07 @monitor.py:362]\u001b[0m ranking_loss: 0.14808\n",
      "\u001b[32m[1112 12:43:07 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70966\n",
      "\u001b[32m[1112 12:43:07 @monitor.py:362]\u001b[0m training_ap: 0.50825\n",
      "\u001b[32m[1112 12:43:07 @monitor.py:362]\u001b[0m training_auc: 0.81471\n",
      "\u001b[32m[1112 12:43:07 @base.py:257]\u001b[0m Start Epoch 104 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:09 @base.py:267]\u001b[0m Epoch 104 (global_step 2392) finished, time:1.53 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,11.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:09 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-2392.\n",
      "\u001b[32m[1112 12:43:09 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.908\n",
      "\u001b[32m[1112 12:43:09 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.943\n",
      "\u001b[32m[1112 12:43:09 @monitor.py:362]\u001b[0m coverage: 2.164\n",
      "\u001b[32m[1112 12:43:09 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.31086\n",
      "\u001b[32m[1112 12:43:09 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:43:09 @monitor.py:362]\u001b[0m loss/value: 7.8364\n",
      "\u001b[32m[1112 12:43:09 @monitor.py:362]\u001b[0m macro_auc: 0.84845\n",
      "\u001b[32m[1112 12:43:09 @monitor.py:362]\u001b[0m macro_f1: 0.53332\n",
      "\u001b[32m[1112 12:43:09 @monitor.py:362]\u001b[0m mean_average_precision: 0.56209\n",
      "\u001b[32m[1112 12:43:09 @monitor.py:362]\u001b[0m micro_auc: 0.85814\n",
      "\u001b[32m[1112 12:43:09 @monitor.py:362]\u001b[0m micro_f1: 0.55787\n",
      "\u001b[32m[1112 12:43:09 @monitor.py:362]\u001b[0m one_error: 0.43494\n",
      "\u001b[32m[1112 12:43:09 @monitor.py:362]\u001b[0m ranking_loss: 0.15039\n",
      "\u001b[32m[1112 12:43:09 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.69951\n",
      "\u001b[32m[1112 12:43:09 @monitor.py:362]\u001b[0m training_ap: 0.50933\n",
      "\u001b[32m[1112 12:43:09 @monitor.py:362]\u001b[0m training_auc: 0.81534\n",
      "\u001b[32m[1112 12:43:09 @base.py:257]\u001b[0m Start Epoch 105 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:11 @base.py:267]\u001b[0m Epoch 105 (global_step 2415) finished, time:1.57 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:11 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-2415.\n",
      "\u001b[32m[1112 12:43:11 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.932\n",
      "\u001b[32m[1112 12:43:11 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.906\n",
      "\u001b[32m[1112 12:43:11 @monitor.py:362]\u001b[0m coverage: 2.1622\n",
      "\u001b[32m[1112 12:43:11 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.91812\n",
      "\u001b[32m[1112 12:43:11 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:43:11 @monitor.py:362]\u001b[0m loss/value: 7.8518\n",
      "\u001b[32m[1112 12:43:11 @monitor.py:362]\u001b[0m macro_auc: 0.85737\n",
      "\u001b[32m[1112 12:43:11 @monitor.py:362]\u001b[0m macro_f1: 0.53611\n",
      "\u001b[32m[1112 12:43:11 @monitor.py:362]\u001b[0m mean_average_precision: 0.55801\n",
      "\u001b[32m[1112 12:43:11 @monitor.py:362]\u001b[0m micro_auc: 0.86266\n",
      "\u001b[32m[1112 12:43:11 @monitor.py:362]\u001b[0m micro_f1: 0.55731\n",
      "\u001b[32m[1112 12:43:11 @monitor.py:362]\u001b[0m one_error: 0.41889\n",
      "\u001b[32m[1112 12:43:11 @monitor.py:362]\u001b[0m ranking_loss: 0.14599\n",
      "\u001b[32m[1112 12:43:11 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.71139\n",
      "\u001b[32m[1112 12:43:11 @monitor.py:362]\u001b[0m training_ap: 0.5104\n",
      "\u001b[32m[1112 12:43:11 @monitor.py:362]\u001b[0m training_auc: 0.81595\n",
      "\u001b[32m[1112 12:43:11 @base.py:257]\u001b[0m Start Epoch 106 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:13 @base.py:267]\u001b[0m Epoch 106 (global_step 2438) finished, time:1.62 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:14 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-2438.\n",
      "\u001b[32m[1112 12:43:14 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.91\n",
      "\u001b[32m[1112 12:43:14 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.917\n",
      "\u001b[32m[1112 12:43:14 @monitor.py:362]\u001b[0m coverage: 2.2299\n",
      "\u001b[32m[1112 12:43:14 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 1.2377\n",
      "\u001b[32m[1112 12:43:14 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:43:14 @monitor.py:362]\u001b[0m loss/value: 7.8456\n",
      "\u001b[32m[1112 12:43:14 @monitor.py:362]\u001b[0m macro_auc: 0.84816\n",
      "\u001b[32m[1112 12:43:14 @monitor.py:362]\u001b[0m macro_f1: 0.53609\n",
      "\u001b[32m[1112 12:43:14 @monitor.py:362]\u001b[0m mean_average_precision: 0.553\n",
      "\u001b[32m[1112 12:43:14 @monitor.py:362]\u001b[0m micro_auc: 0.85426\n",
      "\u001b[32m[1112 12:43:14 @monitor.py:362]\u001b[0m micro_f1: 0.55939\n",
      "\u001b[32m[1112 12:43:14 @monitor.py:362]\u001b[0m one_error: 0.42246\n",
      "\u001b[32m[1112 12:43:14 @monitor.py:362]\u001b[0m ranking_loss: 0.15437\n",
      "\u001b[32m[1112 12:43:14 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70231\n",
      "\u001b[32m[1112 12:43:14 @monitor.py:362]\u001b[0m training_ap: 0.51148\n",
      "\u001b[32m[1112 12:43:14 @monitor.py:362]\u001b[0m training_auc: 0.81652\n",
      "\u001b[32m[1112 12:43:14 @base.py:257]\u001b[0m Start Epoch 107 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:15 @base.py:267]\u001b[0m Epoch 107 (global_step 2461) finished, time:1.53 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:16 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-2461.\n",
      "\u001b[32m[1112 12:43:16 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.893\n",
      "\u001b[32m[1112 12:43:16 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.936\n",
      "\u001b[32m[1112 12:43:16 @monitor.py:362]\u001b[0m coverage: 2.1248\n",
      "\u001b[32m[1112 12:43:16 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 1.2947\n",
      "\u001b[32m[1112 12:43:16 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:43:16 @monitor.py:362]\u001b[0m loss/value: 7.8798\n",
      "\u001b[32m[1112 12:43:16 @monitor.py:362]\u001b[0m macro_auc: 0.85184\n",
      "\u001b[32m[1112 12:43:16 @monitor.py:362]\u001b[0m macro_f1: 0.53979\n",
      "\u001b[32m[1112 12:43:16 @monitor.py:362]\u001b[0m mean_average_precision: 0.56534\n",
      "\u001b[32m[1112 12:43:16 @monitor.py:362]\u001b[0m micro_auc: 0.85831\n",
      "\u001b[32m[1112 12:43:16 @monitor.py:362]\u001b[0m micro_f1: 0.56369\n",
      "\u001b[32m[1112 12:43:16 @monitor.py:362]\u001b[0m one_error: 0.41711\n",
      "\u001b[32m[1112 12:43:16 @monitor.py:362]\u001b[0m ranking_loss: 0.14211\n",
      "\u001b[32m[1112 12:43:16 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.71373\n",
      "\u001b[32m[1112 12:43:16 @monitor.py:362]\u001b[0m training_ap: 0.51249\n",
      "\u001b[32m[1112 12:43:16 @monitor.py:362]\u001b[0m training_auc: 0.81708\n",
      "\u001b[32m[1112 12:43:16 @base.py:257]\u001b[0m Start Epoch 108 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:17 @base.py:267]\u001b[0m Epoch 108 (global_step 2484) finished, time:1.48 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:18 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-2484.\n",
      "\u001b[32m[1112 12:43:18 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.921\n",
      "\u001b[32m[1112 12:43:18 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.933\n",
      "\u001b[32m[1112 12:43:18 @monitor.py:362]\u001b[0m coverage: 2.0945\n",
      "\u001b[32m[1112 12:43:18 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.76916\n",
      "\u001b[32m[1112 12:43:18 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:43:18 @monitor.py:362]\u001b[0m loss/value: 7.8567\n",
      "\u001b[32m[1112 12:43:18 @monitor.py:362]\u001b[0m macro_auc: 0.84848\n",
      "\u001b[32m[1112 12:43:18 @monitor.py:362]\u001b[0m macro_f1: 0.54145\n",
      "\u001b[32m[1112 12:43:18 @monitor.py:362]\u001b[0m mean_average_precision: 0.56035\n",
      "\u001b[32m[1112 12:43:18 @monitor.py:362]\u001b[0m micro_auc: 0.85961\n",
      "\u001b[32m[1112 12:43:18 @monitor.py:362]\u001b[0m micro_f1: 0.56445\n",
      "\u001b[32m[1112 12:43:18 @monitor.py:362]\u001b[0m one_error: 0.42246\n",
      "\u001b[32m[1112 12:43:18 @monitor.py:362]\u001b[0m ranking_loss: 0.14135\n",
      "\u001b[32m[1112 12:43:18 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.71321\n",
      "\u001b[32m[1112 12:43:18 @monitor.py:362]\u001b[0m training_ap: 0.51347\n",
      "\u001b[32m[1112 12:43:18 @monitor.py:362]\u001b[0m training_auc: 0.81763\n",
      "\u001b[32m[1112 12:43:18 @base.py:257]\u001b[0m Start Epoch 109 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:19 @base.py:267]\u001b[0m Epoch 109 (global_step 2507) finished, time:1.59 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,12.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:20 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-2507.\n",
      "\u001b[32m[1112 12:43:20 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.901\n",
      "\u001b[32m[1112 12:43:20 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.909\n",
      "\u001b[32m[1112 12:43:20 @monitor.py:362]\u001b[0m coverage: 2.1836\n",
      "\u001b[32m[1112 12:43:20 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.36963\n",
      "\u001b[32m[1112 12:43:20 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:43:20 @monitor.py:362]\u001b[0m loss/value: 7.8126\n",
      "\u001b[32m[1112 12:43:20 @monitor.py:362]\u001b[0m macro_auc: 0.84916\n",
      "\u001b[32m[1112 12:43:20 @monitor.py:362]\u001b[0m macro_f1: 0.53536\n",
      "\u001b[32m[1112 12:43:20 @monitor.py:362]\u001b[0m mean_average_precision: 0.55935\n",
      "\u001b[32m[1112 12:43:20 @monitor.py:362]\u001b[0m micro_auc: 0.85627\n",
      "\u001b[32m[1112 12:43:20 @monitor.py:362]\u001b[0m micro_f1: 0.55683\n",
      "\u001b[32m[1112 12:43:20 @monitor.py:362]\u001b[0m one_error: 0.4385\n",
      "\u001b[32m[1112 12:43:20 @monitor.py:362]\u001b[0m ranking_loss: 0.15053\n",
      "\u001b[32m[1112 12:43:20 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70292\n",
      "\u001b[32m[1112 12:43:20 @monitor.py:362]\u001b[0m training_ap: 0.51453\n",
      "\u001b[32m[1112 12:43:20 @monitor.py:362]\u001b[0m training_auc: 0.81821\n",
      "\u001b[32m[1112 12:43:20 @base.py:257]\u001b[0m Start Epoch 110 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:21 @base.py:267]\u001b[0m Epoch 110 (global_step 2530) finished, time:1.51 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:22 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-2530.\n",
      "\u001b[32m[1112 12:43:22 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.927\n",
      "\u001b[32m[1112 12:43:22 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.892\n",
      "\u001b[32m[1112 12:43:22 @monitor.py:362]\u001b[0m coverage: 2.1747\n",
      "\u001b[32m[1112 12:43:22 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.4367\n",
      "\u001b[32m[1112 12:43:22 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:43:22 @monitor.py:362]\u001b[0m loss/value: 7.9991\n",
      "\u001b[32m[1112 12:43:22 @monitor.py:362]\u001b[0m macro_auc: 0.85411\n",
      "\u001b[32m[1112 12:43:22 @monitor.py:362]\u001b[0m macro_f1: 0.53417\n",
      "\u001b[32m[1112 12:43:22 @monitor.py:362]\u001b[0m mean_average_precision: 0.56089\n",
      "\u001b[32m[1112 12:43:22 @monitor.py:362]\u001b[0m micro_auc: 0.86013\n",
      "\u001b[32m[1112 12:43:22 @monitor.py:362]\u001b[0m micro_f1: 0.55516\n",
      "\u001b[32m[1112 12:43:22 @monitor.py:362]\u001b[0m one_error: 0.42959\n",
      "\u001b[32m[1112 12:43:22 @monitor.py:362]\u001b[0m ranking_loss: 0.1501\n",
      "\u001b[32m[1112 12:43:22 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70541\n",
      "\u001b[32m[1112 12:43:22 @monitor.py:362]\u001b[0m training_ap: 0.51544\n",
      "\u001b[32m[1112 12:43:22 @monitor.py:362]\u001b[0m training_auc: 0.81874\n",
      "\u001b[32m[1112 12:43:22 @base.py:257]\u001b[0m Start Epoch 111 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:24 @base.py:267]\u001b[0m Epoch 111 (global_step 2553) finished, time:1.53 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:24 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-2553.\n",
      "\u001b[32m[1112 12:43:24 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.906\n",
      "\u001b[32m[1112 12:43:24 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.84\n",
      "\u001b[32m[1112 12:43:24 @monitor.py:362]\u001b[0m coverage: 2.2032\n",
      "\u001b[32m[1112 12:43:24 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.48643\n",
      "\u001b[32m[1112 12:43:24 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:43:24 @monitor.py:362]\u001b[0m loss/value: 7.8711\n",
      "\u001b[32m[1112 12:43:24 @monitor.py:362]\u001b[0m macro_auc: 0.8442\n",
      "\u001b[32m[1112 12:43:24 @monitor.py:362]\u001b[0m macro_f1: 0.53031\n",
      "\u001b[32m[1112 12:43:24 @monitor.py:362]\u001b[0m mean_average_precision: 0.53925\n",
      "\u001b[32m[1112 12:43:24 @monitor.py:362]\u001b[0m micro_auc: 0.85361\n",
      "\u001b[32m[1112 12:43:24 @monitor.py:362]\u001b[0m micro_f1: 0.55903\n",
      "\u001b[32m[1112 12:43:24 @monitor.py:362]\u001b[0m one_error: 0.42781\n",
      "\u001b[32m[1112 12:43:24 @monitor.py:362]\u001b[0m ranking_loss: 0.15395\n",
      "\u001b[32m[1112 12:43:24 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70203\n",
      "\u001b[32m[1112 12:43:24 @monitor.py:362]\u001b[0m training_ap: 0.5165\n",
      "\u001b[32m[1112 12:43:24 @monitor.py:362]\u001b[0m training_auc: 0.81929\n",
      "\u001b[32m[1112 12:43:24 @base.py:257]\u001b[0m Start Epoch 112 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:26 @base.py:267]\u001b[0m Epoch 112 (global_step 2576) finished, time:1.59 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:26 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-2576.\n",
      "\u001b[32m[1112 12:43:26 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.931\n",
      "\u001b[32m[1112 12:43:26 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.838\n",
      "\u001b[32m[1112 12:43:26 @monitor.py:362]\u001b[0m coverage: 2.1836\n",
      "\u001b[32m[1112 12:43:26 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 1.3922\n",
      "\u001b[32m[1112 12:43:26 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:43:26 @monitor.py:362]\u001b[0m loss/value: 7.8243\n",
      "\u001b[32m[1112 12:43:26 @monitor.py:362]\u001b[0m macro_auc: 0.84755\n",
      "\u001b[32m[1112 12:43:26 @monitor.py:362]\u001b[0m macro_f1: 0.53335\n",
      "\u001b[32m[1112 12:43:26 @monitor.py:362]\u001b[0m mean_average_precision: 0.54577\n",
      "\u001b[32m[1112 12:43:26 @monitor.py:362]\u001b[0m micro_auc: 0.85506\n",
      "\u001b[32m[1112 12:43:26 @monitor.py:362]\u001b[0m micro_f1: 0.5531\n",
      "\u001b[32m[1112 12:43:26 @monitor.py:362]\u001b[0m one_error: 0.43672\n",
      "\u001b[32m[1112 12:43:26 @monitor.py:362]\u001b[0m ranking_loss: 0.15119\n",
      "\u001b[32m[1112 12:43:26 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70136\n",
      "\u001b[32m[1112 12:43:26 @monitor.py:362]\u001b[0m training_ap: 0.51745\n",
      "\u001b[32m[1112 12:43:26 @monitor.py:362]\u001b[0m training_auc: 0.81982\n",
      "\u001b[32m[1112 12:43:26 @base.py:257]\u001b[0m Start Epoch 113 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:28 @base.py:267]\u001b[0m Epoch 113 (global_step 2599) finished, time:1.59 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:28 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-2599.\n",
      "\u001b[32m[1112 12:43:28 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.908\n",
      "\u001b[32m[1112 12:43:28 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.896\n",
      "\u001b[32m[1112 12:43:28 @monitor.py:362]\u001b[0m coverage: 2.1408\n",
      "\u001b[32m[1112 12:43:28 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.46887\n",
      "\u001b[32m[1112 12:43:28 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:43:28 @monitor.py:362]\u001b[0m loss/value: 7.7992\n",
      "\u001b[32m[1112 12:43:28 @monitor.py:362]\u001b[0m macro_auc: 0.85408\n",
      "\u001b[32m[1112 12:43:28 @monitor.py:362]\u001b[0m macro_f1: 0.55684\n",
      "\u001b[32m[1112 12:43:28 @monitor.py:362]\u001b[0m mean_average_precision: 0.56193\n",
      "\u001b[32m[1112 12:43:28 @monitor.py:362]\u001b[0m micro_auc: 0.86136\n",
      "\u001b[32m[1112 12:43:28 @monitor.py:362]\u001b[0m micro_f1: 0.57046\n",
      "\u001b[32m[1112 12:43:28 @monitor.py:362]\u001b[0m one_error: 0.4385\n",
      "\u001b[32m[1112 12:43:28 @monitor.py:362]\u001b[0m ranking_loss: 0.14555\n",
      "\u001b[32m[1112 12:43:28 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.7046\n",
      "\u001b[32m[1112 12:43:28 @monitor.py:362]\u001b[0m training_ap: 0.51843\n",
      "\u001b[32m[1112 12:43:28 @monitor.py:362]\u001b[0m training_auc: 0.82036\n",
      "\u001b[32m[1112 12:43:28 @base.py:257]\u001b[0m Start Epoch 114 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:30 @base.py:267]\u001b[0m Epoch 114 (global_step 2622) finished, time:1.57 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:31 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-2622.\n",
      "\u001b[32m[1112 12:43:31 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.892\n",
      "\u001b[32m[1112 12:43:31 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.852\n",
      "\u001b[32m[1112 12:43:31 @monitor.py:362]\u001b[0m coverage: 2.1159\n",
      "\u001b[32m[1112 12:43:31 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.47238\n",
      "\u001b[32m[1112 12:43:31 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:43:31 @monitor.py:362]\u001b[0m loss/value: 7.7325\n",
      "\u001b[32m[1112 12:43:31 @monitor.py:362]\u001b[0m macro_auc: 0.85247\n",
      "\u001b[32m[1112 12:43:31 @monitor.py:362]\u001b[0m macro_f1: 0.5359\n",
      "\u001b[32m[1112 12:43:31 @monitor.py:362]\u001b[0m mean_average_precision: 0.56224\n",
      "\u001b[32m[1112 12:43:31 @monitor.py:362]\u001b[0m micro_auc: 0.85947\n",
      "\u001b[32m[1112 12:43:31 @monitor.py:362]\u001b[0m micro_f1: 0.55879\n",
      "\u001b[32m[1112 12:43:31 @monitor.py:362]\u001b[0m one_error: 0.42424\n",
      "\u001b[32m[1112 12:43:31 @monitor.py:362]\u001b[0m ranking_loss: 0.14411\n",
      "\u001b[32m[1112 12:43:31 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.7085\n",
      "\u001b[32m[1112 12:43:31 @monitor.py:362]\u001b[0m training_ap: 0.51951\n",
      "\u001b[32m[1112 12:43:31 @monitor.py:362]\u001b[0m training_auc: 0.82092\n",
      "\u001b[32m[1112 12:43:31 @base.py:257]\u001b[0m Start Epoch 115 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:32 @base.py:267]\u001b[0m Epoch 115 (global_step 2645) finished, time:1.60 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:33 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-2645.\n",
      "\u001b[32m[1112 12:43:33 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.88\n",
      "\u001b[32m[1112 12:43:33 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.874\n",
      "\u001b[32m[1112 12:43:33 @monitor.py:362]\u001b[0m coverage: 2.148\n",
      "\u001b[32m[1112 12:43:33 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 1.3196\n",
      "\u001b[32m[1112 12:43:33 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:43:33 @monitor.py:362]\u001b[0m loss/value: 7.6863\n",
      "\u001b[32m[1112 12:43:33 @monitor.py:362]\u001b[0m macro_auc: 0.84936\n",
      "\u001b[32m[1112 12:43:33 @monitor.py:362]\u001b[0m macro_f1: 0.53554\n",
      "\u001b[32m[1112 12:43:33 @monitor.py:362]\u001b[0m mean_average_precision: 0.55546\n",
      "\u001b[32m[1112 12:43:33 @monitor.py:362]\u001b[0m micro_auc: 0.85974\n",
      "\u001b[32m[1112 12:43:33 @monitor.py:362]\u001b[0m micro_f1: 0.55938\n",
      "\u001b[32m[1112 12:43:33 @monitor.py:362]\u001b[0m one_error: 0.41533\n",
      "\u001b[32m[1112 12:43:33 @monitor.py:362]\u001b[0m ranking_loss: 0.14558\n",
      "\u001b[32m[1112 12:43:33 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.71311\n",
      "\u001b[32m[1112 12:43:33 @monitor.py:362]\u001b[0m training_ap: 0.52057\n",
      "\u001b[32m[1112 12:43:33 @monitor.py:362]\u001b[0m training_auc: 0.82148\n",
      "\u001b[32m[1112 12:43:33 @base.py:257]\u001b[0m Start Epoch 116 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:34 @base.py:267]\u001b[0m Epoch 116 (global_step 2668) finished, time:1.54 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:35 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-2668.\n",
      "\u001b[32m[1112 12:43:35 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.912\n",
      "\u001b[32m[1112 12:43:35 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.836\n",
      "\u001b[32m[1112 12:43:35 @monitor.py:362]\u001b[0m coverage: 2.1551\n",
      "\u001b[32m[1112 12:43:35 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.33693\n",
      "\u001b[32m[1112 12:43:35 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:43:35 @monitor.py:362]\u001b[0m loss/value: 7.7898\n",
      "\u001b[32m[1112 12:43:35 @monitor.py:362]\u001b[0m macro_auc: 0.84569\n",
      "\u001b[32m[1112 12:43:35 @monitor.py:362]\u001b[0m macro_f1: 0.54043\n",
      "\u001b[32m[1112 12:43:35 @monitor.py:362]\u001b[0m mean_average_precision: 0.55727\n",
      "\u001b[32m[1112 12:43:35 @monitor.py:362]\u001b[0m micro_auc: 0.8592\n",
      "\u001b[32m[1112 12:43:35 @monitor.py:362]\u001b[0m micro_f1: 0.56704\n",
      "\u001b[32m[1112 12:43:35 @monitor.py:362]\u001b[0m one_error: 0.42959\n",
      "\u001b[32m[1112 12:43:35 @monitor.py:362]\u001b[0m ranking_loss: 0.14946\n",
      "\u001b[32m[1112 12:43:35 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70347\n",
      "\u001b[32m[1112 12:43:35 @monitor.py:362]\u001b[0m training_ap: 0.52151\n",
      "\u001b[32m[1112 12:43:35 @monitor.py:362]\u001b[0m training_auc: 0.82197\n",
      "\u001b[32m[1112 12:43:35 @base.py:257]\u001b[0m Start Epoch 117 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:36 @base.py:267]\u001b[0m Epoch 117 (global_step 2691) finished, time:1.46 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:37 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-2691.\n",
      "\u001b[32m[1112 12:43:37 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.935\n",
      "\u001b[32m[1112 12:43:37 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.835\n",
      "\u001b[32m[1112 12:43:37 @monitor.py:362]\u001b[0m coverage: 2.0713\n",
      "\u001b[32m[1112 12:43:37 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.36702\n",
      "\u001b[32m[1112 12:43:37 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:43:37 @monitor.py:362]\u001b[0m loss/value: 7.7562\n",
      "\u001b[32m[1112 12:43:37 @monitor.py:362]\u001b[0m macro_auc: 0.85559\n",
      "\u001b[32m[1112 12:43:37 @monitor.py:362]\u001b[0m macro_f1: 0.53625\n",
      "\u001b[32m[1112 12:43:37 @monitor.py:362]\u001b[0m mean_average_precision: 0.55952\n",
      "\u001b[32m[1112 12:43:37 @monitor.py:362]\u001b[0m micro_auc: 0.86157\n",
      "\u001b[32m[1112 12:43:37 @monitor.py:362]\u001b[0m micro_f1: 0.55707\n",
      "\u001b[32m[1112 12:43:37 @monitor.py:362]\u001b[0m one_error: 0.40285\n",
      "\u001b[32m[1112 12:43:37 @monitor.py:362]\u001b[0m ranking_loss: 0.14039\n",
      "\u001b[32m[1112 12:43:37 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.72295\n",
      "\u001b[32m[1112 12:43:37 @monitor.py:362]\u001b[0m training_ap: 0.52242\n",
      "\u001b[32m[1112 12:43:37 @monitor.py:362]\u001b[0m training_auc: 0.82246\n",
      "\u001b[32m[1112 12:43:37 @base.py:257]\u001b[0m Start Epoch 118 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:38 @base.py:267]\u001b[0m Epoch 118 (global_step 2714) finished, time:1.58 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:39 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-2714.\n",
      "\u001b[32m[1112 12:43:39 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.952\n",
      "\u001b[32m[1112 12:43:39 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.901\n",
      "\u001b[32m[1112 12:43:39 @monitor.py:362]\u001b[0m coverage: 2.1676\n",
      "\u001b[32m[1112 12:43:39 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.91403\n",
      "\u001b[32m[1112 12:43:39 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:43:39 @monitor.py:362]\u001b[0m loss/value: 7.6649\n",
      "\u001b[32m[1112 12:43:39 @monitor.py:362]\u001b[0m macro_auc: 0.85078\n",
      "\u001b[32m[1112 12:43:39 @monitor.py:362]\u001b[0m macro_f1: 0.54566\n",
      "\u001b[32m[1112 12:43:39 @monitor.py:362]\u001b[0m mean_average_precision: 0.55217\n",
      "\u001b[32m[1112 12:43:39 @monitor.py:362]\u001b[0m micro_auc: 0.86072\n",
      "\u001b[32m[1112 12:43:39 @monitor.py:362]\u001b[0m micro_f1: 0.56745\n",
      "\u001b[32m[1112 12:43:39 @monitor.py:362]\u001b[0m one_error: 0.44029\n",
      "\u001b[32m[1112 12:43:39 @monitor.py:362]\u001b[0m ranking_loss: 0.14843\n",
      "\u001b[32m[1112 12:43:39 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.69847\n",
      "\u001b[32m[1112 12:43:39 @monitor.py:362]\u001b[0m training_ap: 0.52341\n",
      "\u001b[32m[1112 12:43:39 @monitor.py:362]\u001b[0m training_auc: 0.82301\n",
      "\u001b[32m[1112 12:43:39 @base.py:257]\u001b[0m Start Epoch 119 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:41 @base.py:267]\u001b[0m Epoch 119 (global_step 2737) finished, time:1.51 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,11.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:41 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-2737.\n",
      "\u001b[32m[1112 12:43:41 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.965\n",
      "\u001b[32m[1112 12:43:41 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.922\n",
      "\u001b[32m[1112 12:43:41 @monitor.py:362]\u001b[0m coverage: 2.18\n",
      "\u001b[32m[1112 12:43:41 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.69752\n",
      "\u001b[32m[1112 12:43:41 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:43:41 @monitor.py:362]\u001b[0m loss/value: 7.78\n",
      "\u001b[32m[1112 12:43:41 @monitor.py:362]\u001b[0m macro_auc: 0.85504\n",
      "\u001b[32m[1112 12:43:41 @monitor.py:362]\u001b[0m macro_f1: 0.52599\n",
      "\u001b[32m[1112 12:43:41 @monitor.py:362]\u001b[0m mean_average_precision: 0.56257\n",
      "\u001b[32m[1112 12:43:41 @monitor.py:362]\u001b[0m micro_auc: 0.85903\n",
      "\u001b[32m[1112 12:43:41 @monitor.py:362]\u001b[0m micro_f1: 0.54768\n",
      "\u001b[32m[1112 12:43:41 @monitor.py:362]\u001b[0m one_error: 0.43316\n",
      "\u001b[32m[1112 12:43:41 @monitor.py:362]\u001b[0m ranking_loss: 0.15185\n",
      "\u001b[32m[1112 12:43:41 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.69982\n",
      "\u001b[32m[1112 12:43:41 @monitor.py:362]\u001b[0m training_ap: 0.52432\n",
      "\u001b[32m[1112 12:43:41 @monitor.py:362]\u001b[0m training_auc: 0.82352\n",
      "\u001b[32m[1112 12:43:41 @base.py:257]\u001b[0m Start Epoch 120 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:43 @base.py:267]\u001b[0m Epoch 120 (global_step 2760) finished, time:1.52 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:43 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-2760.\n",
      "\u001b[32m[1112 12:43:43 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.974\n",
      "\u001b[32m[1112 12:43:43 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.793\n",
      "\u001b[32m[1112 12:43:43 @monitor.py:362]\u001b[0m coverage: 2.1266\n",
      "\u001b[32m[1112 12:43:43 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.27828\n",
      "\u001b[32m[1112 12:43:43 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:43:43 @monitor.py:362]\u001b[0m loss/value: 7.6688\n",
      "\u001b[32m[1112 12:43:43 @monitor.py:362]\u001b[0m macro_auc: 0.85129\n",
      "\u001b[32m[1112 12:43:43 @monitor.py:362]\u001b[0m macro_f1: 0.53956\n",
      "\u001b[32m[1112 12:43:43 @monitor.py:362]\u001b[0m mean_average_precision: 0.55953\n",
      "\u001b[32m[1112 12:43:43 @monitor.py:362]\u001b[0m micro_auc: 0.85742\n",
      "\u001b[32m[1112 12:43:43 @monitor.py:362]\u001b[0m micro_f1: 0.56519\n",
      "\u001b[32m[1112 12:43:43 @monitor.py:362]\u001b[0m one_error: 0.43316\n",
      "\u001b[32m[1112 12:43:43 @monitor.py:362]\u001b[0m ranking_loss: 0.14657\n",
      "\u001b[32m[1112 12:43:43 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70557\n",
      "\u001b[32m[1112 12:43:43 @monitor.py:362]\u001b[0m training_ap: 0.52526\n",
      "\u001b[32m[1112 12:43:43 @monitor.py:362]\u001b[0m training_auc: 0.82401\n",
      "\u001b[32m[1112 12:43:43 @base.py:257]\u001b[0m Start Epoch 121 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:45 @base.py:267]\u001b[0m Epoch 121 (global_step 2783) finished, time:1.61 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:45 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-2783.\n",
      "\u001b[32m[1112 12:43:45 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.981\n",
      "\u001b[32m[1112 12:43:45 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.752\n",
      "\u001b[32m[1112 12:43:45 @monitor.py:362]\u001b[0m coverage: 2.1212\n",
      "\u001b[32m[1112 12:43:45 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.99483\n",
      "\u001b[32m[1112 12:43:45 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:43:45 @monitor.py:362]\u001b[0m loss/value: 7.7663\n",
      "\u001b[32m[1112 12:43:45 @monitor.py:362]\u001b[0m macro_auc: 0.85474\n",
      "\u001b[32m[1112 12:43:45 @monitor.py:362]\u001b[0m macro_f1: 0.53347\n",
      "\u001b[32m[1112 12:43:45 @monitor.py:362]\u001b[0m mean_average_precision: 0.55241\n",
      "\u001b[32m[1112 12:43:45 @monitor.py:362]\u001b[0m micro_auc: 0.8608\n",
      "\u001b[32m[1112 12:43:45 @monitor.py:362]\u001b[0m micro_f1: 0.5599\n",
      "\u001b[32m[1112 12:43:45 @monitor.py:362]\u001b[0m one_error: 0.43316\n",
      "\u001b[32m[1112 12:43:45 @monitor.py:362]\u001b[0m ranking_loss: 0.14359\n",
      "\u001b[32m[1112 12:43:45 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70613\n",
      "\u001b[32m[1112 12:43:45 @monitor.py:362]\u001b[0m training_ap: 0.5261\n",
      "\u001b[32m[1112 12:43:45 @monitor.py:362]\u001b[0m training_auc: 0.82445\n",
      "\u001b[32m[1112 12:43:45 @base.py:257]\u001b[0m Start Epoch 122 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:47 @base.py:267]\u001b[0m Epoch 122 (global_step 2806) finished, time:1.53 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,11.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:47 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-2806.\n",
      "\u001b[32m[1112 12:43:47 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.986\n",
      "\u001b[32m[1112 12:43:47 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.777\n",
      "\u001b[32m[1112 12:43:47 @monitor.py:362]\u001b[0m coverage: 2.1676\n",
      "\u001b[32m[1112 12:43:47 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.58184\n",
      "\u001b[32m[1112 12:43:47 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:43:47 @monitor.py:362]\u001b[0m loss/value: 7.64\n",
      "\u001b[32m[1112 12:43:47 @monitor.py:362]\u001b[0m macro_auc: 0.85495\n",
      "\u001b[32m[1112 12:43:47 @monitor.py:362]\u001b[0m macro_f1: 0.53278\n",
      "\u001b[32m[1112 12:43:47 @monitor.py:362]\u001b[0m mean_average_precision: 0.56067\n",
      "\u001b[32m[1112 12:43:47 @monitor.py:362]\u001b[0m micro_auc: 0.86066\n",
      "\u001b[32m[1112 12:43:47 @monitor.py:362]\u001b[0m micro_f1: 0.55542\n",
      "\u001b[32m[1112 12:43:47 @monitor.py:362]\u001b[0m one_error: 0.42424\n",
      "\u001b[32m[1112 12:43:47 @monitor.py:362]\u001b[0m ranking_loss: 0.14943\n",
      "\u001b[32m[1112 12:43:47 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.7034\n",
      "\u001b[32m[1112 12:43:47 @monitor.py:362]\u001b[0m training_ap: 0.52712\n",
      "\u001b[32m[1112 12:43:47 @monitor.py:362]\u001b[0m training_auc: 0.82494\n",
      "\u001b[32m[1112 12:43:47 @base.py:257]\u001b[0m Start Epoch 123 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:49 @base.py:267]\u001b[0m Epoch 123 (global_step 2829) finished, time:1.63 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:50 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-2829.\n",
      "\u001b[32m[1112 12:43:50 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.99\n",
      "\u001b[32m[1112 12:43:50 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.863\n",
      "\u001b[32m[1112 12:43:50 @monitor.py:362]\u001b[0m coverage: 2.2371\n",
      "\u001b[32m[1112 12:43:50 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.74555\n",
      "\u001b[32m[1112 12:43:50 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:43:50 @monitor.py:362]\u001b[0m loss/value: 7.5989\n",
      "\u001b[32m[1112 12:43:50 @monitor.py:362]\u001b[0m macro_auc: 0.844\n",
      "\u001b[32m[1112 12:43:50 @monitor.py:362]\u001b[0m macro_f1: 0.52209\n",
      "\u001b[32m[1112 12:43:50 @monitor.py:362]\u001b[0m mean_average_precision: 0.55965\n",
      "\u001b[32m[1112 12:43:50 @monitor.py:362]\u001b[0m micro_auc: 0.85233\n",
      "\u001b[32m[1112 12:43:50 @monitor.py:362]\u001b[0m micro_f1: 0.54346\n",
      "\u001b[32m[1112 12:43:50 @monitor.py:362]\u001b[0m one_error: 0.44742\n",
      "\u001b[32m[1112 12:43:50 @monitor.py:362]\u001b[0m ranking_loss: 0.15717\n",
      "\u001b[32m[1112 12:43:50 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.69245\n",
      "\u001b[32m[1112 12:43:50 @monitor.py:362]\u001b[0m training_ap: 0.52812\n",
      "\u001b[32m[1112 12:43:50 @monitor.py:362]\u001b[0m training_auc: 0.82547\n",
      "\u001b[32m[1112 12:43:50 @base.py:257]\u001b[0m Start Epoch 124 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:51 @base.py:267]\u001b[0m Epoch 124 (global_step 2852) finished, time:1.45 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:52 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-2852.\n",
      "\u001b[32m[1112 12:43:52 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.992\n",
      "\u001b[32m[1112 12:43:52 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.94\n",
      "\u001b[32m[1112 12:43:52 @monitor.py:362]\u001b[0m coverage: 2.1141\n",
      "\u001b[32m[1112 12:43:52 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.91223\n",
      "\u001b[32m[1112 12:43:52 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:43:52 @monitor.py:362]\u001b[0m loss/value: 7.6564\n",
      "\u001b[32m[1112 12:43:52 @monitor.py:362]\u001b[0m macro_auc: 0.85197\n",
      "\u001b[32m[1112 12:43:52 @monitor.py:362]\u001b[0m macro_f1: 0.5436\n",
      "\u001b[32m[1112 12:43:52 @monitor.py:362]\u001b[0m mean_average_precision: 0.55915\n",
      "\u001b[32m[1112 12:43:52 @monitor.py:362]\u001b[0m micro_auc: 0.86121\n",
      "\u001b[32m[1112 12:43:52 @monitor.py:362]\u001b[0m micro_f1: 0.56726\n",
      "\u001b[32m[1112 12:43:52 @monitor.py:362]\u001b[0m one_error: 0.41889\n",
      "\u001b[32m[1112 12:43:52 @monitor.py:362]\u001b[0m ranking_loss: 0.14321\n",
      "\u001b[32m[1112 12:43:52 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.71369\n",
      "\u001b[32m[1112 12:43:52 @monitor.py:362]\u001b[0m training_ap: 0.529\n",
      "\u001b[32m[1112 12:43:52 @monitor.py:362]\u001b[0m training_auc: 0.82593\n",
      "\u001b[32m[1112 12:43:52 @base.py:257]\u001b[0m Start Epoch 125 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:53 @base.py:267]\u001b[0m Epoch 125 (global_step 2875) finished, time:1.56 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 8.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:54 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-2875.\n",
      "\u001b[32m[1112 12:43:54 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.994\n",
      "\u001b[32m[1112 12:43:54 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.869\n",
      "\u001b[32m[1112 12:43:54 @monitor.py:362]\u001b[0m coverage: 2.1426\n",
      "\u001b[32m[1112 12:43:54 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.39597\n",
      "\u001b[32m[1112 12:43:54 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:43:54 @monitor.py:362]\u001b[0m loss/value: 7.6965\n",
      "\u001b[32m[1112 12:43:54 @monitor.py:362]\u001b[0m macro_auc: 0.85794\n",
      "\u001b[32m[1112 12:43:54 @monitor.py:362]\u001b[0m macro_f1: 0.54629\n",
      "\u001b[32m[1112 12:43:54 @monitor.py:362]\u001b[0m mean_average_precision: 0.56864\n",
      "\u001b[32m[1112 12:43:54 @monitor.py:362]\u001b[0m micro_auc: 0.86193\n",
      "\u001b[32m[1112 12:43:54 @monitor.py:362]\u001b[0m micro_f1: 0.56529\n",
      "\u001b[32m[1112 12:43:54 @monitor.py:362]\u001b[0m one_error: 0.42068\n",
      "\u001b[32m[1112 12:43:54 @monitor.py:362]\u001b[0m ranking_loss: 0.14602\n",
      "\u001b[32m[1112 12:43:54 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70777\n",
      "\u001b[32m[1112 12:43:54 @monitor.py:362]\u001b[0m training_ap: 0.52991\n",
      "\u001b[32m[1112 12:43:54 @monitor.py:362]\u001b[0m training_auc: 0.8264\n",
      "\u001b[32m[1112 12:43:54 @base.py:257]\u001b[0m Start Epoch 126 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:55 @base.py:267]\u001b[0m Epoch 126 (global_step 2898) finished, time:1.60 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:56 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-2898.\n",
      "\u001b[32m[1112 12:43:56 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.912\n",
      "\u001b[32m[1112 12:43:56 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.85\n",
      "\u001b[32m[1112 12:43:56 @monitor.py:362]\u001b[0m coverage: 2.139\n",
      "\u001b[32m[1112 12:43:56 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.50229\n",
      "\u001b[32m[1112 12:43:56 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:43:56 @monitor.py:362]\u001b[0m loss/value: 7.4552\n",
      "\u001b[32m[1112 12:43:56 @monitor.py:362]\u001b[0m macro_auc: 0.85679\n",
      "\u001b[32m[1112 12:43:56 @monitor.py:362]\u001b[0m macro_f1: 0.53783\n",
      "\u001b[32m[1112 12:43:56 @monitor.py:362]\u001b[0m mean_average_precision: 0.55982\n",
      "\u001b[32m[1112 12:43:56 @monitor.py:362]\u001b[0m micro_auc: 0.86379\n",
      "\u001b[32m[1112 12:43:56 @monitor.py:362]\u001b[0m micro_f1: 0.56092\n",
      "\u001b[32m[1112 12:43:56 @monitor.py:362]\u001b[0m one_error: 0.42602\n",
      "\u001b[32m[1112 12:43:56 @monitor.py:362]\u001b[0m ranking_loss: 0.14582\n",
      "\u001b[32m[1112 12:43:56 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70821\n",
      "\u001b[32m[1112 12:43:56 @monitor.py:362]\u001b[0m training_ap: 0.53086\n",
      "\u001b[32m[1112 12:43:56 @monitor.py:362]\u001b[0m training_auc: 0.82691\n",
      "\u001b[32m[1112 12:43:56 @base.py:257]\u001b[0m Start Epoch 127 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:58 @base.py:267]\u001b[0m Epoch 127 (global_step 2921) finished, time:1.61 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 8.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:43:58 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-2921.\n",
      "\u001b[32m[1112 12:43:58 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.895\n",
      "\u001b[32m[1112 12:43:58 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.887\n",
      "\u001b[32m[1112 12:43:58 @monitor.py:362]\u001b[0m coverage: 2.1444\n",
      "\u001b[32m[1112 12:43:58 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.32504\n",
      "\u001b[32m[1112 12:43:58 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:43:58 @monitor.py:362]\u001b[0m loss/value: 7.4609\n",
      "\u001b[32m[1112 12:43:58 @monitor.py:362]\u001b[0m macro_auc: 0.84955\n",
      "\u001b[32m[1112 12:43:58 @monitor.py:362]\u001b[0m macro_f1: 0.53632\n",
      "\u001b[32m[1112 12:43:58 @monitor.py:362]\u001b[0m mean_average_precision: 0.56486\n",
      "\u001b[32m[1112 12:43:58 @monitor.py:362]\u001b[0m micro_auc: 0.86049\n",
      "\u001b[32m[1112 12:43:58 @monitor.py:362]\u001b[0m micro_f1: 0.5597\n",
      "\u001b[32m[1112 12:43:58 @monitor.py:362]\u001b[0m one_error: 0.4385\n",
      "\u001b[32m[1112 12:43:58 @monitor.py:362]\u001b[0m ranking_loss: 0.14614\n",
      "\u001b[32m[1112 12:43:58 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.703\n",
      "\u001b[32m[1112 12:43:58 @monitor.py:362]\u001b[0m training_ap: 0.53181\n",
      "\u001b[32m[1112 12:43:58 @monitor.py:362]\u001b[0m training_auc: 0.8274\n",
      "\u001b[32m[1112 12:43:58 @base.py:257]\u001b[0m Start Epoch 128 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:00 @base.py:267]\u001b[0m Epoch 128 (global_step 2944) finished, time:1.58 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:00 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-2944.\n",
      "\u001b[32m[1112 12:44:00 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.923\n",
      "\u001b[32m[1112 12:44:00 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.947\n",
      "\u001b[32m[1112 12:44:00 @monitor.py:362]\u001b[0m coverage: 2.1765\n",
      "\u001b[32m[1112 12:44:00 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.8982\n",
      "\u001b[32m[1112 12:44:00 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:44:00 @monitor.py:362]\u001b[0m loss/value: 7.6011\n",
      "\u001b[32m[1112 12:44:00 @monitor.py:362]\u001b[0m macro_auc: 0.84709\n",
      "\u001b[32m[1112 12:44:00 @monitor.py:362]\u001b[0m macro_f1: 0.54\n",
      "\u001b[32m[1112 12:44:00 @monitor.py:362]\u001b[0m mean_average_precision: 0.54996\n",
      "\u001b[32m[1112 12:44:00 @monitor.py:362]\u001b[0m micro_auc: 0.8561\n",
      "\u001b[32m[1112 12:44:00 @monitor.py:362]\u001b[0m micro_f1: 0.56182\n",
      "\u001b[32m[1112 12:44:00 @monitor.py:362]\u001b[0m one_error: 0.4385\n",
      "\u001b[32m[1112 12:44:00 @monitor.py:362]\u001b[0m ranking_loss: 0.15154\n",
      "\u001b[32m[1112 12:44:00 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70032\n",
      "\u001b[32m[1112 12:44:00 @monitor.py:362]\u001b[0m training_ap: 0.5327\n",
      "\u001b[32m[1112 12:44:00 @monitor.py:362]\u001b[0m training_auc: 0.82785\n",
      "\u001b[32m[1112 12:44:00 @base.py:257]\u001b[0m Start Epoch 129 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:02 @base.py:267]\u001b[0m Epoch 129 (global_step 2967) finished, time:1.51 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:02 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-2967.\n",
      "\u001b[32m[1112 12:44:02 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.943\n",
      "\u001b[32m[1112 12:44:02 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.941\n",
      "\u001b[32m[1112 12:44:02 @monitor.py:362]\u001b[0m coverage: 2.1248\n",
      "\u001b[32m[1112 12:44:02 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.72171\n",
      "\u001b[32m[1112 12:44:02 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:44:02 @monitor.py:362]\u001b[0m loss/value: 7.6563\n",
      "\u001b[32m[1112 12:44:02 @monitor.py:362]\u001b[0m macro_auc: 0.84994\n",
      "\u001b[32m[1112 12:44:02 @monitor.py:362]\u001b[0m macro_f1: 0.54204\n",
      "\u001b[32m[1112 12:44:02 @monitor.py:362]\u001b[0m mean_average_precision: 0.56669\n",
      "\u001b[32m[1112 12:44:02 @monitor.py:362]\u001b[0m micro_auc: 0.86026\n",
      "\u001b[32m[1112 12:44:02 @monitor.py:362]\u001b[0m micro_f1: 0.56358\n",
      "\u001b[32m[1112 12:44:02 @monitor.py:362]\u001b[0m one_error: 0.42424\n",
      "\u001b[32m[1112 12:44:02 @monitor.py:362]\u001b[0m ranking_loss: 0.14467\n",
      "\u001b[32m[1112 12:44:02 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70869\n",
      "\u001b[32m[1112 12:44:02 @monitor.py:362]\u001b[0m training_ap: 0.53358\n",
      "\u001b[32m[1112 12:44:02 @monitor.py:362]\u001b[0m training_auc: 0.8283\n",
      "\u001b[32m[1112 12:44:02 @base.py:257]\u001b[0m Start Epoch 130 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:04 @base.py:267]\u001b[0m Epoch 130 (global_step 2990) finished, time:1.61 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,11.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:05 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-2990.\n",
      "\u001b[32m[1112 12:44:05 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.958\n",
      "\u001b[32m[1112 12:44:05 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.881\n",
      "\u001b[32m[1112 12:44:05 @monitor.py:362]\u001b[0m coverage: 2.1711\n",
      "\u001b[32m[1112 12:44:05 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.91497\n",
      "\u001b[32m[1112 12:44:05 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:44:05 @monitor.py:362]\u001b[0m loss/value: 7.5916\n",
      "\u001b[32m[1112 12:44:05 @monitor.py:362]\u001b[0m macro_auc: 0.84593\n",
      "\u001b[32m[1112 12:44:05 @monitor.py:362]\u001b[0m macro_f1: 0.53222\n",
      "\u001b[32m[1112 12:44:05 @monitor.py:362]\u001b[0m mean_average_precision: 0.54683\n",
      "\u001b[32m[1112 12:44:05 @monitor.py:362]\u001b[0m micro_auc: 0.85712\n",
      "\u001b[32m[1112 12:44:05 @monitor.py:362]\u001b[0m micro_f1: 0.55829\n",
      "\u001b[32m[1112 12:44:05 @monitor.py:362]\u001b[0m one_error: 0.4385\n",
      "\u001b[32m[1112 12:44:05 @monitor.py:362]\u001b[0m ranking_loss: 0.15099\n",
      "\u001b[32m[1112 12:44:05 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70287\n",
      "\u001b[32m[1112 12:44:05 @monitor.py:362]\u001b[0m training_ap: 0.53444\n",
      "\u001b[32m[1112 12:44:05 @monitor.py:362]\u001b[0m training_auc: 0.82876\n",
      "\u001b[32m[1112 12:44:05 @base.py:257]\u001b[0m Start Epoch 131 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:06 @base.py:267]\u001b[0m Epoch 131 (global_step 3013) finished, time:1.60 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:07 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-3013.\n",
      "\u001b[32m[1112 12:44:07 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.929\n",
      "\u001b[32m[1112 12:44:07 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.908\n",
      "\u001b[32m[1112 12:44:07 @monitor.py:362]\u001b[0m coverage: 2.18\n",
      "\u001b[32m[1112 12:44:07 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.76413\n",
      "\u001b[32m[1112 12:44:07 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:44:07 @monitor.py:362]\u001b[0m loss/value: 7.5759\n",
      "\u001b[32m[1112 12:44:07 @monitor.py:362]\u001b[0m macro_auc: 0.84214\n",
      "\u001b[32m[1112 12:44:07 @monitor.py:362]\u001b[0m macro_f1: 0.52612\n",
      "\u001b[32m[1112 12:44:07 @monitor.py:362]\u001b[0m mean_average_precision: 0.56202\n",
      "\u001b[32m[1112 12:44:07 @monitor.py:362]\u001b[0m micro_auc: 0.85478\n",
      "\u001b[32m[1112 12:44:07 @monitor.py:362]\u001b[0m micro_f1: 0.55068\n",
      "\u001b[32m[1112 12:44:07 @monitor.py:362]\u001b[0m one_error: 0.42424\n",
      "\u001b[32m[1112 12:44:07 @monitor.py:362]\u001b[0m ranking_loss: 0.14974\n",
      "\u001b[32m[1112 12:44:07 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70574\n",
      "\u001b[32m[1112 12:44:07 @monitor.py:362]\u001b[0m training_ap: 0.5353\n",
      "\u001b[32m[1112 12:44:07 @monitor.py:362]\u001b[0m training_auc: 0.82922\n",
      "\u001b[32m[1112 12:44:07 @base.py:257]\u001b[0m Start Epoch 132 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:08 @base.py:267]\u001b[0m Epoch 132 (global_step 3036) finished, time:1.52 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:09 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-3036.\n",
      "\u001b[32m[1112 12:44:09 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.947\n",
      "\u001b[32m[1112 12:44:09 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.943\n",
      "\u001b[32m[1112 12:44:09 @monitor.py:362]\u001b[0m coverage: 2.1765\n",
      "\u001b[32m[1112 12:44:09 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.63087\n",
      "\u001b[32m[1112 12:44:09 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:44:09 @monitor.py:362]\u001b[0m loss/value: 7.5501\n",
      "\u001b[32m[1112 12:44:09 @monitor.py:362]\u001b[0m macro_auc: 0.84967\n",
      "\u001b[32m[1112 12:44:09 @monitor.py:362]\u001b[0m macro_f1: 0.5319\n",
      "\u001b[32m[1112 12:44:09 @monitor.py:362]\u001b[0m mean_average_precision: 0.54855\n",
      "\u001b[32m[1112 12:44:09 @monitor.py:362]\u001b[0m micro_auc: 0.85799\n",
      "\u001b[32m[1112 12:44:09 @monitor.py:362]\u001b[0m micro_f1: 0.55767\n",
      "\u001b[32m[1112 12:44:09 @monitor.py:362]\u001b[0m one_error: 0.42424\n",
      "\u001b[32m[1112 12:44:09 @monitor.py:362]\u001b[0m ranking_loss: 0.14933\n",
      "\u001b[32m[1112 12:44:09 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70596\n",
      "\u001b[32m[1112 12:44:09 @monitor.py:362]\u001b[0m training_ap: 0.53612\n",
      "\u001b[32m[1112 12:44:09 @monitor.py:362]\u001b[0m training_auc: 0.82969\n",
      "\u001b[32m[1112 12:44:09 @base.py:257]\u001b[0m Start Epoch 133 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:10 @base.py:267]\u001b[0m Epoch 133 (global_step 3059) finished, time:1.54 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:11 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-3059.\n",
      "\u001b[32m[1112 12:44:11 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.921\n",
      "\u001b[32m[1112 12:44:11 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.916\n",
      "\u001b[32m[1112 12:44:11 @monitor.py:362]\u001b[0m coverage: 2.1301\n",
      "\u001b[32m[1112 12:44:11 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.28212\n",
      "\u001b[32m[1112 12:44:11 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:44:11 @monitor.py:362]\u001b[0m loss/value: 7.4913\n",
      "\u001b[32m[1112 12:44:11 @monitor.py:362]\u001b[0m macro_auc: 0.85024\n",
      "\u001b[32m[1112 12:44:11 @monitor.py:362]\u001b[0m macro_f1: 0.53768\n",
      "\u001b[32m[1112 12:44:11 @monitor.py:362]\u001b[0m mean_average_precision: 0.55627\n",
      "\u001b[32m[1112 12:44:11 @monitor.py:362]\u001b[0m micro_auc: 0.86223\n",
      "\u001b[32m[1112 12:44:11 @monitor.py:362]\u001b[0m micro_f1: 0.56264\n",
      "\u001b[32m[1112 12:44:11 @monitor.py:362]\u001b[0m one_error: 0.42424\n",
      "\u001b[32m[1112 12:44:11 @monitor.py:362]\u001b[0m ranking_loss: 0.14589\n",
      "\u001b[32m[1112 12:44:11 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70962\n",
      "\u001b[32m[1112 12:44:11 @monitor.py:362]\u001b[0m training_ap: 0.53696\n",
      "\u001b[32m[1112 12:44:11 @monitor.py:362]\u001b[0m training_auc: 0.83013\n",
      "\u001b[32m[1112 12:44:11 @base.py:257]\u001b[0m Start Epoch 134 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:12 @base.py:267]\u001b[0m Epoch 134 (global_step 3082) finished, time:1.58 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,11.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:13 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-3082.\n",
      "\u001b[32m[1112 12:44:13 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.901\n",
      "\u001b[32m[1112 12:44:13 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.845\n",
      "\u001b[32m[1112 12:44:13 @monitor.py:362]\u001b[0m coverage: 2.1462\n",
      "\u001b[32m[1112 12:44:13 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 1.4233\n",
      "\u001b[32m[1112 12:44:13 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:44:13 @monitor.py:362]\u001b[0m loss/value: 7.574\n",
      "\u001b[32m[1112 12:44:13 @monitor.py:362]\u001b[0m macro_auc: 0.84126\n",
      "\u001b[32m[1112 12:44:13 @monitor.py:362]\u001b[0m macro_f1: 0.52586\n",
      "\u001b[32m[1112 12:44:13 @monitor.py:362]\u001b[0m mean_average_precision: 0.54016\n",
      "\u001b[32m[1112 12:44:13 @monitor.py:362]\u001b[0m micro_auc: 0.85552\n",
      "\u001b[32m[1112 12:44:13 @monitor.py:362]\u001b[0m micro_f1: 0.554\n",
      "\u001b[32m[1112 12:44:13 @monitor.py:362]\u001b[0m one_error: 0.43137\n",
      "\u001b[32m[1112 12:44:13 @monitor.py:362]\u001b[0m ranking_loss: 0.14813\n",
      "\u001b[32m[1112 12:44:13 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70343\n",
      "\u001b[32m[1112 12:44:13 @monitor.py:362]\u001b[0m training_ap: 0.53779\n",
      "\u001b[32m[1112 12:44:13 @monitor.py:362]\u001b[0m training_auc: 0.83056\n",
      "\u001b[32m[1112 12:44:13 @base.py:257]\u001b[0m Start Epoch 135 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:14 @base.py:267]\u001b[0m Epoch 135 (global_step 3105) finished, time:1.54 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:15 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-3105.\n",
      "\u001b[32m[1112 12:44:15 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.886\n",
      "\u001b[32m[1112 12:44:15 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.912\n",
      "\u001b[32m[1112 12:44:15 @monitor.py:362]\u001b[0m coverage: 2.1854\n",
      "\u001b[32m[1112 12:44:15 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.35682\n",
      "\u001b[32m[1112 12:44:15 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:44:15 @monitor.py:362]\u001b[0m loss/value: 7.5532\n",
      "\u001b[32m[1112 12:44:15 @monitor.py:362]\u001b[0m macro_auc: 0.84347\n",
      "\u001b[32m[1112 12:44:15 @monitor.py:362]\u001b[0m macro_f1: 0.52996\n",
      "\u001b[32m[1112 12:44:15 @monitor.py:362]\u001b[0m mean_average_precision: 0.54812\n",
      "\u001b[32m[1112 12:44:15 @monitor.py:362]\u001b[0m micro_auc: 0.8578\n",
      "\u001b[32m[1112 12:44:15 @monitor.py:362]\u001b[0m micro_f1: 0.55354\n",
      "\u001b[32m[1112 12:44:15 @monitor.py:362]\u001b[0m one_error: 0.44207\n",
      "\u001b[32m[1112 12:44:15 @monitor.py:362]\u001b[0m ranking_loss: 0.15038\n",
      "\u001b[32m[1112 12:44:15 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.69577\n",
      "\u001b[32m[1112 12:44:15 @monitor.py:362]\u001b[0m training_ap: 0.53866\n",
      "\u001b[32m[1112 12:44:15 @monitor.py:362]\u001b[0m training_auc: 0.83102\n",
      "\u001b[32m[1112 12:44:15 @base.py:257]\u001b[0m Start Epoch 136 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:17 @base.py:267]\u001b[0m Epoch 136 (global_step 3128) finished, time:1.58 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:17 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-3128.\n",
      "\u001b[32m[1112 12:44:17 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.876\n",
      "\u001b[32m[1112 12:44:17 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.921\n",
      "\u001b[32m[1112 12:44:17 @monitor.py:362]\u001b[0m coverage: 2.0481\n",
      "\u001b[32m[1112 12:44:17 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.94028\n",
      "\u001b[32m[1112 12:44:17 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:44:17 @monitor.py:362]\u001b[0m loss/value: 7.5125\n",
      "\u001b[32m[1112 12:44:17 @monitor.py:362]\u001b[0m macro_auc: 0.85834\n",
      "\u001b[32m[1112 12:44:17 @monitor.py:362]\u001b[0m macro_f1: 0.56073\n",
      "\u001b[32m[1112 12:44:17 @monitor.py:362]\u001b[0m mean_average_precision: 0.57547\n",
      "\u001b[32m[1112 12:44:17 @monitor.py:362]\u001b[0m micro_auc: 0.86716\n",
      "\u001b[32m[1112 12:44:17 @monitor.py:362]\u001b[0m micro_f1: 0.57695\n",
      "\u001b[32m[1112 12:44:17 @monitor.py:362]\u001b[0m one_error: 0.40642\n",
      "\u001b[32m[1112 12:44:17 @monitor.py:362]\u001b[0m ranking_loss: 0.13737\n",
      "\u001b[32m[1112 12:44:17 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.71998\n",
      "\u001b[32m[1112 12:44:17 @monitor.py:362]\u001b[0m training_ap: 0.53942\n",
      "\u001b[32m[1112 12:44:17 @monitor.py:362]\u001b[0m training_auc: 0.83142\n",
      "\u001b[32m[1112 12:44:17 @base.py:257]\u001b[0m Start Epoch 137 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:19 @base.py:267]\u001b[0m Epoch 137 (global_step 3151) finished, time:1.51 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:19 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-3151.\n",
      "\u001b[32m[1112 12:44:19 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.909\n",
      "\u001b[32m[1112 12:44:19 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.899\n",
      "\u001b[32m[1112 12:44:19 @monitor.py:362]\u001b[0m coverage: 2.0873\n",
      "\u001b[32m[1112 12:44:19 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.42346\n",
      "\u001b[32m[1112 12:44:19 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:44:19 @monitor.py:362]\u001b[0m loss/value: 7.5158\n",
      "\u001b[32m[1112 12:44:19 @monitor.py:362]\u001b[0m macro_auc: 0.85915\n",
      "\u001b[32m[1112 12:44:19 @monitor.py:362]\u001b[0m macro_f1: 0.5516\n",
      "\u001b[32m[1112 12:44:19 @monitor.py:362]\u001b[0m mean_average_precision: 0.5834\n",
      "\u001b[32m[1112 12:44:19 @monitor.py:362]\u001b[0m micro_auc: 0.86744\n",
      "\u001b[32m[1112 12:44:19 @monitor.py:362]\u001b[0m micro_f1: 0.57134\n",
      "\u001b[32m[1112 12:44:19 @monitor.py:362]\u001b[0m one_error: 0.41889\n",
      "\u001b[32m[1112 12:44:19 @monitor.py:362]\u001b[0m ranking_loss: 0.13995\n",
      "\u001b[32m[1112 12:44:19 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.71257\n",
      "\u001b[32m[1112 12:44:19 @monitor.py:362]\u001b[0m training_ap: 0.54025\n",
      "\u001b[32m[1112 12:44:19 @monitor.py:362]\u001b[0m training_auc: 0.83183\n",
      "\u001b[32m[1112 12:44:19 @base.py:257]\u001b[0m Start Epoch 138 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:21 @base.py:267]\u001b[0m Epoch 138 (global_step 3174) finished, time:1.60 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:21 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-3174.\n",
      "\u001b[32m[1112 12:44:21 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.933\n",
      "\u001b[32m[1112 12:44:21 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.858\n",
      "\u001b[32m[1112 12:44:21 @monitor.py:362]\u001b[0m coverage: 2.0481\n",
      "\u001b[32m[1112 12:44:21 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.34542\n",
      "\u001b[32m[1112 12:44:21 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:44:21 @monitor.py:362]\u001b[0m loss/value: 7.5019\n",
      "\u001b[32m[1112 12:44:21 @monitor.py:362]\u001b[0m macro_auc: 0.85026\n",
      "\u001b[32m[1112 12:44:21 @monitor.py:362]\u001b[0m macro_f1: 0.5518\n",
      "\u001b[32m[1112 12:44:21 @monitor.py:362]\u001b[0m mean_average_precision: 0.56991\n",
      "\u001b[32m[1112 12:44:21 @monitor.py:362]\u001b[0m micro_auc: 0.86316\n",
      "\u001b[32m[1112 12:44:21 @monitor.py:362]\u001b[0m micro_f1: 0.57093\n",
      "\u001b[32m[1112 12:44:21 @monitor.py:362]\u001b[0m one_error: 0.40998\n",
      "\u001b[32m[1112 12:44:21 @monitor.py:362]\u001b[0m ranking_loss: 0.13638\n",
      "\u001b[32m[1112 12:44:21 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.71832\n",
      "\u001b[32m[1112 12:44:21 @monitor.py:362]\u001b[0m training_ap: 0.54103\n",
      "\u001b[32m[1112 12:44:21 @monitor.py:362]\u001b[0m training_auc: 0.83225\n",
      "\u001b[32m[1112 12:44:21 @base.py:257]\u001b[0m Start Epoch 139 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:23 @base.py:267]\u001b[0m Epoch 139 (global_step 3197) finished, time:1.55 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:23 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-3197.\n",
      "\u001b[32m[1112 12:44:23 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.951\n",
      "\u001b[32m[1112 12:44:23 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.885\n",
      "\u001b[32m[1112 12:44:23 @monitor.py:362]\u001b[0m coverage: 2.1604\n",
      "\u001b[32m[1112 12:44:23 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.47848\n",
      "\u001b[32m[1112 12:44:23 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:44:23 @monitor.py:362]\u001b[0m loss/value: 7.4348\n",
      "\u001b[32m[1112 12:44:23 @monitor.py:362]\u001b[0m macro_auc: 0.85462\n",
      "\u001b[32m[1112 12:44:23 @monitor.py:362]\u001b[0m macro_f1: 0.54472\n",
      "\u001b[32m[1112 12:44:23 @monitor.py:362]\u001b[0m mean_average_precision: 0.54991\n",
      "\u001b[32m[1112 12:44:23 @monitor.py:362]\u001b[0m micro_auc: 0.86295\n",
      "\u001b[32m[1112 12:44:23 @monitor.py:362]\u001b[0m micro_f1: 0.56991\n",
      "\u001b[32m[1112 12:44:24 @monitor.py:362]\u001b[0m one_error: 0.41889\n",
      "\u001b[32m[1112 12:44:24 @monitor.py:362]\u001b[0m ranking_loss: 0.14753\n",
      "\u001b[32m[1112 12:44:24 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70814\n",
      "\u001b[32m[1112 12:44:24 @monitor.py:362]\u001b[0m training_ap: 0.54188\n",
      "\u001b[32m[1112 12:44:24 @monitor.py:362]\u001b[0m training_auc: 0.83268\n",
      "\u001b[32m[1112 12:44:24 @base.py:257]\u001b[0m Start Epoch 140 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:25 @base.py:267]\u001b[0m Epoch 140 (global_step 3220) finished, time:1.57 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,11.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:26 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-3220.\n",
      "\u001b[32m[1112 12:44:26 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.964\n",
      "\u001b[32m[1112 12:44:26 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.827\n",
      "\u001b[32m[1112 12:44:26 @monitor.py:362]\u001b[0m coverage: 2.1533\n",
      "\u001b[32m[1112 12:44:26 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.50774\n",
      "\u001b[32m[1112 12:44:26 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:44:26 @monitor.py:362]\u001b[0m loss/value: 7.4394\n",
      "\u001b[32m[1112 12:44:26 @monitor.py:362]\u001b[0m macro_auc: 0.85747\n",
      "\u001b[32m[1112 12:44:26 @monitor.py:362]\u001b[0m macro_f1: 0.54849\n",
      "\u001b[32m[1112 12:44:26 @monitor.py:362]\u001b[0m mean_average_precision: 0.56269\n",
      "\u001b[32m[1112 12:44:26 @monitor.py:362]\u001b[0m micro_auc: 0.86679\n",
      "\u001b[32m[1112 12:44:26 @monitor.py:362]\u001b[0m micro_f1: 0.56919\n",
      "\u001b[32m[1112 12:44:26 @monitor.py:362]\u001b[0m one_error: 0.42424\n",
      "\u001b[32m[1112 12:44:26 @monitor.py:362]\u001b[0m ranking_loss: 0.14569\n",
      "\u001b[32m[1112 12:44:26 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70982\n",
      "\u001b[32m[1112 12:44:26 @monitor.py:362]\u001b[0m training_ap: 0.54271\n",
      "\u001b[32m[1112 12:44:26 @monitor.py:362]\u001b[0m training_auc: 0.83308\n",
      "\u001b[32m[1112 12:44:26 @base.py:257]\u001b[0m Start Epoch 141 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:27 @base.py:267]\u001b[0m Epoch 141 (global_step 3243) finished, time:1.52 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:28 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-3243.\n",
      "\u001b[32m[1112 12:44:28 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.973\n",
      "\u001b[32m[1112 12:44:28 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.699\n",
      "\u001b[32m[1112 12:44:28 @monitor.py:362]\u001b[0m coverage: 2.1765\n",
      "\u001b[32m[1112 12:44:28 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 1.5428\n",
      "\u001b[32m[1112 12:44:28 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:44:28 @monitor.py:362]\u001b[0m loss/value: 7.5163\n",
      "\u001b[32m[1112 12:44:28 @monitor.py:362]\u001b[0m macro_auc: 0.85237\n",
      "\u001b[32m[1112 12:44:28 @monitor.py:362]\u001b[0m macro_f1: 0.53103\n",
      "\u001b[32m[1112 12:44:28 @monitor.py:362]\u001b[0m mean_average_precision: 0.55096\n",
      "\u001b[32m[1112 12:44:28 @monitor.py:362]\u001b[0m micro_auc: 0.85947\n",
      "\u001b[32m[1112 12:44:28 @monitor.py:362]\u001b[0m micro_f1: 0.55673\n",
      "\u001b[32m[1112 12:44:28 @monitor.py:362]\u001b[0m one_error: 0.42602\n",
      "\u001b[32m[1112 12:44:28 @monitor.py:362]\u001b[0m ranking_loss: 0.14909\n",
      "\u001b[32m[1112 12:44:28 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70665\n",
      "\u001b[32m[1112 12:44:28 @monitor.py:362]\u001b[0m training_ap: 0.54347\n",
      "\u001b[32m[1112 12:44:28 @monitor.py:362]\u001b[0m training_auc: 0.83348\n",
      "\u001b[32m[1112 12:44:28 @base.py:257]\u001b[0m Start Epoch 142 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:29 @base.py:267]\u001b[0m Epoch 142 (global_step 3266) finished, time:1.54 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:30 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-3266.\n",
      "\u001b[32m[1112 12:44:30 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.94\n",
      "\u001b[32m[1112 12:44:30 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.844\n",
      "\u001b[32m[1112 12:44:30 @monitor.py:362]\u001b[0m coverage: 2.148\n",
      "\u001b[32m[1112 12:44:30 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.30211\n",
      "\u001b[32m[1112 12:44:30 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:44:30 @monitor.py:362]\u001b[0m loss/value: 7.4482\n",
      "\u001b[32m[1112 12:44:30 @monitor.py:362]\u001b[0m macro_auc: 0.8474\n",
      "\u001b[32m[1112 12:44:30 @monitor.py:362]\u001b[0m macro_f1: 0.53172\n",
      "\u001b[32m[1112 12:44:30 @monitor.py:362]\u001b[0m mean_average_precision: 0.54953\n",
      "\u001b[32m[1112 12:44:30 @monitor.py:362]\u001b[0m micro_auc: 0.85572\n",
      "\u001b[32m[1112 12:44:30 @monitor.py:362]\u001b[0m micro_f1: 0.55499\n",
      "\u001b[32m[1112 12:44:30 @monitor.py:362]\u001b[0m one_error: 0.43137\n",
      "\u001b[32m[1112 12:44:30 @monitor.py:362]\u001b[0m ranking_loss: 0.14899\n",
      "\u001b[32m[1112 12:44:30 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70337\n",
      "\u001b[32m[1112 12:44:30 @monitor.py:362]\u001b[0m training_ap: 0.54428\n",
      "\u001b[32m[1112 12:44:30 @monitor.py:362]\u001b[0m training_auc: 0.83387\n",
      "\u001b[32m[1112 12:44:30 @base.py:257]\u001b[0m Start Epoch 143 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:31 @base.py:267]\u001b[0m Epoch 143 (global_step 3289) finished, time:1.62 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:32 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-3289.\n",
      "\u001b[32m[1112 12:44:32 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.956\n",
      "\u001b[32m[1112 12:44:32 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.907\n",
      "\u001b[32m[1112 12:44:32 @monitor.py:362]\u001b[0m coverage: 2.1444\n",
      "\u001b[32m[1112 12:44:32 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.70318\n",
      "\u001b[32m[1112 12:44:32 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:44:32 @monitor.py:362]\u001b[0m loss/value: 7.4598\n",
      "\u001b[32m[1112 12:44:32 @monitor.py:362]\u001b[0m macro_auc: 0.84487\n",
      "\u001b[32m[1112 12:44:32 @monitor.py:362]\u001b[0m macro_f1: 0.53361\n",
      "\u001b[32m[1112 12:44:32 @monitor.py:362]\u001b[0m mean_average_precision: 0.55116\n",
      "\u001b[32m[1112 12:44:32 @monitor.py:362]\u001b[0m micro_auc: 0.85744\n",
      "\u001b[32m[1112 12:44:32 @monitor.py:362]\u001b[0m micro_f1: 0.55627\n",
      "\u001b[32m[1112 12:44:32 @monitor.py:362]\u001b[0m one_error: 0.41889\n",
      "\u001b[32m[1112 12:44:32 @monitor.py:362]\u001b[0m ranking_loss: 0.15029\n",
      "\u001b[32m[1112 12:44:32 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70759\n",
      "\u001b[32m[1112 12:44:32 @monitor.py:362]\u001b[0m training_ap: 0.54506\n",
      "\u001b[32m[1112 12:44:32 @monitor.py:362]\u001b[0m training_auc: 0.83426\n",
      "\u001b[32m[1112 12:44:32 @base.py:257]\u001b[0m Start Epoch 144 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:33 @base.py:267]\u001b[0m Epoch 144 (global_step 3312) finished, time:1.53 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:34 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-3312.\n",
      "\u001b[32m[1112 12:44:34 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.967\n",
      "\u001b[32m[1112 12:44:34 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.91\n",
      "\u001b[32m[1112 12:44:34 @monitor.py:362]\u001b[0m coverage: 2.1693\n",
      "\u001b[32m[1112 12:44:34 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.80147\n",
      "\u001b[32m[1112 12:44:34 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:44:34 @monitor.py:362]\u001b[0m loss/value: 7.534\n",
      "\u001b[32m[1112 12:44:34 @monitor.py:362]\u001b[0m macro_auc: 0.85206\n",
      "\u001b[32m[1112 12:44:34 @monitor.py:362]\u001b[0m macro_f1: 0.53421\n",
      "\u001b[32m[1112 12:44:34 @monitor.py:362]\u001b[0m mean_average_precision: 0.56821\n",
      "\u001b[32m[1112 12:44:34 @monitor.py:362]\u001b[0m micro_auc: 0.85995\n",
      "\u001b[32m[1112 12:44:34 @monitor.py:362]\u001b[0m micro_f1: 0.559\n",
      "\u001b[32m[1112 12:44:34 @monitor.py:362]\u001b[0m one_error: 0.44563\n",
      "\u001b[32m[1112 12:44:34 @monitor.py:362]\u001b[0m ranking_loss: 0.15118\n",
      "\u001b[32m[1112 12:44:34 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.69863\n",
      "\u001b[32m[1112 12:44:34 @monitor.py:362]\u001b[0m training_ap: 0.5458\n",
      "\u001b[32m[1112 12:44:34 @monitor.py:362]\u001b[0m training_auc: 0.83464\n",
      "\u001b[32m[1112 12:44:34 @base.py:257]\u001b[0m Start Epoch 145 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:36 @base.py:267]\u001b[0m Epoch 145 (global_step 3335) finished, time:1.57 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,11.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:36 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-3335.\n",
      "\u001b[32m[1112 12:44:36 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.935\n",
      "\u001b[32m[1112 12:44:36 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.918\n",
      "\u001b[32m[1112 12:44:36 @monitor.py:362]\u001b[0m coverage: 2.0713\n",
      "\u001b[32m[1112 12:44:36 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.63062\n",
      "\u001b[32m[1112 12:44:36 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:44:36 @monitor.py:362]\u001b[0m loss/value: 7.4334\n",
      "\u001b[32m[1112 12:44:36 @monitor.py:362]\u001b[0m macro_auc: 0.86389\n",
      "\u001b[32m[1112 12:44:36 @monitor.py:362]\u001b[0m macro_f1: 0.54556\n",
      "\u001b[32m[1112 12:44:36 @monitor.py:362]\u001b[0m mean_average_precision: 0.57278\n",
      "\u001b[32m[1112 12:44:36 @monitor.py:362]\u001b[0m micro_auc: 0.86959\n",
      "\u001b[32m[1112 12:44:36 @monitor.py:362]\u001b[0m micro_f1: 0.56549\n",
      "\u001b[32m[1112 12:44:36 @monitor.py:362]\u001b[0m one_error: 0.4082\n",
      "\u001b[32m[1112 12:44:36 @monitor.py:362]\u001b[0m ranking_loss: 0.13929\n",
      "\u001b[32m[1112 12:44:36 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.71869\n",
      "\u001b[32m[1112 12:44:36 @monitor.py:362]\u001b[0m training_ap: 0.54664\n",
      "\u001b[32m[1112 12:44:36 @monitor.py:362]\u001b[0m training_auc: 0.83506\n",
      "\u001b[32m[1112 12:44:36 @base.py:257]\u001b[0m Start Epoch 146 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:38 @base.py:267]\u001b[0m Epoch 146 (global_step 3358) finished, time:1.51 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,11.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:38 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-3358.\n",
      "\u001b[32m[1112 12:44:38 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.952\n",
      "\u001b[32m[1112 12:44:38 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.935\n",
      "\u001b[32m[1112 12:44:38 @monitor.py:362]\u001b[0m coverage: 2.2068\n",
      "\u001b[32m[1112 12:44:38 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.75874\n",
      "\u001b[32m[1112 12:44:38 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:44:38 @monitor.py:362]\u001b[0m loss/value: 7.3965\n",
      "\u001b[32m[1112 12:44:38 @monitor.py:362]\u001b[0m macro_auc: 0.8369\n",
      "\u001b[32m[1112 12:44:38 @monitor.py:362]\u001b[0m macro_f1: 0.5202\n",
      "\u001b[32m[1112 12:44:38 @monitor.py:362]\u001b[0m mean_average_precision: 0.53029\n",
      "\u001b[32m[1112 12:44:38 @monitor.py:362]\u001b[0m micro_auc: 0.85065\n",
      "\u001b[32m[1112 12:44:38 @monitor.py:362]\u001b[0m micro_f1: 0.54675\n",
      "\u001b[32m[1112 12:44:38 @monitor.py:362]\u001b[0m one_error: 0.42781\n",
      "\u001b[32m[1112 12:44:38 @monitor.py:362]\u001b[0m ranking_loss: 0.15319\n",
      "\u001b[32m[1112 12:44:38 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70263\n",
      "\u001b[32m[1112 12:44:38 @monitor.py:362]\u001b[0m training_ap: 0.54743\n",
      "\u001b[32m[1112 12:44:38 @monitor.py:362]\u001b[0m training_auc: 0.83544\n",
      "\u001b[32m[1112 12:44:38 @base.py:257]\u001b[0m Start Epoch 147 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,13.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:40 @base.py:267]\u001b[0m Epoch 147 (global_step 3381) finished, time:1.65 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:40 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-3381.\n",
      "\u001b[32m[1112 12:44:40 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.965\n",
      "\u001b[32m[1112 12:44:40 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.889\n",
      "\u001b[32m[1112 12:44:40 @monitor.py:362]\u001b[0m coverage: 2.1408\n",
      "\u001b[32m[1112 12:44:40 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 1.2829\n",
      "\u001b[32m[1112 12:44:40 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:44:40 @monitor.py:362]\u001b[0m loss/value: 7.4089\n",
      "\u001b[32m[1112 12:44:40 @monitor.py:362]\u001b[0m macro_auc: 0.85447\n",
      "\u001b[32m[1112 12:44:40 @monitor.py:362]\u001b[0m macro_f1: 0.54378\n",
      "\u001b[32m[1112 12:44:40 @monitor.py:362]\u001b[0m mean_average_precision: 0.55663\n",
      "\u001b[32m[1112 12:44:40 @monitor.py:362]\u001b[0m micro_auc: 0.8627\n",
      "\u001b[32m[1112 12:44:40 @monitor.py:362]\u001b[0m micro_f1: 0.56257\n",
      "\u001b[32m[1112 12:44:40 @monitor.py:362]\u001b[0m one_error: 0.42424\n",
      "\u001b[32m[1112 12:44:40 @monitor.py:362]\u001b[0m ranking_loss: 0.14475\n",
      "\u001b[32m[1112 12:44:40 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70898\n",
      "\u001b[32m[1112 12:44:40 @monitor.py:362]\u001b[0m training_ap: 0.54825\n",
      "\u001b[32m[1112 12:44:40 @monitor.py:362]\u001b[0m training_auc: 0.83585\n",
      "\u001b[32m[1112 12:44:40 @base.py:257]\u001b[0m Start Epoch 148 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:42 @base.py:267]\u001b[0m Epoch 148 (global_step 3404) finished, time:1.53 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:42 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-3404.\n",
      "\u001b[32m[1112 12:44:42 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.974\n",
      "\u001b[32m[1112 12:44:42 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.906\n",
      "\u001b[32m[1112 12:44:42 @monitor.py:362]\u001b[0m coverage: 2.2478\n",
      "\u001b[32m[1112 12:44:42 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.81928\n",
      "\u001b[32m[1112 12:44:42 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:44:42 @monitor.py:362]\u001b[0m loss/value: 7.3857\n",
      "\u001b[32m[1112 12:44:42 @monitor.py:362]\u001b[0m macro_auc: 0.84051\n",
      "\u001b[32m[1112 12:44:42 @monitor.py:362]\u001b[0m macro_f1: 0.51728\n",
      "\u001b[32m[1112 12:44:42 @monitor.py:362]\u001b[0m mean_average_precision: 0.55084\n",
      "\u001b[32m[1112 12:44:42 @monitor.py:362]\u001b[0m micro_auc: 0.85028\n",
      "\u001b[32m[1112 12:44:42 @monitor.py:362]\u001b[0m micro_f1: 0.54205\n",
      "\u001b[32m[1112 12:44:42 @monitor.py:362]\u001b[0m one_error: 0.45098\n",
      "\u001b[32m[1112 12:44:42 @monitor.py:362]\u001b[0m ranking_loss: 0.1566\n",
      "\u001b[32m[1112 12:44:42 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.69052\n",
      "\u001b[32m[1112 12:44:42 @monitor.py:362]\u001b[0m training_ap: 0.54899\n",
      "\u001b[32m[1112 12:44:42 @monitor.py:362]\u001b[0m training_auc: 0.83624\n",
      "\u001b[32m[1112 12:44:42 @base.py:257]\u001b[0m Start Epoch 149 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:44 @base.py:267]\u001b[0m Epoch 149 (global_step 3427) finished, time:1.48 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 8.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:44 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-3427.\n",
      "\u001b[32m[1112 12:44:44 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.981\n",
      "\u001b[32m[1112 12:44:44 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.91\n",
      "\u001b[32m[1112 12:44:44 @monitor.py:362]\u001b[0m coverage: 2.1123\n",
      "\u001b[32m[1112 12:44:44 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.69926\n",
      "\u001b[32m[1112 12:44:44 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:44:44 @monitor.py:362]\u001b[0m loss/value: 7.4097\n",
      "\u001b[32m[1112 12:44:44 @monitor.py:362]\u001b[0m macro_auc: 0.8528\n",
      "\u001b[32m[1112 12:44:44 @monitor.py:362]\u001b[0m macro_f1: 0.53721\n",
      "\u001b[32m[1112 12:44:44 @monitor.py:362]\u001b[0m mean_average_precision: 0.55234\n",
      "\u001b[32m[1112 12:44:44 @monitor.py:362]\u001b[0m micro_auc: 0.86216\n",
      "\u001b[32m[1112 12:44:44 @monitor.py:362]\u001b[0m micro_f1: 0.56415\n",
      "\u001b[32m[1112 12:44:44 @monitor.py:362]\u001b[0m one_error: 0.39394\n",
      "\u001b[32m[1112 12:44:44 @monitor.py:362]\u001b[0m ranking_loss: 0.14211\n",
      "\u001b[32m[1112 12:44:44 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.71898\n",
      "\u001b[32m[1112 12:44:44 @monitor.py:362]\u001b[0m training_ap: 0.54977\n",
      "\u001b[32m[1112 12:44:44 @monitor.py:362]\u001b[0m training_auc: 0.83662\n",
      "\u001b[32m[1112 12:44:44 @base.py:257]\u001b[0m Start Epoch 150 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:46 @base.py:267]\u001b[0m Epoch 150 (global_step 3450) finished, time:1.50 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 8.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:47 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-3450.\n",
      "\u001b[32m[1112 12:44:47 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.986\n",
      "\u001b[32m[1112 12:44:47 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.851\n",
      "\u001b[32m[1112 12:44:47 @monitor.py:362]\u001b[0m coverage: 2.1052\n",
      "\u001b[32m[1112 12:44:47 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.98721\n",
      "\u001b[32m[1112 12:44:47 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:44:47 @monitor.py:362]\u001b[0m loss/value: 7.436\n",
      "\u001b[32m[1112 12:44:47 @monitor.py:362]\u001b[0m macro_auc: 0.85245\n",
      "\u001b[32m[1112 12:44:47 @monitor.py:362]\u001b[0m macro_f1: 0.53933\n",
      "\u001b[32m[1112 12:44:47 @monitor.py:362]\u001b[0m mean_average_precision: 0.55605\n",
      "\u001b[32m[1112 12:44:47 @monitor.py:362]\u001b[0m micro_auc: 0.86155\n",
      "\u001b[32m[1112 12:44:47 @monitor.py:362]\u001b[0m micro_f1: 0.56334\n",
      "\u001b[32m[1112 12:44:47 @monitor.py:362]\u001b[0m one_error: 0.4082\n",
      "\u001b[32m[1112 12:44:47 @monitor.py:362]\u001b[0m ranking_loss: 0.14118\n",
      "\u001b[32m[1112 12:44:47 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.71684\n",
      "\u001b[32m[1112 12:44:47 @monitor.py:362]\u001b[0m training_ap: 0.55048\n",
      "\u001b[32m[1112 12:44:47 @monitor.py:362]\u001b[0m training_auc: 0.837\n",
      "\u001b[32m[1112 12:44:47 @base.py:257]\u001b[0m Start Epoch 151 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:48 @base.py:267]\u001b[0m Epoch 151 (global_step 3473) finished, time:1.55 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:49 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-3473.\n",
      "\u001b[32m[1112 12:44:49 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.947\n",
      "\u001b[32m[1112 12:44:49 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.889\n",
      "\u001b[32m[1112 12:44:49 @monitor.py:362]\u001b[0m coverage: 2.1159\n",
      "\u001b[32m[1112 12:44:49 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.81929\n",
      "\u001b[32m[1112 12:44:49 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:44:49 @monitor.py:362]\u001b[0m loss/value: 7.4367\n",
      "\u001b[32m[1112 12:44:49 @monitor.py:362]\u001b[0m macro_auc: 0.85344\n",
      "\u001b[32m[1112 12:44:49 @monitor.py:362]\u001b[0m macro_f1: 0.53999\n",
      "\u001b[32m[1112 12:44:49 @monitor.py:362]\u001b[0m mean_average_precision: 0.56489\n",
      "\u001b[32m[1112 12:44:49 @monitor.py:362]\u001b[0m micro_auc: 0.86189\n",
      "\u001b[32m[1112 12:44:49 @monitor.py:362]\u001b[0m micro_f1: 0.56056\n",
      "\u001b[32m[1112 12:44:49 @monitor.py:362]\u001b[0m one_error: 0.41355\n",
      "\u001b[32m[1112 12:44:49 @monitor.py:362]\u001b[0m ranking_loss: 0.14491\n",
      "\u001b[32m[1112 12:44:49 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.71211\n",
      "\u001b[32m[1112 12:44:49 @monitor.py:362]\u001b[0m training_ap: 0.55115\n",
      "\u001b[32m[1112 12:44:49 @monitor.py:362]\u001b[0m training_auc: 0.83735\n",
      "\u001b[32m[1112 12:44:49 @base.py:257]\u001b[0m Start Epoch 152 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:50 @base.py:267]\u001b[0m Epoch 152 (global_step 3496) finished, time:1.57 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 8.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:51 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-3496.\n",
      "\u001b[32m[1112 12:44:51 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.92\n",
      "\u001b[32m[1112 12:44:51 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.874\n",
      "\u001b[32m[1112 12:44:51 @monitor.py:362]\u001b[0m coverage: 2.123\n",
      "\u001b[32m[1112 12:44:51 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.5107\n",
      "\u001b[32m[1112 12:44:51 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:44:51 @monitor.py:362]\u001b[0m loss/value: 7.307\n",
      "\u001b[32m[1112 12:44:51 @monitor.py:362]\u001b[0m macro_auc: 0.84507\n",
      "\u001b[32m[1112 12:44:51 @monitor.py:362]\u001b[0m macro_f1: 0.53117\n",
      "\u001b[32m[1112 12:44:51 @monitor.py:362]\u001b[0m mean_average_precision: 0.54843\n",
      "\u001b[32m[1112 12:44:51 @monitor.py:362]\u001b[0m micro_auc: 0.85773\n",
      "\u001b[32m[1112 12:44:51 @monitor.py:362]\u001b[0m micro_f1: 0.55638\n",
      "\u001b[32m[1112 12:44:51 @monitor.py:362]\u001b[0m one_error: 0.43672\n",
      "\u001b[32m[1112 12:44:51 @monitor.py:362]\u001b[0m ranking_loss: 0.14454\n",
      "\u001b[32m[1112 12:44:51 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70886\n",
      "\u001b[32m[1112 12:44:51 @monitor.py:362]\u001b[0m training_ap: 0.55194\n",
      "\u001b[32m[1112 12:44:51 @monitor.py:362]\u001b[0m training_auc: 0.83773\n",
      "\u001b[32m[1112 12:44:51 @base.py:257]\u001b[0m Start Epoch 153 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:53 @base.py:267]\u001b[0m Epoch 153 (global_step 3519) finished, time:1.60 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 8.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:53 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-3519.\n",
      "\u001b[32m[1112 12:44:53 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.941\n",
      "\u001b[32m[1112 12:44:53 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.961\n",
      "\u001b[32m[1112 12:44:53 @monitor.py:362]\u001b[0m coverage: 2.0998\n",
      "\u001b[32m[1112 12:44:53 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.93757\n",
      "\u001b[32m[1112 12:44:53 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:44:53 @monitor.py:362]\u001b[0m loss/value: 7.4038\n",
      "\u001b[32m[1112 12:44:53 @monitor.py:362]\u001b[0m macro_auc: 0.84928\n",
      "\u001b[32m[1112 12:44:53 @monitor.py:362]\u001b[0m macro_f1: 0.54703\n",
      "\u001b[32m[1112 12:44:53 @monitor.py:362]\u001b[0m mean_average_precision: 0.56947\n",
      "\u001b[32m[1112 12:44:53 @monitor.py:362]\u001b[0m micro_auc: 0.8599\n",
      "\u001b[32m[1112 12:44:53 @monitor.py:362]\u001b[0m micro_f1: 0.57103\n",
      "\u001b[32m[1112 12:44:53 @monitor.py:362]\u001b[0m one_error: 0.40463\n",
      "\u001b[32m[1112 12:44:53 @monitor.py:362]\u001b[0m ranking_loss: 0.14165\n",
      "\u001b[32m[1112 12:44:53 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.71819\n",
      "\u001b[32m[1112 12:44:53 @monitor.py:362]\u001b[0m training_ap: 0.55269\n",
      "\u001b[32m[1112 12:44:53 @monitor.py:362]\u001b[0m training_auc: 0.8381\n",
      "\u001b[32m[1112 12:44:53 @base.py:257]\u001b[0m Start Epoch 154 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:55 @base.py:267]\u001b[0m Epoch 154 (global_step 3542) finished, time:1.53 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:55 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-3542.\n",
      "\u001b[32m[1112 12:44:55 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.957\n",
      "\u001b[32m[1112 12:44:55 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.988\n",
      "\u001b[32m[1112 12:44:55 @monitor.py:362]\u001b[0m coverage: 2.1979\n",
      "\u001b[32m[1112 12:44:55 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 2.606\n",
      "\u001b[32m[1112 12:44:55 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:44:55 @monitor.py:362]\u001b[0m loss/value: 7.4223\n",
      "\u001b[32m[1112 12:44:55 @monitor.py:362]\u001b[0m macro_auc: 0.85024\n",
      "\u001b[32m[1112 12:44:55 @monitor.py:362]\u001b[0m macro_f1: 0.54018\n",
      "\u001b[32m[1112 12:44:55 @monitor.py:362]\u001b[0m mean_average_precision: 0.5598\n",
      "\u001b[32m[1112 12:44:55 @monitor.py:362]\u001b[0m micro_auc: 0.86058\n",
      "\u001b[32m[1112 12:44:55 @monitor.py:362]\u001b[0m micro_f1: 0.55983\n",
      "\u001b[32m[1112 12:44:55 @monitor.py:362]\u001b[0m one_error: 0.4385\n",
      "\u001b[32m[1112 12:44:55 @monitor.py:362]\u001b[0m ranking_loss: 0.15169\n",
      "\u001b[32m[1112 12:44:55 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.69948\n",
      "\u001b[32m[1112 12:44:55 @monitor.py:362]\u001b[0m training_ap: 0.55337\n",
      "\u001b[32m[1112 12:44:55 @monitor.py:362]\u001b[0m training_auc: 0.83845\n",
      "\u001b[32m[1112 12:44:55 @base.py:257]\u001b[0m Start Epoch 155 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:57 @base.py:267]\u001b[0m Epoch 155 (global_step 3565) finished, time:1.45 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:57 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-3565.\n",
      "\u001b[32m[1112 12:44:57 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.968\n",
      "\u001b[32m[1112 12:44:57 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.929\n",
      "\u001b[32m[1112 12:44:57 @monitor.py:362]\u001b[0m coverage: 2.1283\n",
      "\u001b[32m[1112 12:44:57 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 1.3172\n",
      "\u001b[32m[1112 12:44:57 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:44:57 @monitor.py:362]\u001b[0m loss/value: 7.3184\n",
      "\u001b[32m[1112 12:44:57 @monitor.py:362]\u001b[0m macro_auc: 0.86377\n",
      "\u001b[32m[1112 12:44:57 @monitor.py:362]\u001b[0m macro_f1: 0.55107\n",
      "\u001b[32m[1112 12:44:57 @monitor.py:362]\u001b[0m mean_average_precision: 0.56597\n",
      "\u001b[32m[1112 12:44:57 @monitor.py:362]\u001b[0m micro_auc: 0.86782\n",
      "\u001b[32m[1112 12:44:57 @monitor.py:362]\u001b[0m micro_f1: 0.57256\n",
      "\u001b[32m[1112 12:44:57 @monitor.py:362]\u001b[0m one_error: 0.41711\n",
      "\u001b[32m[1112 12:44:57 @monitor.py:362]\u001b[0m ranking_loss: 0.14538\n",
      "\u001b[32m[1112 12:44:57 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.71217\n",
      "\u001b[32m[1112 12:44:57 @monitor.py:362]\u001b[0m training_ap: 0.55413\n",
      "\u001b[32m[1112 12:44:57 @monitor.py:362]\u001b[0m training_auc: 0.83882\n",
      "\u001b[32m[1112 12:44:57 @base.py:257]\u001b[0m Start Epoch 156 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:59 @base.py:267]\u001b[0m Epoch 156 (global_step 3588) finished, time:1.52 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,12.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:44:59 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-3588.\n",
      "\u001b[32m[1112 12:44:59 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.977\n",
      "\u001b[32m[1112 12:44:59 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.925\n",
      "\u001b[32m[1112 12:44:59 @monitor.py:362]\u001b[0m coverage: 2.1373\n",
      "\u001b[32m[1112 12:44:59 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.526\n",
      "\u001b[32m[1112 12:44:59 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:44:59 @monitor.py:362]\u001b[0m loss/value: 7.311\n",
      "\u001b[32m[1112 12:44:59 @monitor.py:362]\u001b[0m macro_auc: 0.84962\n",
      "\u001b[32m[1112 12:44:59 @monitor.py:362]\u001b[0m macro_f1: 0.54252\n",
      "\u001b[32m[1112 12:44:59 @monitor.py:362]\u001b[0m mean_average_precision: 0.56193\n",
      "\u001b[32m[1112 12:44:59 @monitor.py:362]\u001b[0m micro_auc: 0.86162\n",
      "\u001b[32m[1112 12:44:59 @monitor.py:362]\u001b[0m micro_f1: 0.5641\n",
      "\u001b[32m[1112 12:44:59 @monitor.py:362]\u001b[0m one_error: 0.41711\n",
      "\u001b[32m[1112 12:44:59 @monitor.py:362]\u001b[0m ranking_loss: 0.14626\n",
      "\u001b[32m[1112 12:44:59 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.71036\n",
      "\u001b[32m[1112 12:44:59 @monitor.py:362]\u001b[0m training_ap: 0.55482\n",
      "\u001b[32m[1112 12:44:59 @monitor.py:362]\u001b[0m training_auc: 0.83919\n",
      "\u001b[32m[1112 12:44:59 @base.py:257]\u001b[0m Start Epoch 157 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:01 @base.py:267]\u001b[0m Epoch 157 (global_step 3611) finished, time:1.63 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,11.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:01 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-3611.\n",
      "\u001b[32m[1112 12:45:01 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.983\n",
      "\u001b[32m[1112 12:45:01 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.977\n",
      "\u001b[32m[1112 12:45:01 @monitor.py:362]\u001b[0m coverage: 2.0624\n",
      "\u001b[32m[1112 12:45:01 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 1.1285\n",
      "\u001b[32m[1112 12:45:01 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:45:01 @monitor.py:362]\u001b[0m loss/value: 7.4006\n",
      "\u001b[32m[1112 12:45:01 @monitor.py:362]\u001b[0m macro_auc: 0.8601\n",
      "\u001b[32m[1112 12:45:01 @monitor.py:362]\u001b[0m macro_f1: 0.54346\n",
      "\u001b[32m[1112 12:45:01 @monitor.py:362]\u001b[0m mean_average_precision: 0.56935\n",
      "\u001b[32m[1112 12:45:01 @monitor.py:362]\u001b[0m micro_auc: 0.86632\n",
      "\u001b[32m[1112 12:45:01 @monitor.py:362]\u001b[0m micro_f1: 0.56459\n",
      "\u001b[32m[1112 12:45:01 @monitor.py:362]\u001b[0m one_error: 0.3975\n",
      "\u001b[32m[1112 12:45:01 @monitor.py:362]\u001b[0m ranking_loss: 0.13821\n",
      "\u001b[32m[1112 12:45:01 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.72148\n",
      "\u001b[32m[1112 12:45:02 @monitor.py:362]\u001b[0m training_ap: 0.55547\n",
      "\u001b[32m[1112 12:45:02 @monitor.py:362]\u001b[0m training_auc: 0.83954\n",
      "\u001b[32m[1112 12:45:02 @base.py:257]\u001b[0m Start Epoch 158 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:03 @base.py:267]\u001b[0m Epoch 158 (global_step 3634) finished, time:1.52 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:04 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-3634.\n",
      "\u001b[32m[1112 12:45:04 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.987\n",
      "\u001b[32m[1112 12:45:04 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.914\n",
      "\u001b[32m[1112 12:45:04 @monitor.py:362]\u001b[0m coverage: 2.0891\n",
      "\u001b[32m[1112 12:45:04 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.39837\n",
      "\u001b[32m[1112 12:45:04 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:45:04 @monitor.py:362]\u001b[0m loss/value: 7.2865\n",
      "\u001b[32m[1112 12:45:04 @monitor.py:362]\u001b[0m macro_auc: 0.85442\n",
      "\u001b[32m[1112 12:45:04 @monitor.py:362]\u001b[0m macro_f1: 0.54871\n",
      "\u001b[32m[1112 12:45:04 @monitor.py:362]\u001b[0m mean_average_precision: 0.56334\n",
      "\u001b[32m[1112 12:45:04 @monitor.py:362]\u001b[0m micro_auc: 0.86349\n",
      "\u001b[32m[1112 12:45:04 @monitor.py:362]\u001b[0m micro_f1: 0.56742\n",
      "\u001b[32m[1112 12:45:04 @monitor.py:362]\u001b[0m one_error: 0.41533\n",
      "\u001b[32m[1112 12:45:04 @monitor.py:362]\u001b[0m ranking_loss: 0.14247\n",
      "\u001b[32m[1112 12:45:04 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.71289\n",
      "\u001b[32m[1112 12:45:04 @monitor.py:362]\u001b[0m training_ap: 0.55616\n",
      "\u001b[32m[1112 12:45:04 @monitor.py:362]\u001b[0m training_auc: 0.83991\n",
      "\u001b[32m[1112 12:45:04 @base.py:257]\u001b[0m Start Epoch 159 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:05 @base.py:267]\u001b[0m Epoch 159 (global_step 3657) finished, time:1.62 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:06 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-3657.\n",
      "\u001b[32m[1112 12:45:06 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.991\n",
      "\u001b[32m[1112 12:45:06 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.894\n",
      "\u001b[32m[1112 12:45:06 @monitor.py:362]\u001b[0m coverage: 2.1355\n",
      "\u001b[32m[1112 12:45:06 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.624\n",
      "\u001b[32m[1112 12:45:06 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:45:06 @monitor.py:362]\u001b[0m loss/value: 7.2996\n",
      "\u001b[32m[1112 12:45:06 @monitor.py:362]\u001b[0m macro_auc: 0.85927\n",
      "\u001b[32m[1112 12:45:06 @monitor.py:362]\u001b[0m macro_f1: 0.54949\n",
      "\u001b[32m[1112 12:45:06 @monitor.py:362]\u001b[0m mean_average_precision: 0.5748\n",
      "\u001b[32m[1112 12:45:06 @monitor.py:362]\u001b[0m micro_auc: 0.86685\n",
      "\u001b[32m[1112 12:45:06 @monitor.py:362]\u001b[0m micro_f1: 0.57009\n",
      "\u001b[32m[1112 12:45:06 @monitor.py:362]\u001b[0m one_error: 0.41355\n",
      "\u001b[32m[1112 12:45:06 @monitor.py:362]\u001b[0m ranking_loss: 0.14531\n",
      "\u001b[32m[1112 12:45:06 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.71384\n",
      "\u001b[32m[1112 12:45:06 @monitor.py:362]\u001b[0m training_ap: 0.55682\n",
      "\u001b[32m[1112 12:45:06 @monitor.py:362]\u001b[0m training_auc: 0.84024\n",
      "\u001b[32m[1112 12:45:06 @base.py:257]\u001b[0m Start Epoch 160 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:07 @base.py:267]\u001b[0m Epoch 160 (global_step 3680) finished, time:1.49 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:08 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-3680.\n",
      "\u001b[32m[1112 12:45:08 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.993\n",
      "\u001b[32m[1112 12:45:08 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.753\n",
      "\u001b[32m[1112 12:45:08 @monitor.py:362]\u001b[0m coverage: 2.041\n",
      "\u001b[32m[1112 12:45:08 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.836\n",
      "\u001b[32m[1112 12:45:08 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:45:08 @monitor.py:362]\u001b[0m loss/value: 7.361\n",
      "\u001b[32m[1112 12:45:08 @monitor.py:362]\u001b[0m macro_auc: 0.85668\n",
      "\u001b[32m[1112 12:45:08 @monitor.py:362]\u001b[0m macro_f1: 0.54256\n",
      "\u001b[32m[1112 12:45:08 @monitor.py:362]\u001b[0m mean_average_precision: 0.55737\n",
      "\u001b[32m[1112 12:45:08 @monitor.py:362]\u001b[0m micro_auc: 0.86638\n",
      "\u001b[32m[1112 12:45:08 @monitor.py:362]\u001b[0m micro_f1: 0.56218\n",
      "\u001b[32m[1112 12:45:08 @monitor.py:362]\u001b[0m one_error: 0.40642\n",
      "\u001b[32m[1112 12:45:08 @monitor.py:362]\u001b[0m ranking_loss: 0.13591\n",
      "\u001b[32m[1112 12:45:08 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.72018\n",
      "\u001b[32m[1112 12:45:08 @monitor.py:362]\u001b[0m training_ap: 0.5575\n",
      "\u001b[32m[1112 12:45:08 @monitor.py:362]\u001b[0m training_auc: 0.84056\n",
      "\u001b[32m[1112 12:45:08 @base.py:257]\u001b[0m Start Epoch 161 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:09 @base.py:267]\u001b[0m Epoch 161 (global_step 3703) finished, time:1.50 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:10 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-3703.\n",
      "\u001b[32m[1112 12:45:10 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.995\n",
      "\u001b[32m[1112 12:45:10 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.834\n",
      "\u001b[32m[1112 12:45:10 @monitor.py:362]\u001b[0m coverage: 2.1408\n",
      "\u001b[32m[1112 12:45:10 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.94651\n",
      "\u001b[32m[1112 12:45:10 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:45:10 @monitor.py:362]\u001b[0m loss/value: 7.3517\n",
      "\u001b[32m[1112 12:45:10 @monitor.py:362]\u001b[0m macro_auc: 0.84641\n",
      "\u001b[32m[1112 12:45:10 @monitor.py:362]\u001b[0m macro_f1: 0.5395\n",
      "\u001b[32m[1112 12:45:10 @monitor.py:362]\u001b[0m mean_average_precision: 0.5506\n",
      "\u001b[32m[1112 12:45:10 @monitor.py:362]\u001b[0m micro_auc: 0.85835\n",
      "\u001b[32m[1112 12:45:10 @monitor.py:362]\u001b[0m micro_f1: 0.56411\n",
      "\u001b[32m[1112 12:45:10 @monitor.py:362]\u001b[0m one_error: 0.42068\n",
      "\u001b[32m[1112 12:45:10 @monitor.py:362]\u001b[0m ranking_loss: 0.14854\n",
      "\u001b[32m[1112 12:45:10 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70561\n",
      "\u001b[32m[1112 12:45:10 @monitor.py:362]\u001b[0m training_ap: 0.55819\n",
      "\u001b[32m[1112 12:45:10 @monitor.py:362]\u001b[0m training_auc: 0.84091\n",
      "\u001b[32m[1112 12:45:10 @base.py:257]\u001b[0m Start Epoch 162 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:12 @base.py:267]\u001b[0m Epoch 162 (global_step 3726) finished, time:1.62 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:12 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-3726.\n",
      "\u001b[32m[1112 12:45:12 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.956\n",
      "\u001b[32m[1112 12:45:12 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.912\n",
      "\u001b[32m[1112 12:45:12 @monitor.py:362]\u001b[0m coverage: 2.0963\n",
      "\u001b[32m[1112 12:45:12 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.85455\n",
      "\u001b[32m[1112 12:45:12 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:45:12 @monitor.py:362]\u001b[0m loss/value: 7.416\n",
      "\u001b[32m[1112 12:45:12 @monitor.py:362]\u001b[0m macro_auc: 0.83685\n",
      "\u001b[32m[1112 12:45:12 @monitor.py:362]\u001b[0m macro_f1: 0.52826\n",
      "\u001b[32m[1112 12:45:12 @monitor.py:362]\u001b[0m mean_average_precision: 0.53724\n",
      "\u001b[32m[1112 12:45:12 @monitor.py:362]\u001b[0m micro_auc: 0.85342\n",
      "\u001b[32m[1112 12:45:12 @monitor.py:362]\u001b[0m micro_f1: 0.55279\n",
      "\u001b[32m[1112 12:45:12 @monitor.py:362]\u001b[0m one_error: 0.43316\n",
      "\u001b[32m[1112 12:45:12 @monitor.py:362]\u001b[0m ranking_loss: 0.14335\n",
      "\u001b[32m[1112 12:45:12 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70784\n",
      "\u001b[32m[1112 12:45:12 @monitor.py:362]\u001b[0m training_ap: 0.55881\n",
      "\u001b[32m[1112 12:45:12 @monitor.py:362]\u001b[0m training_auc: 0.8412\n",
      "\u001b[32m[1112 12:45:12 @base.py:257]\u001b[0m Start Epoch 163 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:14 @base.py:267]\u001b[0m Epoch 163 (global_step 3749) finished, time:1.61 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,11.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:14 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-3749.\n",
      "\u001b[32m[1112 12:45:14 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.967\n",
      "\u001b[32m[1112 12:45:14 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.934\n",
      "\u001b[32m[1112 12:45:14 @monitor.py:362]\u001b[0m coverage: 2.1515\n",
      "\u001b[32m[1112 12:45:14 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.8705\n",
      "\u001b[32m[1112 12:45:14 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:45:14 @monitor.py:362]\u001b[0m loss/value: 7.2435\n",
      "\u001b[32m[1112 12:45:14 @monitor.py:362]\u001b[0m macro_auc: 0.85225\n",
      "\u001b[32m[1112 12:45:14 @monitor.py:362]\u001b[0m macro_f1: 0.53571\n",
      "\u001b[32m[1112 12:45:14 @monitor.py:362]\u001b[0m mean_average_precision: 0.56593\n",
      "\u001b[32m[1112 12:45:14 @monitor.py:362]\u001b[0m micro_auc: 0.86296\n",
      "\u001b[32m[1112 12:45:14 @monitor.py:362]\u001b[0m micro_f1: 0.56016\n",
      "\u001b[32m[1112 12:45:14 @monitor.py:362]\u001b[0m one_error: 0.42246\n",
      "\u001b[32m[1112 12:45:14 @monitor.py:362]\u001b[0m ranking_loss: 0.14713\n",
      "\u001b[32m[1112 12:45:14 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70972\n",
      "\u001b[32m[1112 12:45:14 @monitor.py:362]\u001b[0m training_ap: 0.55952\n",
      "\u001b[32m[1112 12:45:14 @monitor.py:362]\u001b[0m training_auc: 0.84154\n",
      "\u001b[32m[1112 12:45:14 @base.py:257]\u001b[0m Start Epoch 164 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:16 @base.py:267]\u001b[0m Epoch 164 (global_step 3772) finished, time:1.62 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:17 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-3772.\n",
      "\u001b[32m[1112 12:45:17 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.976\n",
      "\u001b[32m[1112 12:45:17 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.87\n",
      "\u001b[32m[1112 12:45:17 @monitor.py:362]\u001b[0m coverage: 2.0945\n",
      "\u001b[32m[1112 12:45:17 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 1.6595\n",
      "\u001b[32m[1112 12:45:17 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:45:17 @monitor.py:362]\u001b[0m loss/value: 7.4221\n",
      "\u001b[32m[1112 12:45:17 @monitor.py:362]\u001b[0m macro_auc: 0.85042\n",
      "\u001b[32m[1112 12:45:17 @monitor.py:362]\u001b[0m macro_f1: 0.53789\n",
      "\u001b[32m[1112 12:45:17 @monitor.py:362]\u001b[0m mean_average_precision: 0.55369\n",
      "\u001b[32m[1112 12:45:17 @monitor.py:362]\u001b[0m micro_auc: 0.86137\n",
      "\u001b[32m[1112 12:45:17 @monitor.py:362]\u001b[0m micro_f1: 0.55865\n",
      "\u001b[32m[1112 12:45:17 @monitor.py:362]\u001b[0m one_error: 0.40642\n",
      "\u001b[32m[1112 12:45:17 @monitor.py:362]\u001b[0m ranking_loss: 0.14382\n",
      "\u001b[32m[1112 12:45:17 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.71519\n",
      "\u001b[32m[1112 12:45:17 @monitor.py:362]\u001b[0m training_ap: 0.56017\n",
      "\u001b[32m[1112 12:45:17 @monitor.py:362]\u001b[0m training_auc: 0.84187\n",
      "\u001b[32m[1112 12:45:17 @base.py:257]\u001b[0m Start Epoch 165 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:18 @base.py:267]\u001b[0m Epoch 165 (global_step 3795) finished, time:1.50 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:19 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-3795.\n",
      "\u001b[32m[1112 12:45:19 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.942\n",
      "\u001b[32m[1112 12:45:19 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.773\n",
      "\u001b[32m[1112 12:45:19 @monitor.py:362]\u001b[0m coverage: 2.0339\n",
      "\u001b[32m[1112 12:45:19 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.48164\n",
      "\u001b[32m[1112 12:45:19 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:45:19 @monitor.py:362]\u001b[0m loss/value: 7.3082\n",
      "\u001b[32m[1112 12:45:19 @monitor.py:362]\u001b[0m macro_auc: 0.85988\n",
      "\u001b[32m[1112 12:45:19 @monitor.py:362]\u001b[0m macro_f1: 0.55023\n",
      "\u001b[32m[1112 12:45:19 @monitor.py:362]\u001b[0m mean_average_precision: 0.56733\n",
      "\u001b[32m[1112 12:45:19 @monitor.py:362]\u001b[0m micro_auc: 0.86719\n",
      "\u001b[32m[1112 12:45:19 @monitor.py:362]\u001b[0m micro_f1: 0.56874\n",
      "\u001b[32m[1112 12:45:19 @monitor.py:362]\u001b[0m one_error: 0.40642\n",
      "\u001b[32m[1112 12:45:19 @monitor.py:362]\u001b[0m ranking_loss: 0.13343\n",
      "\u001b[32m[1112 12:45:19 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.72266\n",
      "\u001b[32m[1112 12:45:19 @monitor.py:362]\u001b[0m training_ap: 0.56086\n",
      "\u001b[32m[1112 12:45:19 @monitor.py:362]\u001b[0m training_auc: 0.84222\n",
      "\u001b[32m[1112 12:45:19 @base.py:257]\u001b[0m Start Epoch 166 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:20 @base.py:267]\u001b[0m Epoch 166 (global_step 3818) finished, time:1.52 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:21 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-3818.\n",
      "\u001b[32m[1112 12:45:21 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.957\n",
      "\u001b[32m[1112 12:45:21 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.754\n",
      "\u001b[32m[1112 12:45:21 @monitor.py:362]\u001b[0m coverage: 2.1016\n",
      "\u001b[32m[1112 12:45:21 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.61922\n",
      "\u001b[32m[1112 12:45:21 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:45:21 @monitor.py:362]\u001b[0m loss/value: 7.3023\n",
      "\u001b[32m[1112 12:45:21 @monitor.py:362]\u001b[0m macro_auc: 0.85302\n",
      "\u001b[32m[1112 12:45:21 @monitor.py:362]\u001b[0m macro_f1: 0.54372\n",
      "\u001b[32m[1112 12:45:21 @monitor.py:362]\u001b[0m mean_average_precision: 0.55552\n",
      "\u001b[32m[1112 12:45:21 @monitor.py:362]\u001b[0m micro_auc: 0.86212\n",
      "\u001b[32m[1112 12:45:21 @monitor.py:362]\u001b[0m micro_f1: 0.56139\n",
      "\u001b[32m[1112 12:45:21 @monitor.py:362]\u001b[0m one_error: 0.41889\n",
      "\u001b[32m[1112 12:45:21 @monitor.py:362]\u001b[0m ranking_loss: 0.14226\n",
      "\u001b[32m[1112 12:45:21 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.71185\n",
      "\u001b[32m[1112 12:45:21 @monitor.py:362]\u001b[0m training_ap: 0.56157\n",
      "\u001b[32m[1112 12:45:21 @monitor.py:362]\u001b[0m training_auc: 0.84257\n",
      "\u001b[32m[1112 12:45:21 @base.py:257]\u001b[0m Start Epoch 167 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:22 @base.py:267]\u001b[0m Epoch 167 (global_step 3841) finished, time:1.56 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:23 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-3841.\n",
      "\u001b[32m[1112 12:45:23 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.928\n",
      "\u001b[32m[1112 12:45:23 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.886\n",
      "\u001b[32m[1112 12:45:23 @monitor.py:362]\u001b[0m coverage: 2.0766\n",
      "\u001b[32m[1112 12:45:23 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.5753\n",
      "\u001b[32m[1112 12:45:23 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:45:23 @monitor.py:362]\u001b[0m loss/value: 7.3042\n",
      "\u001b[32m[1112 12:45:23 @monitor.py:362]\u001b[0m macro_auc: 0.85736\n",
      "\u001b[32m[1112 12:45:23 @monitor.py:362]\u001b[0m macro_f1: 0.54416\n",
      "\u001b[32m[1112 12:45:23 @monitor.py:362]\u001b[0m mean_average_precision: 0.56199\n",
      "\u001b[32m[1112 12:45:23 @monitor.py:362]\u001b[0m micro_auc: 0.8657\n",
      "\u001b[32m[1112 12:45:23 @monitor.py:362]\u001b[0m micro_f1: 0.56737\n",
      "\u001b[32m[1112 12:45:23 @monitor.py:362]\u001b[0m one_error: 0.41176\n",
      "\u001b[32m[1112 12:45:23 @monitor.py:362]\u001b[0m ranking_loss: 0.1411\n",
      "\u001b[32m[1112 12:45:23 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.71688\n",
      "\u001b[32m[1112 12:45:23 @monitor.py:362]\u001b[0m training_ap: 0.56218\n",
      "\u001b[32m[1112 12:45:23 @monitor.py:362]\u001b[0m training_auc: 0.84287\n",
      "\u001b[32m[1112 12:45:23 @base.py:257]\u001b[0m Start Epoch 168 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:25 @base.py:267]\u001b[0m Epoch 168 (global_step 3864) finished, time:1.53 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:25 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-3864.\n",
      "\u001b[32m[1112 12:45:25 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.863\n",
      "\u001b[32m[1112 12:45:25 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.926\n",
      "\u001b[32m[1112 12:45:25 @monitor.py:362]\u001b[0m coverage: 2.0642\n",
      "\u001b[32m[1112 12:45:25 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.51123\n",
      "\u001b[32m[1112 12:45:25 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:45:25 @monitor.py:362]\u001b[0m loss/value: 7.2702\n",
      "\u001b[32m[1112 12:45:25 @monitor.py:362]\u001b[0m macro_auc: 0.85826\n",
      "\u001b[32m[1112 12:45:25 @monitor.py:362]\u001b[0m macro_f1: 0.55123\n",
      "\u001b[32m[1112 12:45:25 @monitor.py:362]\u001b[0m mean_average_precision: 0.5716\n",
      "\u001b[32m[1112 12:45:25 @monitor.py:362]\u001b[0m micro_auc: 0.86633\n",
      "\u001b[32m[1112 12:45:25 @monitor.py:362]\u001b[0m micro_f1: 0.56879\n",
      "\u001b[32m[1112 12:45:25 @monitor.py:362]\u001b[0m one_error: 0.41711\n",
      "\u001b[32m[1112 12:45:25 @monitor.py:362]\u001b[0m ranking_loss: 0.14132\n",
      "\u001b[32m[1112 12:45:25 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.7131\n",
      "\u001b[32m[1112 12:45:25 @monitor.py:362]\u001b[0m training_ap: 0.56285\n",
      "\u001b[32m[1112 12:45:25 @monitor.py:362]\u001b[0m training_auc: 0.8432\n",
      "\u001b[32m[1112 12:45:25 @base.py:257]\u001b[0m Start Epoch 169 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:27 @base.py:267]\u001b[0m Epoch 169 (global_step 3887) finished, time:1.57 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:27 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-3887.\n",
      "\u001b[32m[1112 12:45:27 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.859\n",
      "\u001b[32m[1112 12:45:27 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.892\n",
      "\u001b[32m[1112 12:45:27 @monitor.py:362]\u001b[0m coverage: 2.1355\n",
      "\u001b[32m[1112 12:45:27 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 1.0174\n",
      "\u001b[32m[1112 12:45:27 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:45:27 @monitor.py:362]\u001b[0m loss/value: 7.1895\n",
      "\u001b[32m[1112 12:45:27 @monitor.py:362]\u001b[0m macro_auc: 0.85867\n",
      "\u001b[32m[1112 12:45:27 @monitor.py:362]\u001b[0m macro_f1: 0.54148\n",
      "\u001b[32m[1112 12:45:27 @monitor.py:362]\u001b[0m mean_average_precision: 0.58489\n",
      "\u001b[32m[1112 12:45:27 @monitor.py:362]\u001b[0m micro_auc: 0.86636\n",
      "\u001b[32m[1112 12:45:27 @monitor.py:362]\u001b[0m micro_f1: 0.5587\n",
      "\u001b[32m[1112 12:45:27 @monitor.py:362]\u001b[0m one_error: 0.42781\n",
      "\u001b[32m[1112 12:45:27 @monitor.py:362]\u001b[0m ranking_loss: 0.14727\n",
      "\u001b[32m[1112 12:45:27 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70475\n",
      "\u001b[32m[1112 12:45:27 @monitor.py:362]\u001b[0m training_ap: 0.56346\n",
      "\u001b[32m[1112 12:45:27 @monitor.py:362]\u001b[0m training_auc: 0.84352\n",
      "\u001b[32m[1112 12:45:27 @base.py:257]\u001b[0m Start Epoch 170 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:29 @base.py:267]\u001b[0m Epoch 170 (global_step 3910) finished, time:1.54 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:30 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-3910.\n",
      "\u001b[32m[1112 12:45:30 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.855\n",
      "\u001b[32m[1112 12:45:30 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.891\n",
      "\u001b[32m[1112 12:45:30 @monitor.py:362]\u001b[0m coverage: 2.0677\n",
      "\u001b[32m[1112 12:45:30 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.45213\n",
      "\u001b[32m[1112 12:45:30 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:45:30 @monitor.py:362]\u001b[0m loss/value: 7.2397\n",
      "\u001b[32m[1112 12:45:30 @monitor.py:362]\u001b[0m macro_auc: 0.86115\n",
      "\u001b[32m[1112 12:45:30 @monitor.py:362]\u001b[0m macro_f1: 0.55385\n",
      "\u001b[32m[1112 12:45:30 @monitor.py:362]\u001b[0m mean_average_precision: 0.57527\n",
      "\u001b[32m[1112 12:45:30 @monitor.py:362]\u001b[0m micro_auc: 0.8694\n",
      "\u001b[32m[1112 12:45:30 @monitor.py:362]\u001b[0m micro_f1: 0.57445\n",
      "\u001b[32m[1112 12:45:30 @monitor.py:362]\u001b[0m one_error: 0.41176\n",
      "\u001b[32m[1112 12:45:30 @monitor.py:362]\u001b[0m ranking_loss: 0.13989\n",
      "\u001b[32m[1112 12:45:30 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.7163\n",
      "\u001b[32m[1112 12:45:30 @monitor.py:362]\u001b[0m training_ap: 0.5641\n",
      "\u001b[32m[1112 12:45:30 @monitor.py:362]\u001b[0m training_auc: 0.84384\n",
      "\u001b[32m[1112 12:45:30 @base.py:257]\u001b[0m Start Epoch 171 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:31 @base.py:267]\u001b[0m Epoch 171 (global_step 3933) finished, time:1.57 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 7.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:32 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-3933.\n",
      "\u001b[32m[1112 12:45:32 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.853\n",
      "\u001b[32m[1112 12:45:32 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.903\n",
      "\u001b[32m[1112 12:45:32 @monitor.py:362]\u001b[0m coverage: 2.0463\n",
      "\u001b[32m[1112 12:45:32 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.48792\n",
      "\u001b[32m[1112 12:45:32 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:45:32 @monitor.py:362]\u001b[0m loss/value: 7.2066\n",
      "\u001b[32m[1112 12:45:32 @monitor.py:362]\u001b[0m macro_auc: 0.85228\n",
      "\u001b[32m[1112 12:45:32 @monitor.py:362]\u001b[0m macro_f1: 0.55302\n",
      "\u001b[32m[1112 12:45:32 @monitor.py:362]\u001b[0m mean_average_precision: 0.56501\n",
      "\u001b[32m[1112 12:45:32 @monitor.py:362]\u001b[0m micro_auc: 0.86709\n",
      "\u001b[32m[1112 12:45:32 @monitor.py:362]\u001b[0m micro_f1: 0.57425\n",
      "\u001b[32m[1112 12:45:32 @monitor.py:362]\u001b[0m one_error: 0.40642\n",
      "\u001b[32m[1112 12:45:32 @monitor.py:362]\u001b[0m ranking_loss: 0.13702\n",
      "\u001b[32m[1112 12:45:32 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.72197\n",
      "\u001b[32m[1112 12:45:32 @monitor.py:362]\u001b[0m training_ap: 0.56475\n",
      "\u001b[32m[1112 12:45:32 @monitor.py:362]\u001b[0m training_auc: 0.84417\n",
      "\u001b[32m[1112 12:45:32 @base.py:257]\u001b[0m Start Epoch 172 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########|23/23[00:01<00:00,15.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:33 @base.py:267]\u001b[0m Epoch 172 (global_step 3956) finished, time:1.49 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:34 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-3956.\n",
      "\u001b[32m[1112 12:45:34 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.851\n",
      "\u001b[32m[1112 12:45:34 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.902\n",
      "\u001b[32m[1112 12:45:34 @monitor.py:362]\u001b[0m coverage: 2.098\n",
      "\u001b[32m[1112 12:45:34 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.67618\n",
      "\u001b[32m[1112 12:45:34 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:45:34 @monitor.py:362]\u001b[0m loss/value: 7.2116\n",
      "\u001b[32m[1112 12:45:34 @monitor.py:362]\u001b[0m macro_auc: 0.85675\n",
      "\u001b[32m[1112 12:45:34 @monitor.py:362]\u001b[0m macro_f1: 0.54408\n",
      "\u001b[32m[1112 12:45:34 @monitor.py:362]\u001b[0m mean_average_precision: 0.56323\n",
      "\u001b[32m[1112 12:45:34 @monitor.py:362]\u001b[0m micro_auc: 0.86789\n",
      "\u001b[32m[1112 12:45:34 @monitor.py:362]\u001b[0m micro_f1: 0.5702\n",
      "\u001b[32m[1112 12:45:34 @monitor.py:362]\u001b[0m one_error: 0.40463\n",
      "\u001b[32m[1112 12:45:34 @monitor.py:362]\u001b[0m ranking_loss: 0.14067\n",
      "\u001b[32m[1112 12:45:34 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.71828\n",
      "\u001b[32m[1112 12:45:34 @monitor.py:362]\u001b[0m training_ap: 0.56539\n",
      "\u001b[32m[1112 12:45:34 @monitor.py:362]\u001b[0m training_auc: 0.84448\n",
      "\u001b[32m[1112 12:45:34 @base.py:257]\u001b[0m Start Epoch 173 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:36 @base.py:267]\u001b[0m Epoch 173 (global_step 3979) finished, time:1.50 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:36 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-3979.\n",
      "\u001b[32m[1112 12:45:36 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.891\n",
      "\u001b[32m[1112 12:45:36 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.891\n",
      "\u001b[32m[1112 12:45:36 @monitor.py:362]\u001b[0m coverage: 2.1337\n",
      "\u001b[32m[1112 12:45:36 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 1.7518\n",
      "\u001b[32m[1112 12:45:36 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:45:36 @monitor.py:362]\u001b[0m loss/value: 7.2575\n",
      "\u001b[32m[1112 12:45:36 @monitor.py:362]\u001b[0m macro_auc: 0.84464\n",
      "\u001b[32m[1112 12:45:36 @monitor.py:362]\u001b[0m macro_f1: 0.53906\n",
      "\u001b[32m[1112 12:45:36 @monitor.py:362]\u001b[0m mean_average_precision: 0.54666\n",
      "\u001b[32m[1112 12:45:36 @monitor.py:362]\u001b[0m micro_auc: 0.85663\n",
      "\u001b[32m[1112 12:45:36 @monitor.py:362]\u001b[0m micro_f1: 0.56239\n",
      "\u001b[32m[1112 12:45:36 @monitor.py:362]\u001b[0m one_error: 0.41533\n",
      "\u001b[32m[1112 12:45:36 @monitor.py:362]\u001b[0m ranking_loss: 0.14428\n",
      "\u001b[32m[1112 12:45:36 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.71127\n",
      "\u001b[32m[1112 12:45:36 @monitor.py:362]\u001b[0m training_ap: 0.56602\n",
      "\u001b[32m[1112 12:45:36 @monitor.py:362]\u001b[0m training_auc: 0.84478\n",
      "\u001b[32m[1112 12:45:36 @base.py:257]\u001b[0m Start Epoch 174 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:38 @base.py:267]\u001b[0m Epoch 174 (global_step 4002) finished, time:1.51 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 8.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:38 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-4002.\n",
      "\u001b[32m[1112 12:45:38 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.879\n",
      "\u001b[32m[1112 12:45:38 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.823\n",
      "\u001b[32m[1112 12:45:38 @monitor.py:362]\u001b[0m coverage: 2.0214\n",
      "\u001b[32m[1112 12:45:38 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.45182\n",
      "\u001b[32m[1112 12:45:38 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:45:38 @monitor.py:362]\u001b[0m loss/value: 7.2462\n",
      "\u001b[32m[1112 12:45:38 @monitor.py:362]\u001b[0m macro_auc: 0.86207\n",
      "\u001b[32m[1112 12:45:38 @monitor.py:362]\u001b[0m macro_f1: 0.55168\n",
      "\u001b[32m[1112 12:45:38 @monitor.py:362]\u001b[0m mean_average_precision: 0.5554\n",
      "\u001b[32m[1112 12:45:38 @monitor.py:362]\u001b[0m micro_auc: 0.86902\n",
      "\u001b[32m[1112 12:45:38 @monitor.py:362]\u001b[0m micro_f1: 0.57644\n",
      "\u001b[32m[1112 12:45:38 @monitor.py:362]\u001b[0m one_error: 0.40285\n",
      "\u001b[32m[1112 12:45:38 @monitor.py:362]\u001b[0m ranking_loss: 0.13582\n",
      "\u001b[32m[1112 12:45:38 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.72314\n",
      "\u001b[32m[1112 12:45:38 @monitor.py:362]\u001b[0m training_ap: 0.56665\n",
      "\u001b[32m[1112 12:45:38 @monitor.py:362]\u001b[0m training_auc: 0.84509\n",
      "\u001b[32m[1112 12:45:38 @base.py:257]\u001b[0m Start Epoch 175 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,13.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:40 @base.py:267]\u001b[0m Epoch 175 (global_step 4025) finished, time:1.69 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:41 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-4025.\n",
      "\u001b[32m[1112 12:45:41 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.87\n",
      "\u001b[32m[1112 12:45:41 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.883\n",
      "\u001b[32m[1112 12:45:41 @monitor.py:362]\u001b[0m coverage: 2.1301\n",
      "\u001b[32m[1112 12:45:41 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.31457\n",
      "\u001b[32m[1112 12:45:41 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:45:41 @monitor.py:362]\u001b[0m loss/value: 7.1678\n",
      "\u001b[32m[1112 12:45:41 @monitor.py:362]\u001b[0m macro_auc: 0.85187\n",
      "\u001b[32m[1112 12:45:41 @monitor.py:362]\u001b[0m macro_f1: 0.54614\n",
      "\u001b[32m[1112 12:45:41 @monitor.py:362]\u001b[0m mean_average_precision: 0.55765\n",
      "\u001b[32m[1112 12:45:41 @monitor.py:362]\u001b[0m micro_auc: 0.86293\n",
      "\u001b[32m[1112 12:45:41 @monitor.py:362]\u001b[0m micro_f1: 0.57007\n",
      "\u001b[32m[1112 12:45:41 @monitor.py:362]\u001b[0m one_error: 0.41889\n",
      "\u001b[32m[1112 12:45:41 @monitor.py:362]\u001b[0m ranking_loss: 0.14551\n",
      "\u001b[32m[1112 12:45:41 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70747\n",
      "\u001b[32m[1112 12:45:41 @monitor.py:362]\u001b[0m training_ap: 0.56727\n",
      "\u001b[32m[1112 12:45:41 @monitor.py:362]\u001b[0m training_auc: 0.84541\n",
      "\u001b[32m[1112 12:45:41 @base.py:257]\u001b[0m Start Epoch 176 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:42 @base.py:267]\u001b[0m Epoch 176 (global_step 4048) finished, time:1.56 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:43 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-4048.\n",
      "\u001b[32m[1112 12:45:43 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.862\n",
      "\u001b[32m[1112 12:45:43 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.947\n",
      "\u001b[32m[1112 12:45:43 @monitor.py:362]\u001b[0m coverage: 2.2246\n",
      "\u001b[32m[1112 12:45:43 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.19647\n",
      "\u001b[32m[1112 12:45:43 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:45:43 @monitor.py:362]\u001b[0m loss/value: 7.1629\n",
      "\u001b[32m[1112 12:45:43 @monitor.py:362]\u001b[0m macro_auc: 0.84301\n",
      "\u001b[32m[1112 12:45:43 @monitor.py:362]\u001b[0m macro_f1: 0.53498\n",
      "\u001b[32m[1112 12:45:43 @monitor.py:362]\u001b[0m mean_average_precision: 0.54101\n",
      "\u001b[32m[1112 12:45:43 @monitor.py:362]\u001b[0m micro_auc: 0.85657\n",
      "\u001b[32m[1112 12:45:43 @monitor.py:362]\u001b[0m micro_f1: 0.55947\n",
      "\u001b[32m[1112 12:45:43 @monitor.py:362]\u001b[0m one_error: 0.43494\n",
      "\u001b[32m[1112 12:45:43 @monitor.py:362]\u001b[0m ranking_loss: 0.15669\n",
      "\u001b[32m[1112 12:45:43 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.69642\n",
      "\u001b[32m[1112 12:45:43 @monitor.py:362]\u001b[0m training_ap: 0.56793\n",
      "\u001b[32m[1112 12:45:43 @monitor.py:362]\u001b[0m training_auc: 0.84572\n",
      "\u001b[32m[1112 12:45:43 @base.py:257]\u001b[0m Start Epoch 177 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:44 @base.py:267]\u001b[0m Epoch 177 (global_step 4071) finished, time:1.49 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:45 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-4071.\n",
      "\u001b[32m[1112 12:45:45 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.858\n",
      "\u001b[32m[1112 12:45:45 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.921\n",
      "\u001b[32m[1112 12:45:45 @monitor.py:362]\u001b[0m coverage: 2.0891\n",
      "\u001b[32m[1112 12:45:45 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 1.2165\n",
      "\u001b[32m[1112 12:45:45 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:45:45 @monitor.py:362]\u001b[0m loss/value: 7.175\n",
      "\u001b[32m[1112 12:45:45 @monitor.py:362]\u001b[0m macro_auc: 0.85325\n",
      "\u001b[32m[1112 12:45:45 @monitor.py:362]\u001b[0m macro_f1: 0.54038\n",
      "\u001b[32m[1112 12:45:45 @monitor.py:362]\u001b[0m mean_average_precision: 0.56536\n",
      "\u001b[32m[1112 12:45:45 @monitor.py:362]\u001b[0m micro_auc: 0.86453\n",
      "\u001b[32m[1112 12:45:45 @monitor.py:362]\u001b[0m micro_f1: 0.56164\n",
      "\u001b[32m[1112 12:45:45 @monitor.py:362]\u001b[0m one_error: 0.42602\n",
      "\u001b[32m[1112 12:45:45 @monitor.py:362]\u001b[0m ranking_loss: 0.14309\n",
      "\u001b[32m[1112 12:45:45 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.71065\n",
      "\u001b[32m[1112 12:45:45 @monitor.py:362]\u001b[0m training_ap: 0.56856\n",
      "\u001b[32m[1112 12:45:45 @monitor.py:362]\u001b[0m training_auc: 0.84602\n",
      "\u001b[32m[1112 12:45:45 @base.py:257]\u001b[0m Start Epoch 178 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:46 @base.py:267]\u001b[0m Epoch 178 (global_step 4094) finished, time:1.49 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:47 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-4094.\n",
      "\u001b[32m[1112 12:45:47 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.895\n",
      "\u001b[32m[1112 12:45:47 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.892\n",
      "\u001b[32m[1112 12:45:47 @monitor.py:362]\u001b[0m coverage: 2.0749\n",
      "\u001b[32m[1112 12:45:47 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.28637\n",
      "\u001b[32m[1112 12:45:47 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:45:47 @monitor.py:362]\u001b[0m loss/value: 7.1457\n",
      "\u001b[32m[1112 12:45:47 @monitor.py:362]\u001b[0m macro_auc: 0.85207\n",
      "\u001b[32m[1112 12:45:47 @monitor.py:362]\u001b[0m macro_f1: 0.5425\n",
      "\u001b[32m[1112 12:45:47 @monitor.py:362]\u001b[0m mean_average_precision: 0.54401\n",
      "\u001b[32m[1112 12:45:47 @monitor.py:362]\u001b[0m micro_auc: 0.86118\n",
      "\u001b[32m[1112 12:45:47 @monitor.py:362]\u001b[0m micro_f1: 0.56571\n",
      "\u001b[32m[1112 12:45:47 @monitor.py:362]\u001b[0m one_error: 0.43494\n",
      "\u001b[32m[1112 12:45:47 @monitor.py:362]\u001b[0m ranking_loss: 0.14079\n",
      "\u001b[32m[1112 12:45:47 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70982\n",
      "\u001b[32m[1112 12:45:47 @monitor.py:362]\u001b[0m training_ap: 0.56919\n",
      "\u001b[32m[1112 12:45:47 @monitor.py:362]\u001b[0m training_auc: 0.84633\n",
      "\u001b[32m[1112 12:45:47 @base.py:257]\u001b[0m Start Epoch 179 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:49 @base.py:267]\u001b[0m Epoch 179 (global_step 4117) finished, time:1.59 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:49 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-4117.\n",
      "\u001b[32m[1112 12:45:49 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.882\n",
      "\u001b[32m[1112 12:45:49 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.876\n",
      "\u001b[32m[1112 12:45:49 @monitor.py:362]\u001b[0m coverage: 2.1016\n",
      "\u001b[32m[1112 12:45:49 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.53406\n",
      "\u001b[32m[1112 12:45:49 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:45:49 @monitor.py:362]\u001b[0m loss/value: 7.0668\n",
      "\u001b[32m[1112 12:45:49 @monitor.py:362]\u001b[0m macro_auc: 0.84846\n",
      "\u001b[32m[1112 12:45:49 @monitor.py:362]\u001b[0m macro_f1: 0.55732\n",
      "\u001b[32m[1112 12:45:49 @monitor.py:362]\u001b[0m mean_average_precision: 0.55437\n",
      "\u001b[32m[1112 12:45:49 @monitor.py:362]\u001b[0m micro_auc: 0.86212\n",
      "\u001b[32m[1112 12:45:49 @monitor.py:362]\u001b[0m micro_f1: 0.57274\n",
      "\u001b[32m[1112 12:45:49 @monitor.py:362]\u001b[0m one_error: 0.41176\n",
      "\u001b[32m[1112 12:45:49 @monitor.py:362]\u001b[0m ranking_loss: 0.14241\n",
      "\u001b[32m[1112 12:45:49 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.71442\n",
      "\u001b[32m[1112 12:45:49 @monitor.py:362]\u001b[0m training_ap: 0.56979\n",
      "\u001b[32m[1112 12:45:49 @monitor.py:362]\u001b[0m training_auc: 0.84663\n",
      "\u001b[32m[1112 12:45:49 @base.py:257]\u001b[0m Start Epoch 180 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:51 @base.py:267]\u001b[0m Epoch 180 (global_step 4140) finished, time:1.51 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,11.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:51 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-4140.\n",
      "\u001b[32m[1112 12:45:51 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.873\n",
      "\u001b[32m[1112 12:45:51 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.862\n",
      "\u001b[32m[1112 12:45:51 @monitor.py:362]\u001b[0m coverage: 2.0606\n",
      "\u001b[32m[1112 12:45:51 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.43222\n",
      "\u001b[32m[1112 12:45:51 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:45:51 @monitor.py:362]\u001b[0m loss/value: 7.0328\n",
      "\u001b[32m[1112 12:45:51 @monitor.py:362]\u001b[0m macro_auc: 0.85389\n",
      "\u001b[32m[1112 12:45:51 @monitor.py:362]\u001b[0m macro_f1: 0.54244\n",
      "\u001b[32m[1112 12:45:51 @monitor.py:362]\u001b[0m mean_average_precision: 0.55586\n",
      "\u001b[32m[1112 12:45:51 @monitor.py:362]\u001b[0m micro_auc: 0.86485\n",
      "\u001b[32m[1112 12:45:51 @monitor.py:362]\u001b[0m micro_f1: 0.56612\n",
      "\u001b[32m[1112 12:45:51 @monitor.py:362]\u001b[0m one_error: 0.40998\n",
      "\u001b[32m[1112 12:45:51 @monitor.py:362]\u001b[0m ranking_loss: 0.13969\n",
      "\u001b[32m[1112 12:45:51 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.71764\n",
      "\u001b[32m[1112 12:45:51 @monitor.py:362]\u001b[0m training_ap: 0.5704\n",
      "\u001b[32m[1112 12:45:51 @monitor.py:362]\u001b[0m training_auc: 0.84695\n",
      "\u001b[32m[1112 12:45:51 @base.py:257]\u001b[0m Start Epoch 181 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:53 @base.py:267]\u001b[0m Epoch 181 (global_step 4163) finished, time:1.45 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:53 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-4163.\n",
      "\u001b[32m[1112 12:45:53 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.906\n",
      "\u001b[32m[1112 12:45:53 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.84\n",
      "\u001b[32m[1112 12:45:53 @monitor.py:362]\u001b[0m coverage: 2.0642\n",
      "\u001b[32m[1112 12:45:53 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.51363\n",
      "\u001b[32m[1112 12:45:53 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:45:53 @monitor.py:362]\u001b[0m loss/value: 7.025\n",
      "\u001b[32m[1112 12:45:53 @monitor.py:362]\u001b[0m macro_auc: 0.85599\n",
      "\u001b[32m[1112 12:45:53 @monitor.py:362]\u001b[0m macro_f1: 0.55079\n",
      "\u001b[32m[1112 12:45:53 @monitor.py:362]\u001b[0m mean_average_precision: 0.55588\n",
      "\u001b[32m[1112 12:45:53 @monitor.py:362]\u001b[0m micro_auc: 0.86737\n",
      "\u001b[32m[1112 12:45:53 @monitor.py:362]\u001b[0m micro_f1: 0.57002\n",
      "\u001b[32m[1112 12:45:53 @monitor.py:362]\u001b[0m one_error: 0.41711\n",
      "\u001b[32m[1112 12:45:53 @monitor.py:362]\u001b[0m ranking_loss: 0.13968\n",
      "\u001b[32m[1112 12:45:53 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.71387\n",
      "\u001b[32m[1112 12:45:53 @monitor.py:362]\u001b[0m training_ap: 0.57106\n",
      "\u001b[32m[1112 12:45:53 @monitor.py:362]\u001b[0m training_auc: 0.84728\n",
      "\u001b[32m[1112 12:45:53 @base.py:257]\u001b[0m Start Epoch 182 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:55 @base.py:267]\u001b[0m Epoch 182 (global_step 4186) finished, time:1.50 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:55 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-4186.\n",
      "\u001b[32m[1112 12:45:55 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.931\n",
      "\u001b[32m[1112 12:45:55 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.861\n",
      "\u001b[32m[1112 12:45:55 @monitor.py:362]\u001b[0m coverage: 2.1016\n",
      "\u001b[32m[1112 12:45:55 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.68877\n",
      "\u001b[32m[1112 12:45:55 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:45:55 @monitor.py:362]\u001b[0m loss/value: 7.0462\n",
      "\u001b[32m[1112 12:45:55 @monitor.py:362]\u001b[0m macro_auc: 0.8505\n",
      "\u001b[32m[1112 12:45:55 @monitor.py:362]\u001b[0m macro_f1: 0.54275\n",
      "\u001b[32m[1112 12:45:55 @monitor.py:362]\u001b[0m mean_average_precision: 0.56626\n",
      "\u001b[32m[1112 12:45:55 @monitor.py:362]\u001b[0m micro_auc: 0.86074\n",
      "\u001b[32m[1112 12:45:55 @monitor.py:362]\u001b[0m micro_f1: 0.5667\n",
      "\u001b[32m[1112 12:45:55 @monitor.py:362]\u001b[0m one_error: 0.39929\n",
      "\u001b[32m[1112 12:45:55 @monitor.py:362]\u001b[0m ranking_loss: 0.14051\n",
      "\u001b[32m[1112 12:45:55 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.71904\n",
      "\u001b[32m[1112 12:45:55 @monitor.py:362]\u001b[0m training_ap: 0.57172\n",
      "\u001b[32m[1112 12:45:55 @monitor.py:362]\u001b[0m training_auc: 0.84759\n",
      "\u001b[32m[1112 12:45:55 @base.py:257]\u001b[0m Start Epoch 183 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:57 @base.py:267]\u001b[0m Epoch 183 (global_step 4209) finished, time:1.52 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:57 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-4209.\n",
      "\u001b[32m[1112 12:45:57 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.949\n",
      "\u001b[32m[1112 12:45:57 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.918\n",
      "\u001b[32m[1112 12:45:57 @monitor.py:362]\u001b[0m coverage: 2.0891\n",
      "\u001b[32m[1112 12:45:57 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.62497\n",
      "\u001b[32m[1112 12:45:57 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:45:57 @monitor.py:362]\u001b[0m loss/value: 7.0333\n",
      "\u001b[32m[1112 12:45:57 @monitor.py:362]\u001b[0m macro_auc: 0.85882\n",
      "\u001b[32m[1112 12:45:57 @monitor.py:362]\u001b[0m macro_f1: 0.54807\n",
      "\u001b[32m[1112 12:45:57 @monitor.py:362]\u001b[0m mean_average_precision: 0.55661\n",
      "\u001b[32m[1112 12:45:57 @monitor.py:362]\u001b[0m micro_auc: 0.86664\n",
      "\u001b[32m[1112 12:45:57 @monitor.py:362]\u001b[0m micro_f1: 0.56744\n",
      "\u001b[32m[1112 12:45:57 @monitor.py:362]\u001b[0m one_error: 0.41889\n",
      "\u001b[32m[1112 12:45:57 @monitor.py:362]\u001b[0m ranking_loss: 0.13979\n",
      "\u001b[32m[1112 12:45:57 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.71247\n",
      "\u001b[32m[1112 12:45:57 @monitor.py:362]\u001b[0m training_ap: 0.57231\n",
      "\u001b[32m[1112 12:45:57 @monitor.py:362]\u001b[0m training_auc: 0.84788\n",
      "\u001b[32m[1112 12:45:57 @base.py:257]\u001b[0m Start Epoch 184 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:59 @base.py:267]\u001b[0m Epoch 184 (global_step 4232) finished, time:1.56 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:45:59 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-4232.\n",
      "\u001b[32m[1112 12:45:59 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.963\n",
      "\u001b[32m[1112 12:45:59 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.864\n",
      "\u001b[32m[1112 12:45:59 @monitor.py:362]\u001b[0m coverage: 2.0374\n",
      "\u001b[32m[1112 12:45:59 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.61921\n",
      "\u001b[32m[1112 12:45:59 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:45:59 @monitor.py:362]\u001b[0m loss/value: 7.0348\n",
      "\u001b[32m[1112 12:45:59 @monitor.py:362]\u001b[0m macro_auc: 0.86788\n",
      "\u001b[32m[1112 12:45:59 @monitor.py:362]\u001b[0m macro_f1: 0.56289\n",
      "\u001b[32m[1112 12:45:59 @monitor.py:362]\u001b[0m mean_average_precision: 0.58559\n",
      "\u001b[32m[1112 12:45:59 @monitor.py:362]\u001b[0m micro_auc: 0.87459\n",
      "\u001b[32m[1112 12:45:59 @monitor.py:362]\u001b[0m micro_f1: 0.57897\n",
      "\u001b[32m[1112 12:45:59 @monitor.py:362]\u001b[0m one_error: 0.40463\n",
      "\u001b[32m[1112 12:45:59 @monitor.py:362]\u001b[0m ranking_loss: 0.13667\n",
      "\u001b[32m[1112 12:45:59 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.72344\n",
      "\u001b[32m[1112 12:45:59 @monitor.py:362]\u001b[0m training_ap: 0.57294\n",
      "\u001b[32m[1112 12:45:59 @monitor.py:362]\u001b[0m training_auc: 0.84819\n",
      "\u001b[32m[1112 12:45:59 @base.py:257]\u001b[0m Start Epoch 185 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:46:01 @base.py:267]\u001b[0m Epoch 185 (global_step 4255) finished, time:1.47 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 8.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:46:02 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-4255.\n",
      "\u001b[32m[1112 12:46:02 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.973\n",
      "\u001b[32m[1112 12:46:02 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.895\n",
      "\u001b[32m[1112 12:46:02 @monitor.py:362]\u001b[0m coverage: 2.0998\n",
      "\u001b[32m[1112 12:46:02 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.23268\n",
      "\u001b[32m[1112 12:46:02 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:46:02 @monitor.py:362]\u001b[0m loss/value: 7.1553\n",
      "\u001b[32m[1112 12:46:02 @monitor.py:362]\u001b[0m macro_auc: 0.85798\n",
      "\u001b[32m[1112 12:46:02 @monitor.py:362]\u001b[0m macro_f1: 0.54827\n",
      "\u001b[32m[1112 12:46:02 @monitor.py:362]\u001b[0m mean_average_precision: 0.57016\n",
      "\u001b[32m[1112 12:46:02 @monitor.py:362]\u001b[0m micro_auc: 0.86747\n",
      "\u001b[32m[1112 12:46:02 @monitor.py:362]\u001b[0m micro_f1: 0.56444\n",
      "\u001b[32m[1112 12:46:02 @monitor.py:362]\u001b[0m one_error: 0.41711\n",
      "\u001b[32m[1112 12:46:02 @monitor.py:362]\u001b[0m ranking_loss: 0.14245\n",
      "\u001b[32m[1112 12:46:02 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.71189\n",
      "\u001b[32m[1112 12:46:02 @monitor.py:362]\u001b[0m training_ap: 0.5735\n",
      "\u001b[32m[1112 12:46:02 @monitor.py:362]\u001b[0m training_auc: 0.84845\n",
      "\u001b[32m[1112 12:46:02 @base.py:257]\u001b[0m Start Epoch 186 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:46:03 @base.py:267]\u001b[0m Epoch 186 (global_step 4278) finished, time:1.48 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,11.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:46:04 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-4278.\n",
      "\u001b[32m[1112 12:46:04 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.98\n",
      "\u001b[32m[1112 12:46:04 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.968\n",
      "\u001b[32m[1112 12:46:04 @monitor.py:362]\u001b[0m coverage: 2.0749\n",
      "\u001b[32m[1112 12:46:04 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 1.1025\n",
      "\u001b[32m[1112 12:46:04 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:46:04 @monitor.py:362]\u001b[0m loss/value: 7.0459\n",
      "\u001b[32m[1112 12:46:04 @monitor.py:362]\u001b[0m macro_auc: 0.85587\n",
      "\u001b[32m[1112 12:46:04 @monitor.py:362]\u001b[0m macro_f1: 0.55086\n",
      "\u001b[32m[1112 12:46:04 @monitor.py:362]\u001b[0m mean_average_precision: 0.56309\n",
      "\u001b[32m[1112 12:46:04 @monitor.py:362]\u001b[0m micro_auc: 0.86761\n",
      "\u001b[32m[1112 12:46:04 @monitor.py:362]\u001b[0m micro_f1: 0.57266\n",
      "\u001b[32m[1112 12:46:04 @monitor.py:362]\u001b[0m one_error: 0.4082\n",
      "\u001b[32m[1112 12:46:04 @monitor.py:362]\u001b[0m ranking_loss: 0.13916\n",
      "\u001b[32m[1112 12:46:04 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.71731\n",
      "\u001b[32m[1112 12:46:04 @monitor.py:362]\u001b[0m training_ap: 0.57412\n",
      "\u001b[32m[1112 12:46:04 @monitor.py:362]\u001b[0m training_auc: 0.84876\n",
      "\u001b[32m[1112 12:46:04 @base.py:257]\u001b[0m Start Epoch 187 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:46:05 @base.py:267]\u001b[0m Epoch 187 (global_step 4301) finished, time:1.55 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:46:06 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-4301.\n",
      "\u001b[32m[1112 12:46:06 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.945\n",
      "\u001b[32m[1112 12:46:06 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.905\n",
      "\u001b[32m[1112 12:46:06 @monitor.py:362]\u001b[0m coverage: 2.0802\n",
      "\u001b[32m[1112 12:46:06 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 1.4063\n",
      "\u001b[32m[1112 12:46:06 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:46:06 @monitor.py:362]\u001b[0m loss/value: 7.2541\n",
      "\u001b[32m[1112 12:46:06 @monitor.py:362]\u001b[0m macro_auc: 0.8539\n",
      "\u001b[32m[1112 12:46:06 @monitor.py:362]\u001b[0m macro_f1: 0.54376\n",
      "\u001b[32m[1112 12:46:06 @monitor.py:362]\u001b[0m mean_average_precision: 0.56498\n",
      "\u001b[32m[1112 12:46:06 @monitor.py:362]\u001b[0m micro_auc: 0.86681\n",
      "\u001b[32m[1112 12:46:06 @monitor.py:362]\u001b[0m micro_f1: 0.56857\n",
      "\u001b[32m[1112 12:46:06 @monitor.py:362]\u001b[0m one_error: 0.40642\n",
      "\u001b[32m[1112 12:46:06 @monitor.py:362]\u001b[0m ranking_loss: 0.13994\n",
      "\u001b[32m[1112 12:46:06 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.72225\n",
      "\u001b[32m[1112 12:46:06 @monitor.py:362]\u001b[0m training_ap: 0.57472\n",
      "\u001b[32m[1112 12:46:06 @monitor.py:362]\u001b[0m training_auc: 0.84904\n",
      "\u001b[32m[1112 12:46:06 @base.py:257]\u001b[0m Start Epoch 188 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:46:07 @base.py:267]\u001b[0m Epoch 188 (global_step 4324) finished, time:1.52 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:46:08 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-4324.\n",
      "\u001b[32m[1112 12:46:08 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.959\n",
      "\u001b[32m[1112 12:46:08 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.915\n",
      "\u001b[32m[1112 12:46:08 @monitor.py:362]\u001b[0m coverage: 2.0232\n",
      "\u001b[32m[1112 12:46:08 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.55215\n",
      "\u001b[32m[1112 12:46:08 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:46:08 @monitor.py:362]\u001b[0m loss/value: 7.2319\n",
      "\u001b[32m[1112 12:46:08 @monitor.py:362]\u001b[0m macro_auc: 0.86525\n",
      "\u001b[32m[1112 12:46:08 @monitor.py:362]\u001b[0m macro_f1: 0.54004\n",
      "\u001b[32m[1112 12:46:08 @monitor.py:362]\u001b[0m mean_average_precision: 0.57981\n",
      "\u001b[32m[1112 12:46:08 @monitor.py:362]\u001b[0m micro_auc: 0.87232\n",
      "\u001b[32m[1112 12:46:08 @monitor.py:362]\u001b[0m micro_f1: 0.56554\n",
      "\u001b[32m[1112 12:46:08 @monitor.py:362]\u001b[0m one_error: 0.3975\n",
      "\u001b[32m[1112 12:46:08 @monitor.py:362]\u001b[0m ranking_loss: 0.13337\n",
      "\u001b[32m[1112 12:46:08 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.72675\n",
      "\u001b[32m[1112 12:46:08 @monitor.py:362]\u001b[0m training_ap: 0.5753\n",
      "\u001b[32m[1112 12:46:08 @monitor.py:362]\u001b[0m training_auc: 0.84931\n",
      "\u001b[32m[1112 12:46:08 @base.py:257]\u001b[0m Start Epoch 189 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,16.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:46:09 @base.py:267]\u001b[0m Epoch 189 (global_step 4347) finished, time:1.44 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:46:10 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-4347.\n",
      "\u001b[32m[1112 12:46:10 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.929\n",
      "\u001b[32m[1112 12:46:10 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.861\n",
      "\u001b[32m[1112 12:46:10 @monitor.py:362]\u001b[0m coverage: 2.0624\n",
      "\u001b[32m[1112 12:46:10 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.73812\n",
      "\u001b[32m[1112 12:46:10 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:46:10 @monitor.py:362]\u001b[0m loss/value: 7.062\n",
      "\u001b[32m[1112 12:46:10 @monitor.py:362]\u001b[0m macro_auc: 0.85895\n",
      "\u001b[32m[1112 12:46:10 @monitor.py:362]\u001b[0m macro_f1: 0.54733\n",
      "\u001b[32m[1112 12:46:10 @monitor.py:362]\u001b[0m mean_average_precision: 0.56397\n",
      "\u001b[32m[1112 12:46:10 @monitor.py:362]\u001b[0m micro_auc: 0.86737\n",
      "\u001b[32m[1112 12:46:10 @monitor.py:362]\u001b[0m micro_f1: 0.56943\n",
      "\u001b[32m[1112 12:46:10 @monitor.py:362]\u001b[0m one_error: 0.41355\n",
      "\u001b[32m[1112 12:46:10 @monitor.py:362]\u001b[0m ranking_loss: 0.13981\n",
      "\u001b[32m[1112 12:46:10 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.71639\n",
      "\u001b[32m[1112 12:46:10 @monitor.py:362]\u001b[0m training_ap: 0.57596\n",
      "\u001b[32m[1112 12:46:10 @monitor.py:362]\u001b[0m training_auc: 0.84962\n",
      "\u001b[32m[1112 12:46:10 @base.py:257]\u001b[0m Start Epoch 190 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:46:11 @base.py:267]\u001b[0m Epoch 190 (global_step 4370) finished, time:1.53 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:46:12 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-4370.\n",
      "\u001b[32m[1112 12:46:12 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.907\n",
      "\u001b[32m[1112 12:46:12 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.919\n",
      "\u001b[32m[1112 12:46:12 @monitor.py:362]\u001b[0m coverage: 2.0963\n",
      "\u001b[32m[1112 12:46:12 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.28796\n",
      "\u001b[32m[1112 12:46:12 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:46:12 @monitor.py:362]\u001b[0m loss/value: 7.0056\n",
      "\u001b[32m[1112 12:46:12 @monitor.py:362]\u001b[0m macro_auc: 0.85451\n",
      "\u001b[32m[1112 12:46:12 @monitor.py:362]\u001b[0m macro_f1: 0.54587\n",
      "\u001b[32m[1112 12:46:12 @monitor.py:362]\u001b[0m mean_average_precision: 0.55514\n",
      "\u001b[32m[1112 12:46:12 @monitor.py:362]\u001b[0m micro_auc: 0.86494\n",
      "\u001b[32m[1112 12:46:12 @monitor.py:362]\u001b[0m micro_f1: 0.5709\n",
      "\u001b[32m[1112 12:46:12 @monitor.py:362]\u001b[0m one_error: 0.42246\n",
      "\u001b[32m[1112 12:46:12 @monitor.py:362]\u001b[0m ranking_loss: 0.14423\n",
      "\u001b[32m[1112 12:46:12 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.711\n",
      "\u001b[32m[1112 12:46:12 @monitor.py:362]\u001b[0m training_ap: 0.57652\n",
      "\u001b[32m[1112 12:46:12 @monitor.py:362]\u001b[0m training_auc: 0.8499\n",
      "\u001b[32m[1112 12:46:12 @base.py:257]\u001b[0m Start Epoch 191 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:46:14 @base.py:267]\u001b[0m Epoch 191 (global_step 4393) finished, time:1.51 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:46:14 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-4393.\n",
      "\u001b[32m[1112 12:46:14 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.891\n",
      "\u001b[32m[1112 12:46:14 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.873\n",
      "\u001b[32m[1112 12:46:14 @monitor.py:362]\u001b[0m coverage: 2.1497\n",
      "\u001b[32m[1112 12:46:14 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.30858\n",
      "\u001b[32m[1112 12:46:14 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:46:14 @monitor.py:362]\u001b[0m loss/value: 7.0119\n",
      "\u001b[32m[1112 12:46:14 @monitor.py:362]\u001b[0m macro_auc: 0.84677\n",
      "\u001b[32m[1112 12:46:14 @monitor.py:362]\u001b[0m macro_f1: 0.54964\n",
      "\u001b[32m[1112 12:46:14 @monitor.py:362]\u001b[0m mean_average_precision: 0.56939\n",
      "\u001b[32m[1112 12:46:14 @monitor.py:362]\u001b[0m micro_auc: 0.86052\n",
      "\u001b[32m[1112 12:46:14 @monitor.py:362]\u001b[0m micro_f1: 0.57023\n",
      "\u001b[32m[1112 12:46:14 @monitor.py:362]\u001b[0m one_error: 0.41355\n",
      "\u001b[32m[1112 12:46:14 @monitor.py:362]\u001b[0m ranking_loss: 0.1477\n",
      "\u001b[32m[1112 12:46:14 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.71139\n",
      "\u001b[32m[1112 12:46:14 @monitor.py:362]\u001b[0m training_ap: 0.5771\n",
      "\u001b[32m[1112 12:46:14 @monitor.py:362]\u001b[0m training_auc: 0.85021\n",
      "\u001b[32m[1112 12:46:14 @base.py:257]\u001b[0m Start Epoch 192 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:46:16 @base.py:267]\u001b[0m Epoch 192 (global_step 4416) finished, time:1.58 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:46:16 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-4416.\n",
      "\u001b[32m[1112 12:46:16 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.92\n",
      "\u001b[32m[1112 12:46:16 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.778\n",
      "\u001b[32m[1112 12:46:16 @monitor.py:362]\u001b[0m coverage: 2.1194\n",
      "\u001b[32m[1112 12:46:16 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.74279\n",
      "\u001b[32m[1112 12:46:16 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:46:16 @monitor.py:362]\u001b[0m loss/value: 7.0088\n",
      "\u001b[32m[1112 12:46:16 @monitor.py:362]\u001b[0m macro_auc: 0.84589\n",
      "\u001b[32m[1112 12:46:16 @monitor.py:362]\u001b[0m macro_f1: 0.53888\n",
      "\u001b[32m[1112 12:46:16 @monitor.py:362]\u001b[0m mean_average_precision: 0.54218\n",
      "\u001b[32m[1112 12:46:16 @monitor.py:362]\u001b[0m micro_auc: 0.86093\n",
      "\u001b[32m[1112 12:46:16 @monitor.py:362]\u001b[0m micro_f1: 0.56166\n",
      "\u001b[32m[1112 12:46:16 @monitor.py:362]\u001b[0m one_error: 0.41533\n",
      "\u001b[32m[1112 12:46:16 @monitor.py:362]\u001b[0m ranking_loss: 0.14469\n",
      "\u001b[32m[1112 12:46:16 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.71151\n",
      "\u001b[32m[1112 12:46:16 @monitor.py:362]\u001b[0m training_ap: 0.57769\n",
      "\u001b[32m[1112 12:46:16 @monitor.py:362]\u001b[0m training_auc: 0.8505\n",
      "\u001b[32m[1112 12:46:16 @base.py:257]\u001b[0m Start Epoch 193 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:46:18 @base.py:267]\u001b[0m Epoch 193 (global_step 4439) finished, time:1.55 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:46:18 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-4439.\n",
      "\u001b[32m[1112 12:46:18 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.9\n",
      "\u001b[32m[1112 12:46:18 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.888\n",
      "\u001b[32m[1112 12:46:18 @monitor.py:362]\u001b[0m coverage: 2.1034\n",
      "\u001b[32m[1112 12:46:18 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 1.1658\n",
      "\u001b[32m[1112 12:46:18 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:46:18 @monitor.py:362]\u001b[0m loss/value: 6.9396\n",
      "\u001b[32m[1112 12:46:18 @monitor.py:362]\u001b[0m macro_auc: 0.85472\n",
      "\u001b[32m[1112 12:46:18 @monitor.py:362]\u001b[0m macro_f1: 0.54408\n",
      "\u001b[32m[1112 12:46:18 @monitor.py:362]\u001b[0m mean_average_precision: 0.56014\n",
      "\u001b[32m[1112 12:46:18 @monitor.py:362]\u001b[0m micro_auc: 0.86637\n",
      "\u001b[32m[1112 12:46:18 @monitor.py:362]\u001b[0m micro_f1: 0.56695\n",
      "\u001b[32m[1112 12:46:18 @monitor.py:362]\u001b[0m one_error: 0.42602\n",
      "\u001b[32m[1112 12:46:18 @monitor.py:362]\u001b[0m ranking_loss: 0.14405\n",
      "\u001b[32m[1112 12:46:18 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.71047\n",
      "\u001b[32m[1112 12:46:18 @monitor.py:362]\u001b[0m training_ap: 0.5783\n",
      "\u001b[32m[1112 12:46:18 @monitor.py:362]\u001b[0m training_auc: 0.85079\n",
      "\u001b[32m[1112 12:46:18 @base.py:257]\u001b[0m Start Epoch 194 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:46:20 @base.py:267]\u001b[0m Epoch 194 (global_step 4462) finished, time:1.50 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:46:20 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-4462.\n",
      "\u001b[32m[1112 12:46:20 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.927\n",
      "\u001b[32m[1112 12:46:20 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.908\n",
      "\u001b[32m[1112 12:46:20 @monitor.py:362]\u001b[0m coverage: 2.1515\n",
      "\u001b[32m[1112 12:46:20 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.7304\n",
      "\u001b[32m[1112 12:46:20 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:46:20 @monitor.py:362]\u001b[0m loss/value: 7.1257\n",
      "\u001b[32m[1112 12:46:20 @monitor.py:362]\u001b[0m macro_auc: 0.8525\n",
      "\u001b[32m[1112 12:46:20 @monitor.py:362]\u001b[0m macro_f1: 0.54853\n",
      "\u001b[32m[1112 12:46:20 @monitor.py:362]\u001b[0m mean_average_precision: 0.56778\n",
      "\u001b[32m[1112 12:46:20 @monitor.py:362]\u001b[0m micro_auc: 0.86375\n",
      "\u001b[32m[1112 12:46:20 @monitor.py:362]\u001b[0m micro_f1: 0.5669\n",
      "\u001b[32m[1112 12:46:20 @monitor.py:362]\u001b[0m one_error: 0.43316\n",
      "\u001b[32m[1112 12:46:20 @monitor.py:362]\u001b[0m ranking_loss: 0.14788\n",
      "\u001b[32m[1112 12:46:20 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70196\n",
      "\u001b[32m[1112 12:46:20 @monitor.py:362]\u001b[0m training_ap: 0.57883\n",
      "\u001b[32m[1112 12:46:20 @monitor.py:362]\u001b[0m training_auc: 0.85105\n",
      "\u001b[32m[1112 12:46:20 @base.py:257]\u001b[0m Start Epoch 195 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:46:22 @base.py:267]\u001b[0m Epoch 195 (global_step 4485) finished, time:1.59 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 8.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:46:23 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-4485.\n",
      "\u001b[32m[1112 12:46:23 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.946\n",
      "\u001b[32m[1112 12:46:23 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.896\n",
      "\u001b[32m[1112 12:46:23 @monitor.py:362]\u001b[0m coverage: 2.0606\n",
      "\u001b[32m[1112 12:46:23 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.91617\n",
      "\u001b[32m[1112 12:46:23 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:46:23 @monitor.py:362]\u001b[0m loss/value: 7.0951\n",
      "\u001b[32m[1112 12:46:23 @monitor.py:362]\u001b[0m macro_auc: 0.84941\n",
      "\u001b[32m[1112 12:46:23 @monitor.py:362]\u001b[0m macro_f1: 0.55327\n",
      "\u001b[32m[1112 12:46:23 @monitor.py:362]\u001b[0m mean_average_precision: 0.57455\n",
      "\u001b[32m[1112 12:46:23 @monitor.py:362]\u001b[0m micro_auc: 0.86518\n",
      "\u001b[32m[1112 12:46:23 @monitor.py:362]\u001b[0m micro_f1: 0.57775\n",
      "\u001b[32m[1112 12:46:23 @monitor.py:362]\u001b[0m one_error: 0.42068\n",
      "\u001b[32m[1112 12:46:23 @monitor.py:362]\u001b[0m ranking_loss: 0.14156\n",
      "\u001b[32m[1112 12:46:23 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.71355\n",
      "\u001b[32m[1112 12:46:23 @monitor.py:362]\u001b[0m training_ap: 0.57939\n",
      "\u001b[32m[1112 12:46:23 @monitor.py:362]\u001b[0m training_auc: 0.85132\n",
      "\u001b[32m[1112 12:46:23 @base.py:257]\u001b[0m Start Epoch 196 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:46:24 @base.py:267]\u001b[0m Epoch 196 (global_step 4508) finished, time:1.52 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:46:25 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-4508.\n",
      "\u001b[32m[1112 12:46:25 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.92\n",
      "\u001b[32m[1112 12:46:25 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.867\n",
      "\u001b[32m[1112 12:46:25 @monitor.py:362]\u001b[0m coverage: 2.1586\n",
      "\u001b[32m[1112 12:46:25 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.30716\n",
      "\u001b[32m[1112 12:46:25 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:46:25 @monitor.py:362]\u001b[0m loss/value: 6.9998\n",
      "\u001b[32m[1112 12:46:25 @monitor.py:362]\u001b[0m macro_auc: 0.84727\n",
      "\u001b[32m[1112 12:46:25 @monitor.py:362]\u001b[0m macro_f1: 0.54038\n",
      "\u001b[32m[1112 12:46:25 @monitor.py:362]\u001b[0m mean_average_precision: 0.54343\n",
      "\u001b[32m[1112 12:46:25 @monitor.py:362]\u001b[0m micro_auc: 0.85955\n",
      "\u001b[32m[1112 12:46:25 @monitor.py:362]\u001b[0m micro_f1: 0.56411\n",
      "\u001b[32m[1112 12:46:25 @monitor.py:362]\u001b[0m one_error: 0.42781\n",
      "\u001b[32m[1112 12:46:25 @monitor.py:362]\u001b[0m ranking_loss: 0.15125\n",
      "\u001b[32m[1112 12:46:25 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70353\n",
      "\u001b[32m[1112 12:46:25 @monitor.py:362]\u001b[0m training_ap: 0.57997\n",
      "\u001b[32m[1112 12:46:25 @monitor.py:362]\u001b[0m training_auc: 0.85159\n",
      "\u001b[32m[1112 12:46:25 @base.py:257]\u001b[0m Start Epoch 197 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,13.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:46:26 @base.py:267]\u001b[0m Epoch 197 (global_step 4531) finished, time:1.72 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:46:27 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-4531.\n",
      "\u001b[32m[1112 12:46:27 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.941\n",
      "\u001b[32m[1112 12:46:27 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.959\n",
      "\u001b[32m[1112 12:46:27 @monitor.py:362]\u001b[0m coverage: 2.0642\n",
      "\u001b[32m[1112 12:46:27 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.94695\n",
      "\u001b[32m[1112 12:46:27 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:46:27 @monitor.py:362]\u001b[0m loss/value: 7.0275\n",
      "\u001b[32m[1112 12:46:27 @monitor.py:362]\u001b[0m macro_auc: 0.8533\n",
      "\u001b[32m[1112 12:46:27 @monitor.py:362]\u001b[0m macro_f1: 0.54858\n",
      "\u001b[32m[1112 12:46:27 @monitor.py:362]\u001b[0m mean_average_precision: 0.55757\n",
      "\u001b[32m[1112 12:46:27 @monitor.py:362]\u001b[0m micro_auc: 0.86605\n",
      "\u001b[32m[1112 12:46:27 @monitor.py:362]\u001b[0m micro_f1: 0.57155\n",
      "\u001b[32m[1112 12:46:27 @monitor.py:362]\u001b[0m one_error: 0.4082\n",
      "\u001b[32m[1112 12:46:27 @monitor.py:362]\u001b[0m ranking_loss: 0.1383\n",
      "\u001b[32m[1112 12:46:27 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.72169\n",
      "\u001b[32m[1112 12:46:27 @monitor.py:362]\u001b[0m training_ap: 0.58054\n",
      "\u001b[32m[1112 12:46:27 @monitor.py:362]\u001b[0m training_auc: 0.85188\n",
      "\u001b[32m[1112 12:46:27 @base.py:257]\u001b[0m Start Epoch 198 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:46:29 @base.py:267]\u001b[0m Epoch 198 (global_step 4554) finished, time:1.53 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,11.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:46:29 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-4554.\n",
      "\u001b[32m[1112 12:46:29 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.957\n",
      "\u001b[32m[1112 12:46:29 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.931\n",
      "\u001b[32m[1112 12:46:29 @monitor.py:362]\u001b[0m coverage: 2.0963\n",
      "\u001b[32m[1112 12:46:29 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 1.8028\n",
      "\u001b[32m[1112 12:46:29 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:46:29 @monitor.py:362]\u001b[0m loss/value: 6.9792\n",
      "\u001b[32m[1112 12:46:29 @monitor.py:362]\u001b[0m macro_auc: 0.85109\n",
      "\u001b[32m[1112 12:46:29 @monitor.py:362]\u001b[0m macro_f1: 0.54698\n",
      "\u001b[32m[1112 12:46:29 @monitor.py:362]\u001b[0m mean_average_precision: 0.55834\n",
      "\u001b[32m[1112 12:46:29 @monitor.py:362]\u001b[0m micro_auc: 0.86346\n",
      "\u001b[32m[1112 12:46:29 @monitor.py:362]\u001b[0m micro_f1: 0.566\n",
      "\u001b[32m[1112 12:46:29 @monitor.py:362]\u001b[0m one_error: 0.43137\n",
      "\u001b[32m[1112 12:46:29 @monitor.py:362]\u001b[0m ranking_loss: 0.13987\n",
      "\u001b[32m[1112 12:46:29 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70671\n",
      "\u001b[32m[1112 12:46:29 @monitor.py:362]\u001b[0m training_ap: 0.58114\n",
      "\u001b[32m[1112 12:46:29 @monitor.py:362]\u001b[0m training_auc: 0.85217\n",
      "\u001b[32m[1112 12:46:29 @base.py:257]\u001b[0m Start Epoch 199 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,14.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:46:31 @base.py:267]\u001b[0m Epoch 199 (global_step 4577) finished, time:1.55 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00, 9.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:46:31 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-4577.\n",
      "\u001b[32m[1112 12:46:31 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.968\n",
      "\u001b[32m[1112 12:46:31 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.911\n",
      "\u001b[32m[1112 12:46:31 @monitor.py:362]\u001b[0m coverage: 2.0606\n",
      "\u001b[32m[1112 12:46:31 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.39563\n",
      "\u001b[32m[1112 12:46:31 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:46:31 @monitor.py:362]\u001b[0m loss/value: 6.981\n",
      "\u001b[32m[1112 12:46:31 @monitor.py:362]\u001b[0m macro_auc: 0.86304\n",
      "\u001b[32m[1112 12:46:31 @monitor.py:362]\u001b[0m macro_f1: 0.55828\n",
      "\u001b[32m[1112 12:46:31 @monitor.py:362]\u001b[0m mean_average_precision: 0.57579\n",
      "\u001b[32m[1112 12:46:31 @monitor.py:362]\u001b[0m micro_auc: 0.86784\n",
      "\u001b[32m[1112 12:46:31 @monitor.py:362]\u001b[0m micro_f1: 0.57418\n",
      "\u001b[32m[1112 12:46:31 @monitor.py:362]\u001b[0m one_error: 0.41355\n",
      "\u001b[32m[1112 12:46:31 @monitor.py:362]\u001b[0m ranking_loss: 0.13817\n",
      "\u001b[32m[1112 12:46:31 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.71661\n",
      "\u001b[32m[1112 12:46:31 @monitor.py:362]\u001b[0m training_ap: 0.58169\n",
      "\u001b[32m[1112 12:46:31 @monitor.py:362]\u001b[0m training_auc: 0.85243\n",
      "\u001b[32m[1112 12:46:31 @base.py:257]\u001b[0m Start Epoch 200 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|23/23[00:01<00:00,15.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:46:33 @base.py:267]\u001b[0m Epoch 200 (global_step 4600) finished, time:1.50 sec.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|##########|5/5[00:00<00:00,10.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:46:33 @saver.py:90]\u001b[0m Model saved to train_log/sift-512-hidden-gls-r5/model-4600.\n",
      "\u001b[32m[1112 12:46:33 @monitor.py:362]\u001b[0m DataParallelInferenceRunner/QueueInput/queue_size: 49.936\n",
      "\u001b[32m[1112 12:46:33 @monitor.py:362]\u001b[0m QueueInput/queue_size: 49.924\n",
      "\u001b[32m[1112 12:46:33 @monitor.py:362]\u001b[0m coverage: 2.1105\n",
      "\u001b[32m[1112 12:46:33 @monitor.py:362]\u001b[0m doubly_stochastic_loss: 0.25796\n",
      "\u001b[32m[1112 12:46:33 @monitor.py:362]\u001b[0m learning_rate: 0.0001\n",
      "\u001b[32m[1112 12:46:33 @monitor.py:362]\u001b[0m loss/value: 7.0162\n",
      "\u001b[32m[1112 12:46:33 @monitor.py:362]\u001b[0m macro_auc: 0.85095\n",
      "\u001b[32m[1112 12:46:33 @monitor.py:362]\u001b[0m macro_f1: 0.54945\n",
      "\u001b[32m[1112 12:46:33 @monitor.py:362]\u001b[0m mean_average_precision: 0.55784\n",
      "\u001b[32m[1112 12:46:33 @monitor.py:362]\u001b[0m micro_auc: 0.86347\n",
      "\u001b[32m[1112 12:46:33 @monitor.py:362]\u001b[0m micro_f1: 0.57351\n",
      "\u001b[32m[1112 12:46:33 @monitor.py:362]\u001b[0m one_error: 0.41711\n",
      "\u001b[32m[1112 12:46:33 @monitor.py:362]\u001b[0m ranking_loss: 0.14412\n",
      "\u001b[32m[1112 12:46:33 @monitor.py:362]\u001b[0m ranking_mean_average_precision: 0.70821\n",
      "\u001b[32m[1112 12:46:33 @monitor.py:362]\u001b[0m training_ap: 0.5822\n",
      "\u001b[32m[1112 12:46:33 @monitor.py:362]\u001b[0m training_auc: 0.85269\n",
      "\u001b[32m[1112 12:46:33 @base.py:271]\u001b[0m Training has finished!\n",
      "\u001b[32m[1112 12:46:33 @input_source.py:148]\u001b[0m EnqueueThread DataParallelInferenceRunner/QueueInput/input_queue Exited.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1112 12:46:33 @input_source.py:148]\u001b[0m EnqueueThread QueueInput/input_queue Exited.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "train_config = TrainConfig(model=model, dataflow=train_data,\n",
    "                           callbacks=[\n",
    "                               ScheduledHyperParamSetter('learning_rate', [(0, 1e-3), (40, 1e-4)]),\n",
    "                               InfRunner(val_data, [AggregateMetric(config.validation_metrics, threshold)],\n",
    "                                         [1]),\n",
    "                               ModelSaver(),\n",
    "                               MaxSaver('micro_f1', save_name),\n",
    "                           ],\n",
    "                           #session_init=SaverRestore(\n",
    "                           #    model_path=resnet_loc, ignore=ignore_restore),\n",
    "                           max_epoch=200, tower=[1])\n",
    "Trainer(train_config).train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "\u001b[32m[1112 12:54:39 @collection.py:133]\u001b[0m New collections created in : tf.GraphKeys.MODEL_VARIABLES\n",
      "\u001b[32m[1112 12:54:39 @collection.py:152]\u001b[0m These collections were modified but restored in : (tf.GraphKeys.SUMMARIES: 0->3), (tf.GraphKeys.UPDATE_OPS: 0->2)\n",
      "\u001b[32m[1112 12:54:39 @sessinit.py:89]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m The following variables are in the checkpoint, but not found in the graph: beta1_power:0, beta2_power:0, global_step:0, learning_rate:0\n",
      "\u001b[32m[1112 12:54:39 @sessinit.py:116]\u001b[0m Restoring checkpoint from train_log/sift-512-hidden-gls-r5/max-micro_f1.tfmodel ...\n",
      "INFO:tensorflow:Restoring parameters from train_log/sift-512-hidden-gls-r5/max-micro_f1.tfmodel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########|27/27[00:03<00:00, 7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_average_precision': 0.58345858737950296, 'macro_auc': 0.86372708322749281, 'micro_auc': 0.87064159832769239, 'macro_f1': 0.56485263927373663, 'micro_f1': 0.5855830030940874, 'ranking_mean_average_precision': 0.72564077194774257, 'coverage': 2.1718796544533809, 'ranking_loss': 0.13832106330765842, 'one_error': 0.39142091152815012}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#np.set_printoptions(formatter={'float_kind': lambda x: '%.3f' % x})\n",
    "tf.reset_default_graph()\n",
    "pred_config = PredictConfig(model=model,\n",
    "                            session_init=SaverRestore(\n",
    "                                model_path=log_dir + save_name),\n",
    "                            output_names=['logits_export', 'label'],\n",
    "                            )\n",
    "# pred = Predictor(pred_config, test_data, nr_proc=2, ordered=False)\n",
    "pred = SimpleDatasetPredictor(pred_config, test_data)\n",
    "\n",
    "accumulator = seq(pred.get_result()) \\\n",
    "    .smap(lambda a, b: (a.shape[0], calcu_metrics(a, b, config.validation_metrics, threshold))) \\\n",
    "    .aggregate(Accumulator(*config.validation_metrics), lambda accu, args: accu.feed(args[0], *args[1]))\n",
    "metrics = accumulator.retrive()\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coverage': 1.484361036639857,\n",
       " 'macro_auc': 0.93432849392906958,\n",
       " 'macro_f1': 0.67689911330644059,\n",
       " 'mean_average_precision': 0.77945948576496482,\n",
       " 'micro_auc': 0.94224849099895258,\n",
       " 'micro_f1': 0.69087716105781305,\n",
       " 'one_error': 0.25350014894250822,\n",
       " 'ranking_loss': 0.067377654368270937,\n",
       " 'ranking_mean_average_precision': 0.83210500683244182}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coverage': 3.1480654761904763,\n",
       " 'macro_auc': 0.9269738270190967,\n",
       " 'macro_f1': 0.58264172937949921,\n",
       " 'mean_average_precision': 0.70567296989950623,\n",
       " 'micro_auc': 0.93068400367147741,\n",
       " 'micro_f1': 0.64035019016619532,\n",
       " 'one_error': 0.27752976190476192,\n",
       " 'ranking_loss': 0.065381924694430743,\n",
       " 'ranking_mean_average_precision': 0.78210932154875579}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc0dc01d0b8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAACoCAYAAADuISUAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc17c3d0588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data.reset_state()\n",
    "batch = next(train_data.get_data())\n",
    "plt.imshow(batch[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = batch[0][0][0]\n",
    "img.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'tower0/custom_cnn/conv1/kernel/Regularizer/l2_regularizer:0' shape=() dtype=float32>,\n",
       " <tf.Tensor 'tower0/rnn/process/fc_read/kernel/Regularizer/l2_regularizer:0' shape=() dtype=float32>,\n",
       " <tf.Tensor 'tower0/rnn/process/fc_fg/kernel/Regularizer/l2_regularizer:0' shape=() dtype=float32>,\n",
       " <tf.Tensor 'tower0/rnn/process/fc_ig/kernel/Regularizer/l2_regularizer:0' shape=() dtype=float32>,\n",
       " <tf.Tensor 'tower0/logits/kernel/Regularizer/l2_regularizer:0' shape=() dtype=float32>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
